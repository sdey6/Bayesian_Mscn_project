{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTfYgWCEM2C4"
      },
      "source": [
        "# BNNs Early Tests, copied from Javier Antoran's repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8kN_fUgtx-H"
      },
      "source": [
        "Code mostly from: [Javier Antoran's Bayesian Neural Networks GitHub repository](https://github.com/JavierAntoran/Bayesian-Neural-Networks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAYPKSFrG8AF",
        "outputId": "7eb949b4-ae8d-4286-f2a9-21a3a8df9d4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n",
            "Collecting GPy\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy) (0.29.24)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.4.1)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565116 sha256=1e838ead7ff2ed41d15626f6368a533f3cff3d959943518ceb12ccd16223eac5\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102565 sha256=3bd128751452972a9d563772ea9a9833e3c6b33c11922af24a3e9ceeafe389f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built GPy paramz\n",
            "Installing collected packages: paramz, GPy\n",
            "Successfully installed GPy-1.10.0 paramz-0.9.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip install GPy\n",
        "import GPy\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-7dNcVmHA3I",
        "outputId": "ba27d683-faad-45b4-ede6-f64ece0e9353"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.device(0)\n",
        "torch.cuda.get_device_name(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BzTB5WsP9Kx"
      },
      "outputs": [],
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVYCgsBoJTcr"
      },
      "source": [
        "calculating losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va8V78eFFsc9"
      },
      "outputs": [],
      "source": [
        "def log_gaussian_loss(output, target, sigma, no_dim, sum_reduce=True):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma) - 0.5*no_dim*np.log(2*np.pi)\n",
        "    \n",
        "    if sum_reduce:\n",
        "        return -(log_coeff + exponent).sum()\n",
        "    else:\n",
        "        return -(log_coeff + exponent)\n",
        "\n",
        "\n",
        "def get_kl_divergence(weights, prior, varpost):\n",
        "    prior_loglik = prior.loglik(weights)\n",
        "    \n",
        "    varpost_loglik = varpost.loglik(weights)\n",
        "    varpost_lik = varpost_loglik.exp()\n",
        "    \n",
        "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
        "\n",
        "\n",
        "class gaussian:\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def loglik(self, weights):\n",
        "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
        "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
        "        \n",
        "        return (exponent + log_coeff).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiMLluYgJkq2"
      },
      "source": [
        "sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASGi2Ecx5G-F"
      },
      "outputs": [],
      "source": [
        "class BayesLinear_Normalq(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, prior):\n",
        "        super(BayesLinear_Normalq, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.prior = prior\n",
        "        \n",
        "        scale = (2/self.input_dim)**0.5\n",
        "        rho_init = np.log(np.exp((2/self.input_dim)**0.5) - 1)\n",
        "        self.weight_mus = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
        "        self.weight_rhos = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-3, -3))\n",
        "        \n",
        "        self.bias_mus = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
        "        self.bias_rhos = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-4, -3))\n",
        "        \n",
        "    def forward(self, x, sample = True):\n",
        "        \n",
        "        if sample:\n",
        "            # sample gaussian noise for each weight and each bias\n",
        "            weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())\n",
        "            bias_epsilons =  Variable(self.bias_mus.data.new(self.bias_mus.size()).normal_())\n",
        "            \n",
        "            # calculate the weight and bias stds from the rho parameters\n",
        "            weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
        "            bias_stds = torch.log(1 + torch.exp(self.bias_rhos))\n",
        "            \n",
        "            # calculate samples from the posterior from the sampled noise and mus/stds\n",
        "            weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
        "            bias_sample = self.bias_mus + bias_epsilons*bias_stds\n",
        "            \n",
        "            output = torch.mm(x, weight_sample) + bias_sample\n",
        "            \n",
        "            # computing the KL loss term\n",
        "            prior_cov, varpost_cov = self.prior.sigma**2, weight_stds**2\n",
        "            KL_loss = 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*weight_stds.numel()\n",
        "            KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
        "            KL_loss = KL_loss + 0.5*((self.weight_mus - self.prior.mu)**2/prior_cov).sum()\n",
        "            \n",
        "            prior_cov, varpost_cov = self.prior.sigma**2, bias_stds**2\n",
        "            KL_loss = KL_loss + 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*bias_stds.numel()\n",
        "            KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
        "            KL_loss = KL_loss + 0.5*((self.bias_mus - self.prior.mu)**2/prior_cov).sum()\n",
        "            \n",
        "            return output, KL_loss\n",
        "        \n",
        "        else:\n",
        "            output = torch.mm(x, self.weight_mus) + self.bias_mus\n",
        "            return output, KL_loss\n",
        "        \n",
        "    def sample_layer(self, no_samples):\n",
        "        all_samples = []\n",
        "        for i in range(no_samples):\n",
        "            # sample gaussian noise for each weight and each bias\n",
        "            weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())\n",
        "            \n",
        "            # calculate the weight and bias stds from the rho parameters\n",
        "            weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
        "            \n",
        "            # calculate samples from the posterior from the sampled noise and mus/stds\n",
        "            weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
        "            \n",
        "            all_samples += weight_sample.view(-1).cpu().data.numpy().tolist()\n",
        "            \n",
        "        return all_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8dV-QIq5G-I"
      },
      "outputs": [],
      "source": [
        "class BBP_Heteroscedastic_Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(BBP_Heteroscedastic_Model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = BayesLinear_Normalq(input_dim, num_units, gaussian(0, 1))\n",
        "        self.layer2 = BayesLinear_Normalq(num_units, 2*output_dim, gaussian(0, 1))\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        KL_loss_total = 0\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x, KL_loss = self.layer1(x)\n",
        "        KL_loss_total = KL_loss_total + KL_loss\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x, KL_loss = self.layer2(x)\n",
        "        KL_loss_total = KL_loss_total + KL_loss\n",
        "        \n",
        "        return x, KL_loss_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAYelw3B5G-K"
      },
      "outputs": [],
      "source": [
        "class BBP_Heteroscedastic_Model_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size, no_batches):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.no_batches = no_batches\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y, no_samples):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        fit_loss_total = 0\n",
        "        \n",
        "        for i in range(no_samples):\n",
        "            output, KL_loss_total = self.network(x)\n",
        "\n",
        "            # calculate fit loss based on mean and standard deviation of output\n",
        "            fit_loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "            fit_loss_total = fit_loss_total + fit_loss\n",
        "        \n",
        "        KL_loss_total = KL_loss_total/self.no_batches\n",
        "        total_loss = (fit_loss_total + KL_loss_total)/(no_samples*x.shape[0])\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return fit_loss_total/no_samples, KL_loss_total\n",
        "    \n",
        "    def get_loss_and_rmse(self, x, y, no_samples):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        means, stds = [], []\n",
        "        for i in range(no_samples):\n",
        "            output, KL_loss_total = self.network(x)\n",
        "            means.append(output[:, :1, None])\n",
        "            stds.append(output[:, 1:, None].exp())\n",
        "            \n",
        "        means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
        "        mean = means.mean(dim=2)\n",
        "        std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
        "            \n",
        "        # calculate fit loss based on mean and standard deviation of output\n",
        "        logliks = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1, sum_reduce=False)\n",
        "        rmse = float((((mean - y)**2).mean()**0.5).cpu().data)\n",
        "\n",
        "        return logliks, rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym6HBK-s8GnO",
        "outputId": "6bbe6ea9-8778-4c07-de8b-174b252cea8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:     1/ 2000, Fit loss = 355.431, KL loss = 2122.010\n",
            "Epoch:   101/ 2000, Fit loss = 104.316, KL loss = 1836.494\n",
            "Epoch:   201/ 2000, Fit loss =  95.512, KL loss = 1765.585\n",
            "Epoch:   301/ 2000, Fit loss =  69.128, KL loss = 1701.091\n",
            "Epoch:   401/ 2000, Fit loss =  66.324, KL loss = 1643.952\n",
            "Epoch:   501/ 2000, Fit loss =  45.362, KL loss = 1588.294\n",
            "Epoch:   601/ 2000, Fit loss =  52.558, KL loss = 1532.322\n",
            "Epoch:   701/ 2000, Fit loss =  40.983, KL loss = 1474.382\n",
            "Epoch:   801/ 2000, Fit loss =  27.533, KL loss = 1419.050\n",
            "Epoch:   901/ 2000, Fit loss =  27.017, KL loss = 1370.871\n",
            "Epoch:  1001/ 2000, Fit loss =  17.365, KL loss = 1329.558\n",
            "Epoch:  1101/ 2000, Fit loss =  15.395, KL loss = 1287.507\n",
            "Epoch:  1201/ 2000, Fit loss =  25.372, KL loss = 1247.564\n",
            "Epoch:  1301/ 2000, Fit loss =  19.556, KL loss = 1213.938\n",
            "Epoch:  1401/ 2000, Fit loss =  11.323, KL loss = 1181.054\n",
            "Epoch:  1501/ 2000, Fit loss =  11.517, KL loss = 1152.398\n",
            "Epoch:  1601/ 2000, Fit loss =  21.588, KL loss = 1124.573\n",
            "Epoch:  1701/ 2000, Fit loss =  12.187, KL loss = 1100.716\n",
            "Epoch:  1801/ 2000, Fit loss =  11.039, KL loss = 1075.310\n",
            "Epoch:  1901/ 2000, Fit loss =  11.491, KL loss = 1056.698\n",
            "Epoch:  2000/ 2000, Fit loss =  17.022, KL loss = 1040.584\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(2)\n",
        "no_points = 400\n",
        "lengthscale = 1\n",
        "variance = 1.0\n",
        "sig_noise = 0.3\n",
        "x = np.random.uniform(-3, 3, no_points)[:, None]\n",
        "x.sort(axis = 0)\n",
        "\n",
        "k = GPy.kern.RBF(input_dim = 1, variance = variance, lengthscale = lengthscale)\n",
        "C = k.K(x, x) + np.eye(no_points)*(x + 2)**2*sig_noise**2\n",
        "\n",
        "y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n",
        "y = (y - y.mean())\n",
        "x_train = x[75:325]\n",
        "y_mean = y[75:325].mean()\n",
        "y_std = y[75:325].var()**0.5\n",
        "y_train = (y[75:325] - y_mean)/y_std\n",
        "\n",
        "\n",
        "num_epochs, batch_size, nb_train = 2000, len(x_train), len(x_train)\n",
        "\n",
        "net = BBP_Heteroscedastic_Model_Wrapper(network=BBP_Heteroscedastic_Model(input_dim=1, output_dim=1, num_units=200),\n",
        "                                        learn_rate=1e-2, batch_size=batch_size, no_batches=1)\n",
        "\n",
        "fit_loss_train = np.zeros(num_epochs)\n",
        "KL_loss_train = np.zeros(num_epochs)\n",
        "total_loss = np.zeros(num_epochs)\n",
        "\n",
        "best_net, best_loss = None, float('inf')\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    \n",
        "    fit_loss, KL_loss = net.fit(x_train, y_train, no_samples = 10)\n",
        "    fit_loss_train[i] += fit_loss.cpu().data.numpy()\n",
        "    KL_loss_train[i] += KL_loss.cpu().data.numpy()\n",
        "    \n",
        "    total_loss[i] = fit_loss_train[i] + KL_loss_train[i]\n",
        "    \n",
        "    if fit_loss < best_loss:\n",
        "        best_loss = fit_loss\n",
        "        best_net = copy.deepcopy(net.network)\n",
        "        \n",
        "    if i % 100 == 0 or i == num_epochs - 1:\n",
        "        \n",
        "        print(\"Epoch: %5d/%5d, Fit loss = %7.3f, KL loss = %8.3f\" %\n",
        "              (i + 1, num_epochs, fit_loss_train[i], KL_loss_train[i]))\n",
        "\n",
        "        samples = []\n",
        "        for i in range(100):\n",
        "            preds = net.network.forward(torch.linspace(-3, 3, 200).cuda())[0]\n",
        "            samples.append(preds.cpu().data.numpy()[:, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "D-I8x6HNO_2M",
        "outputId": "5a483507-a22e-46a1-923c-e8df5494300b"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_80dcfc1a-f6d7-4eb0-952f-2860a86c311b\", \"bbp_hetero.pdf\", 22190)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"393.98125pt\" version=\"1.1\" viewBox=\"0 0 352.7 393.98125\" width=\"352.7pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 393.98125 \nL 352.7 393.98125 \nL 352.7 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 10.7 317.7125 \nL 345.5 317.7125 \nL 345.5 45.9125 \nL 10.7 45.9125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M -1.581139 1.581139 \nL 1.581139 -1.581139 \nM -1.581139 -1.581139 \nL 1.581139 1.581139 \n\" id=\"m0cea898512\" style=\"stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\"/>\n    </defs>\n    <g clip-path=\"url(#p69f385ce35)\">\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"105.87546\" xlink:href=\"#m0cea898512\" y=\"186.064732\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"107.466501\" xlink:href=\"#m0cea898512\" y=\"183.472913\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"107.926363\" xlink:href=\"#m0cea898512\" y=\"186.628921\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"108.151589\" xlink:href=\"#m0cea898512\" y=\"183.307337\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"108.336378\" xlink:href=\"#m0cea898512\" y=\"185.887026\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"108.74819\" xlink:href=\"#m0cea898512\" y=\"185.28046\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"108.939104\" xlink:href=\"#m0cea898512\" y=\"183.532077\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"109.131065\" xlink:href=\"#m0cea898512\" y=\"184.434204\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"109.554814\" xlink:href=\"#m0cea898512\" y=\"184.100948\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"109.797471\" xlink:href=\"#m0cea898512\" y=\"178.481265\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"109.861885\" xlink:href=\"#m0cea898512\" y=\"182.811676\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"110.063675\" xlink:href=\"#m0cea898512\" y=\"183.79906\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"110.37152\" xlink:href=\"#m0cea898512\" y=\"184.069878\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"111.027428\" xlink:href=\"#m0cea898512\" y=\"178.412275\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"111.671272\" xlink:href=\"#m0cea898512\" y=\"183.165134\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"111.73244\" xlink:href=\"#m0cea898512\" y=\"181.312437\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"112.257584\" xlink:href=\"#m0cea898512\" y=\"179.044038\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"112.558161\" xlink:href=\"#m0cea898512\" y=\"179.868883\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"112.869491\" xlink:href=\"#m0cea898512\" y=\"180.40116\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"112.87295\" xlink:href=\"#m0cea898512\" y=\"181.996744\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"112.985629\" xlink:href=\"#m0cea898512\" y=\"182.021389\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"112.994806\" xlink:href=\"#m0cea898512\" y=\"178.100443\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"114.082115\" xlink:href=\"#m0cea898512\" y=\"176.513273\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"114.632734\" xlink:href=\"#m0cea898512\" y=\"178.76563\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"114.836976\" xlink:href=\"#m0cea898512\" y=\"179.928257\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"115.376145\" xlink:href=\"#m0cea898512\" y=\"177.495573\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"118.104789\" xlink:href=\"#m0cea898512\" y=\"173.892613\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"118.277932\" xlink:href=\"#m0cea898512\" y=\"173.52196\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"118.37169\" xlink:href=\"#m0cea898512\" y=\"172.381177\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"118.567429\" xlink:href=\"#m0cea898512\" y=\"179.607074\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"118.64661\" xlink:href=\"#m0cea898512\" y=\"171.171311\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"118.897397\" xlink:href=\"#m0cea898512\" y=\"169.057035\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"119.437501\" xlink:href=\"#m0cea898512\" y=\"177.010506\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"119.91757\" xlink:href=\"#m0cea898512\" y=\"175.78202\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"120.061341\" xlink:href=\"#m0cea898512\" y=\"178.039218\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"121.174702\" xlink:href=\"#m0cea898512\" y=\"169.923488\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"121.230723\" xlink:href=\"#m0cea898512\" y=\"176.456055\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"121.4374\" xlink:href=\"#m0cea898512\" y=\"173.587151\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"122.085556\" xlink:href=\"#m0cea898512\" y=\"168.828959\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"122.438389\" xlink:href=\"#m0cea898512\" y=\"167.950146\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"122.688966\" xlink:href=\"#m0cea898512\" y=\"171.763056\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"122.731643\" xlink:href=\"#m0cea898512\" y=\"172.400767\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"123.274478\" xlink:href=\"#m0cea898512\" y=\"167.729668\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"124.264235\" xlink:href=\"#m0cea898512\" y=\"170.420514\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"125.580047\" xlink:href=\"#m0cea898512\" y=\"172.349545\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"126.213781\" xlink:href=\"#m0cea898512\" y=\"174.513061\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"126.319119\" xlink:href=\"#m0cea898512\" y=\"168.709604\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"126.710729\" xlink:href=\"#m0cea898512\" y=\"162.218315\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"127.141247\" xlink:href=\"#m0cea898512\" y=\"169.644226\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"127.573611\" xlink:href=\"#m0cea898512\" y=\"163.712477\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"127.732816\" xlink:href=\"#m0cea898512\" y=\"170.706752\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"127.925869\" xlink:href=\"#m0cea898512\" y=\"170.133845\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"128.283061\" xlink:href=\"#m0cea898512\" y=\"169.232923\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"128.73849\" xlink:href=\"#m0cea898512\" y=\"167.492527\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"129.022556\" xlink:href=\"#m0cea898512\" y=\"164.673651\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"129.688389\" xlink:href=\"#m0cea898512\" y=\"177.958751\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"129.891218\" xlink:href=\"#m0cea898512\" y=\"167.892919\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"131.110364\" xlink:href=\"#m0cea898512\" y=\"168.901553\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"131.585964\" xlink:href=\"#m0cea898512\" y=\"158.237433\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"132.204868\" xlink:href=\"#m0cea898512\" y=\"164.963054\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"132.571302\" xlink:href=\"#m0cea898512\" y=\"168.27509\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"134.747706\" xlink:href=\"#m0cea898512\" y=\"170.315815\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"134.75042\" xlink:href=\"#m0cea898512\" y=\"166.895069\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"134.794767\" xlink:href=\"#m0cea898512\" y=\"173.683349\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"135.07158\" xlink:href=\"#m0cea898512\" y=\"155.688363\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"135.396133\" xlink:href=\"#m0cea898512\" y=\"158.971164\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"135.566915\" xlink:href=\"#m0cea898512\" y=\"162.910774\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"136.190091\" xlink:href=\"#m0cea898512\" y=\"171.017734\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"136.277445\" xlink:href=\"#m0cea898512\" y=\"161.440922\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"136.516211\" xlink:href=\"#m0cea898512\" y=\"166.054675\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"136.704024\" xlink:href=\"#m0cea898512\" y=\"165.357372\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"137.680617\" xlink:href=\"#m0cea898512\" y=\"166.967345\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"138.494738\" xlink:href=\"#m0cea898512\" y=\"160.641846\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"138.580289\" xlink:href=\"#m0cea898512\" y=\"163.504762\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"139.293673\" xlink:href=\"#m0cea898512\" y=\"166.888381\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"139.560256\" xlink:href=\"#m0cea898512\" y=\"161.151323\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"139.643351\" xlink:href=\"#m0cea898512\" y=\"162.325177\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"140.145943\" xlink:href=\"#m0cea898512\" y=\"170.141582\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"140.320701\" xlink:href=\"#m0cea898512\" y=\"160.438556\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"140.542287\" xlink:href=\"#m0cea898512\" y=\"153.695616\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"141.038599\" xlink:href=\"#m0cea898512\" y=\"167.891314\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"141.761324\" xlink:href=\"#m0cea898512\" y=\"150.427237\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"141.869541\" xlink:href=\"#m0cea898512\" y=\"156.97029\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"141.95077\" xlink:href=\"#m0cea898512\" y=\"161.992341\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"143.820778\" xlink:href=\"#m0cea898512\" y=\"166.614663\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"144.865277\" xlink:href=\"#m0cea898512\" y=\"162.476549\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"145.208949\" xlink:href=\"#m0cea898512\" y=\"161.612648\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"145.73632\" xlink:href=\"#m0cea898512\" y=\"178.675566\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"146.291753\" xlink:href=\"#m0cea898512\" y=\"160.336315\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"146.547785\" xlink:href=\"#m0cea898512\" y=\"174.347226\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"146.883044\" xlink:href=\"#m0cea898512\" y=\"172.36135\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"148.066217\" xlink:href=\"#m0cea898512\" y=\"183.61982\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"148.175178\" xlink:href=\"#m0cea898512\" y=\"180.952088\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"148.175764\" xlink:href=\"#m0cea898512\" y=\"178.102712\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"148.177394\" xlink:href=\"#m0cea898512\" y=\"170.139201\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"148.309437\" xlink:href=\"#m0cea898512\" y=\"177.427659\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"148.481251\" xlink:href=\"#m0cea898512\" y=\"177.534413\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"149.925813\" xlink:href=\"#m0cea898512\" y=\"193.607268\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"151.512484\" xlink:href=\"#m0cea898512\" y=\"174.455887\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"152.283259\" xlink:href=\"#m0cea898512\" y=\"199.167648\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"154.017817\" xlink:href=\"#m0cea898512\" y=\"181.981116\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"154.021885\" xlink:href=\"#m0cea898512\" y=\"195.083367\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"154.061939\" xlink:href=\"#m0cea898512\" y=\"191.665041\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"154.112287\" xlink:href=\"#m0cea898512\" y=\"182.210539\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"154.503829\" xlink:href=\"#m0cea898512\" y=\"172.048836\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"154.733296\" xlink:href=\"#m0cea898512\" y=\"202.57965\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"155.135776\" xlink:href=\"#m0cea898512\" y=\"171.735414\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"155.269781\" xlink:href=\"#m0cea898512\" y=\"159.645157\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"156.329267\" xlink:href=\"#m0cea898512\" y=\"186.60633\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"156.790968\" xlink:href=\"#m0cea898512\" y=\"164.998954\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"156.913111\" xlink:href=\"#m0cea898512\" y=\"198.915065\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"157.001683\" xlink:href=\"#m0cea898512\" y=\"195.65687\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"157.223846\" xlink:href=\"#m0cea898512\" y=\"185.102891\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"157.81732\" xlink:href=\"#m0cea898512\" y=\"183.00839\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"158.388347\" xlink:href=\"#m0cea898512\" y=\"185.101405\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"158.438941\" xlink:href=\"#m0cea898512\" y=\"198.969414\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"158.581458\" xlink:href=\"#m0cea898512\" y=\"190.545831\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"158.724499\" xlink:href=\"#m0cea898512\" y=\"199.773391\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"158.740898\" xlink:href=\"#m0cea898512\" y=\"189.901962\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"159.717347\" xlink:href=\"#m0cea898512\" y=\"215.125486\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"160.398883\" xlink:href=\"#m0cea898512\" y=\"217.642195\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"160.399587\" xlink:href=\"#m0cea898512\" y=\"207.455559\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"161.128684\" xlink:href=\"#m0cea898512\" y=\"177.951266\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"161.597547\" xlink:href=\"#m0cea898512\" y=\"188.634761\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"164.037099\" xlink:href=\"#m0cea898512\" y=\"191.578785\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"164.399554\" xlink:href=\"#m0cea898512\" y=\"213.810276\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"164.561268\" xlink:href=\"#m0cea898512\" y=\"202.003977\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"165.318907\" xlink:href=\"#m0cea898512\" y=\"180.668257\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"166.086556\" xlink:href=\"#m0cea898512\" y=\"199.416933\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"166.255822\" xlink:href=\"#m0cea898512\" y=\"217.213673\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"167.42726\" xlink:href=\"#m0cea898512\" y=\"223.773805\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"168.215982\" xlink:href=\"#m0cea898512\" y=\"206.235143\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"168.557526\" xlink:href=\"#m0cea898512\" y=\"234.8434\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"169.870645\" xlink:href=\"#m0cea898512\" y=\"201.484759\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"169.878088\" xlink:href=\"#m0cea898512\" y=\"202.844831\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"170.475236\" xlink:href=\"#m0cea898512\" y=\"207.488379\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"170.924024\" xlink:href=\"#m0cea898512\" y=\"223.694299\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"171.451843\" xlink:href=\"#m0cea898512\" y=\"242.673594\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"171.52523\" xlink:href=\"#m0cea898512\" y=\"208.13298\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"172.147119\" xlink:href=\"#m0cea898512\" y=\"188.963646\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"172.403136\" xlink:href=\"#m0cea898512\" y=\"211.568992\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"172.734217\" xlink:href=\"#m0cea898512\" y=\"216.866655\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"172.782539\" xlink:href=\"#m0cea898512\" y=\"214.836427\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"172.993116\" xlink:href=\"#m0cea898512\" y=\"230.577504\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"173.218064\" xlink:href=\"#m0cea898512\" y=\"235.931349\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"173.255107\" xlink:href=\"#m0cea898512\" y=\"201.726485\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"173.257093\" xlink:href=\"#m0cea898512\" y=\"230.804329\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"173.435262\" xlink:href=\"#m0cea898512\" y=\"220.496535\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"173.759637\" xlink:href=\"#m0cea898512\" y=\"246.897432\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"175.022808\" xlink:href=\"#m0cea898512\" y=\"231.594885\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"175.197416\" xlink:href=\"#m0cea898512\" y=\"227.563626\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"177.055365\" xlink:href=\"#m0cea898512\" y=\"244.209616\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"177.245573\" xlink:href=\"#m0cea898512\" y=\"235.058745\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"177.598709\" xlink:href=\"#m0cea898512\" y=\"245.998608\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"177.790532\" xlink:href=\"#m0cea898512\" y=\"227.67584\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"178.321108\" xlink:href=\"#m0cea898512\" y=\"211.24263\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"179.372142\" xlink:href=\"#m0cea898512\" y=\"219.983661\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"179.690544\" xlink:href=\"#m0cea898512\" y=\"270.303939\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"179.847216\" xlink:href=\"#m0cea898512\" y=\"245.914669\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"180.662689\" xlink:href=\"#m0cea898512\" y=\"225.691235\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"181.33102\" xlink:href=\"#m0cea898512\" y=\"228.591933\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"181.513891\" xlink:href=\"#m0cea898512\" y=\"244.040834\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"182.297911\" xlink:href=\"#m0cea898512\" y=\"231.819918\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"182.634659\" xlink:href=\"#m0cea898512\" y=\"271.42177\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"182.669764\" xlink:href=\"#m0cea898512\" y=\"235.702248\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"182.948528\" xlink:href=\"#m0cea898512\" y=\"271.211618\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"184.195091\" xlink:href=\"#m0cea898512\" y=\"239.947761\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"184.638134\" xlink:href=\"#m0cea898512\" y=\"231.988542\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"185.575942\" xlink:href=\"#m0cea898512\" y=\"241.506904\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"186.374456\" xlink:href=\"#m0cea898512\" y=\"220.881009\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"186.797532\" xlink:href=\"#m0cea898512\" y=\"287.304525\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"187.03778\" xlink:href=\"#m0cea898512\" y=\"237.694686\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"187.575646\" xlink:href=\"#m0cea898512\" y=\"235.872177\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"189.099744\" xlink:href=\"#m0cea898512\" y=\"248.077663\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"189.667837\" xlink:href=\"#m0cea898512\" y=\"254.454193\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"191.171847\" xlink:href=\"#m0cea898512\" y=\"243.686898\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"191.484504\" xlink:href=\"#m0cea898512\" y=\"242.858321\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"191.659809\" xlink:href=\"#m0cea898512\" y=\"260.364962\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"192.647506\" xlink:href=\"#m0cea898512\" y=\"280.859829\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"192.704986\" xlink:href=\"#m0cea898512\" y=\"260.955658\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"193.722318\" xlink:href=\"#m0cea898512\" y=\"237.55026\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"195.126289\" xlink:href=\"#m0cea898512\" y=\"289.959998\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"195.229703\" xlink:href=\"#m0cea898512\" y=\"279.076586\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"195.687268\" xlink:href=\"#m0cea898512\" y=\"246.741127\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"196.569968\" xlink:href=\"#m0cea898512\" y=\"245.69184\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"197.421118\" xlink:href=\"#m0cea898512\" y=\"223.862795\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"197.499633\" xlink:href=\"#m0cea898512\" y=\"259.195308\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"197.815894\" xlink:href=\"#m0cea898512\" y=\"228.858455\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"198.292899\" xlink:href=\"#m0cea898512\" y=\"260.839696\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"198.33072\" xlink:href=\"#m0cea898512\" y=\"264.217904\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"198.920019\" xlink:href=\"#m0cea898512\" y=\"231.693949\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"199.484888\" xlink:href=\"#m0cea898512\" y=\"267.970041\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"199.559116\" xlink:href=\"#m0cea898512\" y=\"259.895691\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"201.900184\" xlink:href=\"#m0cea898512\" y=\"242.095069\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"202.012907\" xlink:href=\"#m0cea898512\" y=\"249.651411\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"202.197939\" xlink:href=\"#m0cea898512\" y=\"243.00706\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"202.333705\" xlink:href=\"#m0cea898512\" y=\"243.369441\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"202.515746\" xlink:href=\"#m0cea898512\" y=\"294.984904\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"202.62817\" xlink:href=\"#m0cea898512\" y=\"265.130317\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"202.773177\" xlink:href=\"#m0cea898512\" y=\"254.887994\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"204.549218\" xlink:href=\"#m0cea898512\" y=\"272.797736\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"204.573284\" xlink:href=\"#m0cea898512\" y=\"264.961477\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"205.525197\" xlink:href=\"#m0cea898512\" y=\"303.171521\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"206.36629\" xlink:href=\"#m0cea898512\" y=\"263.748425\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"206.469381\" xlink:href=\"#m0cea898512\" y=\"257.620921\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"206.519019\" xlink:href=\"#m0cea898512\" y=\"236.99199\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"207.768772\" xlink:href=\"#m0cea898512\" y=\"237.792295\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"207.83822\" xlink:href=\"#m0cea898512\" y=\"259.449531\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"209.778929\" xlink:href=\"#m0cea898512\" y=\"248.229422\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"209.956235\" xlink:href=\"#m0cea898512\" y=\"294.155693\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"212.251394\" xlink:href=\"#m0cea898512\" y=\"277.16151\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"212.918382\" xlink:href=\"#m0cea898512\" y=\"231.183488\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"213.116437\" xlink:href=\"#m0cea898512\" y=\"272.678613\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"213.289939\" xlink:href=\"#m0cea898512\" y=\"244.361679\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"213.308285\" xlink:href=\"#m0cea898512\" y=\"237.288563\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"214.427535\" xlink:href=\"#m0cea898512\" y=\"250.997388\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"214.688561\" xlink:href=\"#m0cea898512\" y=\"252.799121\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"215.00896\" xlink:href=\"#m0cea898512\" y=\"276.280951\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"215.509779\" xlink:href=\"#m0cea898512\" y=\"241.578086\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"215.69532\" xlink:href=\"#m0cea898512\" y=\"243.82834\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"216.218366\" xlink:href=\"#m0cea898512\" y=\"247.921705\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"217.285616\" xlink:href=\"#m0cea898512\" y=\"254.617255\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"217.32475\" xlink:href=\"#m0cea898512\" y=\"284.639428\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"217.352215\" xlink:href=\"#m0cea898512\" y=\"251.271014\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"217.862734\" xlink:href=\"#m0cea898512\" y=\"270.862096\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"217.866229\" xlink:href=\"#m0cea898512\" y=\"261.541598\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"218.701286\" xlink:href=\"#m0cea898512\" y=\"257.456395\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"220.674943\" xlink:href=\"#m0cea898512\" y=\"260.316251\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"221.030839\" xlink:href=\"#m0cea898512\" y=\"226.776543\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"221.664391\" xlink:href=\"#m0cea898512\" y=\"257.125986\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"222.31652\" xlink:href=\"#m0cea898512\" y=\"233.947603\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"222.666759\" xlink:href=\"#m0cea898512\" y=\"214.135089\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"223.377806\" xlink:href=\"#m0cea898512\" y=\"229.118183\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"224.780489\" xlink:href=\"#m0cea898512\" y=\"219.602793\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"224.866079\" xlink:href=\"#m0cea898512\" y=\"260.095853\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"227.73914\" xlink:href=\"#m0cea898512\" y=\"236.398562\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"228.295346\" xlink:href=\"#m0cea898512\" y=\"228.368283\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"229.333706\" xlink:href=\"#m0cea898512\" y=\"243.866737\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"230.592134\" xlink:href=\"#m0cea898512\" y=\"188.229254\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"230.753462\" xlink:href=\"#m0cea898512\" y=\"211.792896\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"230.868707\" xlink:href=\"#m0cea898512\" y=\"170.648407\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"230.971468\" xlink:href=\"#m0cea898512\" y=\"186.636578\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"231.060527\" xlink:href=\"#m0cea898512\" y=\"205.668276\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"231.8343\" xlink:href=\"#m0cea898512\" y=\"231.335265\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"232.613254\" xlink:href=\"#m0cea898512\" y=\"240.421786\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"232.711312\" xlink:href=\"#m0cea898512\" y=\"224.749823\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"233.653491\" xlink:href=\"#m0cea898512\" y=\"245.191894\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"234.095642\" xlink:href=\"#m0cea898512\" y=\"203.081815\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"234.37267\" xlink:href=\"#m0cea898512\" y=\"173.2468\"/>\n     <use style=\"fill-opacity:0.5;stroke:#000000;stroke-opacity:0.5;stroke-width:1.5;\" x=\"235.171071\" xlink:href=\"#m0cea898512\" y=\"208.04053\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_1\">\n    <path clip-path=\"url(#p69f385ce35)\" d=\"M -4.453726 275.605092 \nL -4.453726 290.017041 \nL -2.678861 288.328885 \nL -0.903996 286.637808 \nL 0.870869 284.945245 \nL 2.645734 283.260129 \nL 4.4206 281.576817 \nL 6.195465 279.893613 \nL 7.97033 278.209081 \nL 9.745195 276.520461 \nL 11.52006 274.836695 \nL 13.294925 273.156332 \nL 15.06979 271.474845 \nL 16.844655 269.793164 \nL 18.61952 268.112034 \nL 20.394385 266.439511 \nL 22.16925 264.775008 \nL 23.944115 263.115479 \nL 25.71898 261.456408 \nL 27.493845 259.799281 \nL 29.26871 258.13736 \nL 31.043575 256.475616 \nL 32.81844 254.817393 \nL 34.593305 253.163383 \nL 36.368171 251.510382 \nL 38.143036 249.857581 \nL 39.917901 248.203878 \nL 41.692766 246.549592 \nL 43.467631 244.892207 \nL 45.242496 243.236057 \nL 47.017361 241.575437 \nL 48.792226 239.913456 \nL 50.567091 238.249031 \nL 52.341956 236.568735 \nL 54.116821 234.868588 \nL 55.891686 233.162377 \nL 57.666551 231.45677 \nL 59.441416 229.754142 \nL 61.216281 228.059689 \nL 62.991146 226.364386 \nL 64.766011 224.667538 \nL 66.540876 222.968369 \nL 68.315742 221.276616 \nL 70.090607 219.586585 \nL 71.865472 217.900151 \nL 73.640337 216.213951 \nL 75.415202 214.522994 \nL 77.190067 212.823069 \nL 78.964932 211.120471 \nL 80.739797 209.411537 \nL 82.514662 207.701811 \nL 84.289527 205.984481 \nL 86.064392 204.262014 \nL 87.839257 202.540803 \nL 89.614122 200.815944 \nL 91.388987 199.085933 \nL 93.163852 197.351253 \nL 94.938717 195.610787 \nL 96.713582 193.8565 \nL 98.488447 192.093927 \nL 100.263313 190.330547 \nL 102.038178 188.563494 \nL 103.813043 186.790215 \nL 105.587908 185.010135 \nL 107.362773 183.221355 \nL 109.137638 181.422372 \nL 110.912503 179.614716 \nL 112.687368 177.798413 \nL 114.462233 175.9744 \nL 116.237098 174.142764 \nL 118.011963 172.360769 \nL 119.786828 170.689305 \nL 121.561693 169.072515 \nL 123.336558 167.649581 \nL 125.111423 166.463833 \nL 126.886288 165.266438 \nL 128.661153 164.051343 \nL 130.436018 162.821536 \nL 132.210884 161.575881 \nL 133.985749 160.310762 \nL 135.760614 159.030104 \nL 137.535479 157.744418 \nL 139.310344 156.543585 \nL 141.085209 155.814042 \nL 142.860074 157.111484 \nL 144.634939 159.671732 \nL 146.409804 162.336699 \nL 148.184669 164.990759 \nL 149.959534 167.62255 \nL 151.734399 170.232185 \nL 153.509264 172.813679 \nL 155.284129 175.364384 \nL 157.058994 177.880958 \nL 158.833859 180.358993 \nL 160.608724 182.855184 \nL 162.38359 185.812491 \nL 164.158455 189.143046 \nL 165.93332 192.481844 \nL 167.708185 195.812096 \nL 169.48305 199.133569 \nL 171.257915 202.447258 \nL 173.03278 205.752116 \nL 174.807645 209.047761 \nL 176.58251 212.332551 \nL 178.357375 215.607246 \nL 180.13224 218.870181 \nL 181.907105 222.114845 \nL 183.68197 224.805801 \nL 185.456835 226.751375 \nL 187.2317 228.678037 \nL 189.006565 230.604468 \nL 190.78143 232.528479 \nL 192.556295 234.440281 \nL 194.331161 236.06205 \nL 196.106026 236.767256 \nL 197.880891 237.020115 \nL 199.655756 237.137574 \nL 201.430621 237.247011 \nL 203.205486 237.340853 \nL 204.980351 237.438564 \nL 206.755216 237.530305 \nL 208.530081 237.617186 \nL 210.304946 237.702846 \nL 212.079811 237.78218 \nL 213.854676 237.849399 \nL 215.629541 237.694397 \nL 217.404406 236.312804 \nL 219.179271 232.553495 \nL 220.954136 226.898607 \nL 222.729001 220.453021 \nL 224.503866 213.530579 \nL 226.278732 206.42812 \nL 228.053597 199.170475 \nL 229.828462 191.747237 \nL 231.603327 184.133085 \nL 233.378192 176.320468 \nL 235.153057 168.233073 \nL 236.927922 159.838662 \nL 238.702787 150.950936 \nL 240.477652 141.497574 \nL 242.252517 131.443939 \nL 244.027382 120.718926 \nL 245.802247 109.131574 \nL 247.577112 96.477201 \nL 249.351977 82.555384 \nL 251.126842 67.000141 \nL 252.901707 49.557579 \nL 254.676572 29.607356 \nL 256.451437 6.273726 \nL 258.226303 -22.275065 \nL 260.001168 -55.934521 \nL 261.776033 -95.535127 \nL 263.550898 -143.198635 \nL 265.325763 -200.989772 \nL 267.100628 -271.146761 \nL 268.875493 -353.730949 \nL 270.650358 -454.790321 \nL 272.425223 -578.631977 \nL 274.200088 -729.860714 \nL 275.974953 -918.046561 \nL 277.749818 -1152.551443 \nL 279.524683 -1440.570649 \nL 281.299548 -1806.946218 \nL 283.074413 -2260.363026 \nL 284.849278 -2823.980676 \nL 286.624143 -3515.648277 \nL 288.399009 -4383.458198 \nL 290.173874 -5533.92758 \nL 291.948739 -7030.190672 \nL 293.723604 -8979.537402 \nL 295.498469 -11497.341208 \nL 297.273334 -14696.335745 \nL 299.048199 -18681.125195 \nL 300.823064 -23665.593353 \nL 302.597929 -30033.370447 \nL 304.372794 -38120.899664 \nL 306.147659 -47636.766858 \nL 307.922524 -59640.127722 \nL 309.697389 -74915.125146 \nL 311.472254 -94828.522949 \nL 313.247119 -120410.746826 \nL 315.021984 -153470.722217 \nL 316.796849 -197256.202539 \nL 318.571714 -226612.858691 \nL 320.34658 -226619.981055 \nL 322.121445 -226627.081299 \nL 323.89631 -226634.181543 \nL 325.671175 -226641.259668 \nL 327.44604 -226648.40415 \nL 329.220905 -226655.570752 \nL 330.99577 -226662.693115 \nL 332.770635 -226669.815479 \nL 334.5455 -226676.915723 \nL 336.320365 -226683.993848 \nL 338.09523 -226691.094092 \nL 339.870095 -226698.194336 \nL 341.64496 -226705.316699 \nL 343.419825 -226712.483301 \nL 345.19469 -226719.649902 \nL 346.969555 -226726.838623 \nL 348.74442 -226734.049463 \nL 348.74442 -226734.093701 \nL 348.74442 -226734.093701 \nL 346.969555 -226726.882861 \nL 345.19469 -226719.694141 \nL 343.419825 -226712.527539 \nL 341.64496 -226705.360938 \nL 339.870095 -226698.238574 \nL 338.09523 -226691.13833 \nL 336.320365 -226684.038086 \nL 334.5455 -226676.959961 \nL 332.770635 -226669.859717 \nL 330.99577 -226662.737354 \nL 329.220905 -226655.61499 \nL 327.44604 -226648.448389 \nL 325.671175 -226641.281787 \nL 323.89631 -226634.203662 \nL 322.121445 -226627.103418 \nL 320.34658 -226620.003174 \nL 318.571714 -226612.880811 \nL 316.796849 -197256.224658 \nL 315.021984 -153470.755396 \nL 313.247119 -120410.780005 \nL 311.472254 -94828.567188 \nL 309.697389 -74915.180444 \nL 307.922524 -59640.19408 \nL 306.147659 -47636.844275 \nL 304.372794 -38120.988141 \nL 302.597929 -30033.478278 \nL 300.823064 -23665.720538 \nL 299.048199 -18681.273117 \nL 297.273334 -14696.509933 \nL 295.498469 -11497.547192 \nL 293.723604 -8979.780713 \nL 291.948739 -7030.476839 \nL 290.173874 -5534.261787 \nL 288.399009 -4383.845283 \nL 286.624143 -3516.089968 \nL 284.849278 -2824.483195 \nL 283.074413 -2260.934322 \nL 281.299548 -1807.594586 \nL 279.524683 -1441.305073 \nL 277.749818 -1153.376418 \nL 275.974953 -918.970035 \nL 274.200088 -730.886489 \nL 272.425223 -579.761263 \nL 270.650358 -456.029555 \nL 268.875493 -355.084925 \nL 267.100628 -272.612673 \nL 265.325763 -202.578721 \nL 263.550898 -144.917146 \nL 261.776033 -97.381168 \nL 260.001168 -57.904032 \nL 258.226303 -24.361349 \nL 256.451437 4.085594 \nL 254.676572 27.36789 \nL 252.901707 47.298921 \nL 251.126842 64.743751 \nL 249.351977 80.323662 \nL 247.577112 94.302699 \nL 245.802247 107.030774 \nL 244.027382 118.691622 \nL 242.252517 129.505873 \nL 240.477652 139.660157 \nL 238.702787 149.213045 \nL 236.927922 158.219472 \nL 235.153057 166.74255 \nL 233.378192 174.953374 \nL 231.603327 182.894702 \nL 229.828462 190.631665 \nL 228.053597 198.160679 \nL 226.278732 205.515098 \nL 224.503866 212.695876 \nL 222.729001 219.679232 \nL 220.954136 226.189038 \nL 219.179271 231.944312 \nL 217.404406 235.84817 \nL 215.629541 237.295853 \nL 213.854676 237.491734 \nL 212.079811 237.454489 \nL 210.304946 237.400784 \nL 208.530081 237.337246 \nL 206.755216 237.272184 \nL 204.980351 237.200146 \nL 203.205486 237.120979 \nL 201.430621 237.045636 \nL 199.655756 236.951181 \nL 197.880891 236.846769 \nL 196.106026 236.614639 \nL 194.331161 235.951149 \nL 192.556295 234.342624 \nL 190.78143 232.436546 \nL 189.006565 230.518297 \nL 187.2317 228.597539 \nL 185.456835 226.676097 \nL 183.68197 224.736919 \nL 181.907105 222.055598 \nL 180.13224 218.813792 \nL 178.357375 215.553552 \nL 176.58251 212.281258 \nL 174.807645 208.998636 \nL 173.03278 205.704967 \nL 171.257915 202.401713 \nL 169.48305 199.089263 \nL 167.708185 195.768694 \nL 165.93332 192.43905 \nL 164.158455 189.100494 \nL 162.38359 185.770047 \nL 160.608724 182.813529 \nL 158.833859 180.315689 \nL 157.058994 177.835558 \nL 155.284129 175.31629 \nL 153.509264 172.762256 \nL 151.734399 170.176539 \nL 149.959534 167.56152 \nL 148.184669 164.923618 \nL 146.409804 162.262393 \nL 144.634939 159.591093 \nL 142.860074 157.038981 \nL 141.085209 155.77548 \nL 139.310344 156.520926 \nL 137.535479 157.725647 \nL 135.760614 159.011349 \nL 133.985749 160.291073 \nL 132.210884 161.554426 \nL 130.436018 162.798196 \nL 128.661153 164.025887 \nL 126.886288 165.238403 \nL 125.111423 166.432623 \nL 123.336558 167.614156 \nL 121.561693 169.028609 \nL 119.786828 170.637671 \nL 118.011963 172.297735 \nL 116.237098 174.062503 \nL 114.462233 175.878965 \nL 112.687368 177.685412 \nL 110.912503 179.480721 \nL 109.137638 181.264225 \nL 107.362773 183.035145 \nL 105.587908 184.790669 \nL 103.813043 186.531715 \nL 102.038178 188.260076 \nL 100.263313 189.976173 \nL 98.488447 191.682628 \nL 96.713582 193.382722 \nL 94.938717 195.067401 \nL 93.163852 196.726915 \nL 91.388987 198.365345 \nL 89.614122 199.982942 \nL 87.839257 201.582336 \nL 86.064392 203.164593 \nL 84.289527 204.729424 \nL 82.514662 206.268644 \nL 80.739797 207.791184 \nL 78.964932 209.310026 \nL 77.190067 210.821176 \nL 75.415202 212.331352 \nL 73.640337 213.827824 \nL 71.865472 215.310502 \nL 70.090607 216.787172 \nL 68.315742 218.26027 \nL 66.540876 219.728594 \nL 64.766011 221.194954 \nL 62.991146 222.652971 \nL 61.216281 224.105944 \nL 59.441416 225.553341 \nL 57.666551 227.004951 \nL 55.891686 228.457653 \nL 54.116821 229.916736 \nL 52.341956 231.378305 \nL 50.567091 232.830606 \nL 48.792226 234.262823 \nL 47.017361 235.68755 \nL 45.242496 237.100947 \nL 43.467631 238.503528 \nL 41.692766 239.902423 \nL 39.917901 241.291971 \nL 38.143036 242.674906 \nL 36.368171 244.056502 \nL 34.593305 245.436067 \nL 32.81844 246.814061 \nL 31.043575 248.193737 \nL 29.26871 249.572932 \nL 27.493845 250.953413 \nL 25.71898 252.331161 \nL 23.944115 253.706976 \nL 22.16925 255.080501 \nL 20.394385 256.463725 \nL 18.61952 257.851377 \nL 16.844655 259.232154 \nL 15.06979 260.604772 \nL 13.294925 261.976904 \nL 11.52006 263.345186 \nL 9.745195 264.703807 \nL 7.97033 266.06078 \nL 6.195465 267.417203 \nL 4.4206 268.784097 \nL 2.645734 270.149155 \nL 0.870869 271.513564 \nL -0.903996 272.883839 \nL -2.678861 274.24706 \nL -4.453726 275.605092 \nz\n\" style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#1f77b4;stroke-opacity:0.3;\"/>\n   </g>\n   <g id=\"PolyCollection_2\">\n    <path clip-path=\"url(#p69f385ce35)\" d=\"M -4.453726 290.287028 \nL -4.453726 304.698971 \nL -2.678861 302.683214 \nL -0.903996 300.667002 \nL 0.870869 298.655089 \nL 2.645734 296.652443 \nL 4.4206 294.655056 \nL 6.195465 292.660683 \nL 7.97033 290.653474 \nL 9.745195 288.638953 \nL 11.52006 286.636079 \nL 13.294925 284.650017 \nL 15.06979 282.665948 \nL 16.844655 280.682396 \nL 18.61952 278.70856 \nL 20.394385 276.7593 \nL 22.16925 274.822136 \nL 23.944115 272.885696 \nL 25.71898 270.953058 \nL 27.493845 269.026889 \nL 29.26871 267.094596 \nL 31.043575 265.162574 \nL 32.81844 263.23884 \nL 34.593305 261.322753 \nL 36.368171 259.41117 \nL 38.143036 257.502898 \nL 39.917901 255.595257 \nL 41.692766 253.694193 \nL 43.467631 251.797645 \nL 45.242496 249.90864 \nL 47.017361 248.023048 \nL 48.792226 246.147708 \nL 50.567091 244.276748 \nL 52.341956 242.395965 \nL 54.116821 240.486591 \nL 55.891686 238.564717 \nL 57.666551 236.639866 \nL 59.441416 234.722394 \nL 61.216281 232.819584 \nL 62.991146 230.923424 \nL 64.766011 229.032151 \nL 66.540876 227.147752 \nL 68.315742 225.283567 \nL 70.090607 223.431692 \nL 71.865472 221.594838 \nL 73.640337 219.768393 \nL 75.415202 217.950313 \nL 77.190067 216.132368 \nL 78.964932 214.315261 \nL 80.739797 212.498548 \nL 82.514662 210.689579 \nL 84.289527 208.889048 \nL 86.064392 207.11073 \nL 87.839257 205.359715 \nL 89.614122 203.626253 \nL 91.388987 201.909092 \nL 93.163852 200.21196 \nL 94.938717 198.533633 \nL 96.713582 196.86284 \nL 98.488447 195.201958 \nL 100.263313 193.557316 \nL 102.038178 191.927608 \nL 103.813043 190.31135 \nL 105.587908 188.709469 \nL 107.362773 187.120628 \nL 109.137638 185.54372 \nL 110.912503 183.981178 \nL 112.687368 182.432836 \nL 114.462233 180.901176 \nL 116.237098 179.386914 \nL 118.011963 177.944164 \nL 119.786828 176.634971 \nL 121.561693 175.406811 \nL 123.336558 174.406384 \nL 125.111423 173.685843 \nL 126.886288 172.988094 \nL 128.661153 172.309127 \nL 130.436018 171.651112 \nL 132.210884 171.018243 \nL 133.985749 170.410024 \nL 135.760614 169.834929 \nL 137.535479 169.310096 \nL 139.310344 168.954383 \nL 141.085209 169.268128 \nL 142.860074 171.502712 \nL 144.634939 174.891145 \nL 146.409804 178.456309 \nL 148.184669 182.070819 \nL 149.959534 185.725579 \nL 151.734399 189.426201 \nL 153.509264 193.170032 \nL 155.284129 196.95842 \nL 157.058994 200.792031 \nL 158.833859 204.671208 \nL 160.608724 208.602276 \nL 162.38359 212.628419 \nL 164.158455 216.706758 \nL 165.93332 220.799857 \nL 167.708185 224.907216 \nL 169.48305 229.029918 \nL 171.257915 233.169837 \nL 173.03278 237.325381 \nL 174.807645 241.498873 \nL 176.58251 245.690148 \nL 178.357375 249.898136 \nL 180.13224 254.120808 \nL 181.907105 258.348366 \nL 183.68197 261.728674 \nL 185.456835 263.912459 \nL 187.2317 266.062211 \nL 189.006565 268.215269 \nL 190.78143 270.364881 \nL 192.556295 272.499232 \nL 194.331161 274.283349 \nL 196.106026 274.928865 \nL 197.880891 274.962649 \nL 199.655756 274.802399 \nL 201.430621 274.638228 \nL 203.205486 274.467582 \nL 204.980351 274.302477 \nL 206.755216 274.142696 \nL 208.530081 273.989677 \nL 210.304946 273.838321 \nL 212.079811 273.692024 \nL 213.854676 273.541635 \nL 215.629541 273.269482 \nL 217.404406 272.196417 \nL 219.179271 269.63496 \nL 220.954136 265.832029 \nL 222.729001 261.596148 \nL 224.503866 257.19506 \nL 226.278732 252.857138 \nL 228.053597 248.611538 \nL 229.828462 244.440238 \nL 231.603327 240.376305 \nL 233.378192 236.497004 \nL 235.153057 232.818801 \nL 236.927922 229.357058 \nL 238.702787 226.267308 \nL 240.477652 223.606538 \nL 242.252517 221.446125 \nL 244.027382 219.883248 \nL 245.802247 219.137294 \nL 247.577112 219.282902 \nL 249.351977 220.57874 \nL 251.126842 223.366508 \nL 252.901707 227.972884 \nL 254.676572 234.995878 \nL 256.451437 245.348079 \nL 258.226303 260.844254 \nL 260.001168 281.482957 \nL 261.776033 308.090922 \nL 263.550898 342.7649 \nL 265.325763 387.492556 \nL 267.100628 444.605733 \nL 268.875493 514.107097 \nL 270.650358 601.985449 \nL 272.425223 712.648978 \nL 274.200088 850.574694 \nL 275.974953 1025.434795 \nL 277.749818 1246.595269 \nL 279.524683 1521.129057 \nL 281.299548 1873.875089 \nL 283.074413 2313.667888 \nL 284.849278 2863.671898 \nL 286.624143 3541.65017 \nL 288.399009 4395.677793 \nL 290.173874 5532.283659 \nL 291.948739 7014.677013 \nL 293.723604 8950.159189 \nL 295.498469 11454.173093 \nL 297.273334 14639.309988 \nL 299.048199 18610.149173 \nL 300.823064 23580.597943 \nL 302.597929 29934.337677 \nL 304.372794 38007.824005 \nL 306.147659 47509.656604 \nL 307.922524 59499.016052 \nL 309.697389 74759.978882 \nL 311.472254 94659.275732 \nL 313.247119 120227.442896 \nL 315.021984 153273.328394 \nL 316.796849 197044.663525 \nL 318.571714 226387.163428 \nL 320.34658 226380.041064 \nL 322.121445 226372.94082 \nL 323.89631 226365.840576 \nL 325.671175 226358.762451 \nL 327.44604 226351.640088 \nL 329.220905 226344.473486 \nL 330.99577 226337.351123 \nL 332.770635 226330.22876 \nL 334.5455 226323.128516 \nL 336.320365 226316.050391 \nL 338.09523 226308.950146 \nL 339.870095 226301.849902 \nL 341.64496 226294.727539 \nL 343.419825 226287.560938 \nL 345.19469 226280.394336 \nL 346.969555 226273.205615 \nL 348.74442 226265.994775 \nL 348.74442 226265.950537 \nL 348.74442 226265.950537 \nL 346.969555 226273.161377 \nL 345.19469 226280.350098 \nL 343.419825 226287.516699 \nL 341.64496 226294.683301 \nL 339.870095 226301.805664 \nL 338.09523 226308.905908 \nL 336.320365 226316.006152 \nL 334.5455 226323.084277 \nL 332.770635 226330.184521 \nL 330.99577 226337.306885 \nL 329.220905 226344.429248 \nL 327.44604 226351.59585 \nL 325.671175 226358.740332 \nL 323.89631 226365.818457 \nL 322.121445 226372.918701 \nL 320.34658 226380.018945 \nL 318.571714 226387.141309 \nL 316.796849 197044.641406 \nL 315.021984 153273.295215 \nL 313.247119 120227.409717 \nL 311.472254 94659.231494 \nL 309.697389 74759.923584 \nL 307.922524 59498.949695 \nL 306.147659 47509.579187 \nL 304.372794 38007.735529 \nL 302.597929 29934.229846 \nL 300.823064 23580.470758 \nL 299.048199 18610.001251 \nL 297.273334 14639.1358 \nL 295.498469 11453.967108 \nL 293.723604 8949.915878 \nL 291.948739 7014.390846 \nL 290.173874 5531.949452 \nL 288.399009 4395.290708 \nL 286.624143 3541.208479 \nL 284.849278 2863.169724 \nL 283.074413 2313.096592 \nL 281.299548 1873.226722 \nL 279.524683 1520.394632 \nL 277.749818 1245.770295 \nL 275.974953 1024.511321 \nL 274.200088 849.548919 \nL 272.425223 711.519692 \nL 270.650358 600.746215 \nL 268.875493 512.75312 \nL 267.100628 443.139821 \nL 265.325763 385.903607 \nL 263.550898 341.046411 \nL 261.776033 306.24486 \nL 260.001168 279.513446 \nL 258.226303 258.75797 \nL 256.451437 243.159958 \nL 254.676572 232.756413 \nL 252.901707 225.714226 \nL 251.126842 221.110129 \nL 249.351977 218.347008 \nL 247.577112 217.108395 \nL 245.802247 217.036494 \nL 244.027382 217.85595 \nL 242.252517 219.508063 \nL 240.477652 221.769118 \nL 238.702787 224.529417 \nL 236.927922 227.737868 \nL 235.153057 231.328278 \nL 233.378192 235.129911 \nL 231.603327 239.137922 \nL 229.828462 243.324666 \nL 228.053597 247.601742 \nL 226.278732 251.944119 \nL 224.503866 256.360355 \nL 222.729001 260.822362 \nL 220.954136 265.122461 \nL 219.179271 269.025777 \nL 217.404406 271.73178 \nL 215.629541 272.870938 \nL 213.854676 273.18397 \nL 212.079811 273.364336 \nL 210.304946 273.536256 \nL 208.530081 273.709737 \nL 206.755216 273.884578 \nL 204.980351 274.064059 \nL 203.205486 274.247708 \nL 201.430621 274.436855 \nL 199.655756 274.616006 \nL 197.880891 274.789303 \nL 196.106026 274.776246 \nL 194.331161 274.172451 \nL 192.556295 272.401575 \nL 190.78143 270.272948 \nL 189.006565 268.129098 \nL 187.2317 265.981716 \nL 185.456835 263.83718 \nL 183.68197 261.65979 \nL 181.907105 258.289121 \nL 180.13224 254.064419 \nL 178.357375 249.844442 \nL 176.58251 245.638857 \nL 174.807645 241.449748 \nL 173.03278 237.278232 \nL 171.257915 233.124292 \nL 169.48305 228.985612 \nL 167.708185 224.863813 \nL 165.93332 220.757064 \nL 164.158455 216.664206 \nL 162.38359 212.585974 \nL 160.608724 208.56062 \nL 158.833859 204.627905 \nL 157.058994 200.746633 \nL 155.284129 196.910325 \nL 153.509264 193.11861 \nL 151.734399 189.370555 \nL 149.959534 185.664549 \nL 148.184669 182.003677 \nL 146.409804 178.382003 \nL 144.634939 174.810507 \nL 142.860074 171.43021 \nL 141.085209 169.229566 \nL 139.310344 168.931724 \nL 137.535479 169.291322 \nL 135.760614 169.816171 \nL 133.985749 170.390335 \nL 132.210884 170.996788 \nL 130.436018 171.627772 \nL 128.661153 172.28367 \nL 126.886288 172.960059 \nL 125.111423 173.654633 \nL 123.336558 174.370959 \nL 121.561693 175.362905 \nL 119.786828 176.583338 \nL 118.011963 177.881131 \nL 116.237098 179.306654 \nL 114.462233 180.805742 \nL 112.687368 182.319833 \nL 110.912503 183.847181 \nL 109.137638 185.385573 \nL 107.362773 186.934419 \nL 105.587908 188.490003 \nL 103.813043 190.052851 \nL 102.038178 191.62419 \nL 100.263313 193.202943 \nL 98.488447 194.790659 \nL 96.713582 196.389062 \nL 94.938717 197.990247 \nL 93.163852 199.587622 \nL 91.388987 201.188504 \nL 89.614122 202.793251 \nL 87.839257 204.401247 \nL 86.064392 206.01331 \nL 84.289527 207.633991 \nL 82.514662 209.256412 \nL 80.739797 210.878195 \nL 78.964932 212.504816 \nL 77.190067 214.130475 \nL 75.415202 215.758671 \nL 73.640337 217.382267 \nL 71.865472 219.00519 \nL 70.090607 220.632279 \nL 68.315742 222.26722 \nL 66.540876 223.907979 \nL 64.766011 225.559567 \nL 62.991146 227.212009 \nL 61.216281 228.865839 \nL 59.441416 230.521594 \nL 57.666551 232.188046 \nL 55.891686 233.859994 \nL 54.116821 235.534738 \nL 52.341956 237.205535 \nL 50.567091 238.858323 \nL 48.792226 240.497075 \nL 47.017361 242.13516 \nL 45.242496 243.773527 \nL 43.467631 245.408966 \nL 41.692766 247.047024 \nL 39.917901 248.683349 \nL 38.143036 250.320225 \nL 36.368171 251.95729 \nL 34.593305 253.59544 \nL 32.81844 255.235508 \nL 31.043575 256.880694 \nL 29.26871 258.530169 \nL 27.493845 260.18102 \nL 25.71898 261.827811 \nL 23.944115 263.477193 \nL 22.16925 265.127629 \nL 20.394385 266.783514 \nL 18.61952 268.447903 \nL 16.844655 270.121387 \nL 15.06979 271.795875 \nL 13.294925 273.47059 \nL 11.52006 275.14457 \nL 9.745195 276.822298 \nL 7.97033 278.505173 \nL 6.195465 280.184273 \nL 4.4206 281.862336 \nL 2.645734 283.541468 \nL 0.870869 285.223408 \nL -0.903996 286.913033 \nL -2.678861 288.601389 \nL -4.453726 290.287028 \nz\n\" style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#1f77b4;stroke-opacity:0.3;\"/>\n   </g>\n   <g id=\"PolyCollection_3\">\n    <path clip-path=\"url(#p69f385ce35)\" d=\"M -4.453726 290.017041 \nL -4.453726 290.287028 \nL -2.678861 288.601389 \nL -0.903996 286.913033 \nL 0.870869 285.223408 \nL 2.645734 283.541468 \nL 4.4206 281.862336 \nL 6.195465 280.184273 \nL 7.97033 278.505173 \nL 9.745195 276.822298 \nL 11.52006 275.14457 \nL 13.294925 273.47059 \nL 15.06979 271.795875 \nL 16.844655 270.121387 \nL 18.61952 268.447903 \nL 20.394385 266.783514 \nL 22.16925 265.127629 \nL 23.944115 263.477193 \nL 25.71898 261.827811 \nL 27.493845 260.18102 \nL 29.26871 258.530169 \nL 31.043575 256.880694 \nL 32.81844 255.235508 \nL 34.593305 253.59544 \nL 36.368171 251.95729 \nL 38.143036 250.320225 \nL 39.917901 248.683349 \nL 41.692766 247.047024 \nL 43.467631 245.408966 \nL 45.242496 243.773527 \nL 47.017361 242.13516 \nL 48.792226 240.497075 \nL 50.567091 238.858323 \nL 52.341956 237.205535 \nL 54.116821 235.534738 \nL 55.891686 233.859994 \nL 57.666551 232.188046 \nL 59.441416 230.521594 \nL 61.216281 228.865839 \nL 62.991146 227.212009 \nL 64.766011 225.559567 \nL 66.540876 223.907979 \nL 68.315742 222.26722 \nL 70.090607 220.632279 \nL 71.865472 219.00519 \nL 73.640337 217.382267 \nL 75.415202 215.758671 \nL 77.190067 214.130475 \nL 78.964932 212.504816 \nL 80.739797 210.878195 \nL 82.514662 209.256412 \nL 84.289527 207.633991 \nL 86.064392 206.01331 \nL 87.839257 204.401247 \nL 89.614122 202.793251 \nL 91.388987 201.188504 \nL 93.163852 199.587622 \nL 94.938717 197.990247 \nL 96.713582 196.389062 \nL 98.488447 194.790659 \nL 100.263313 193.202943 \nL 102.038178 191.62419 \nL 103.813043 190.052851 \nL 105.587908 188.490003 \nL 107.362773 186.934419 \nL 109.137638 185.385573 \nL 110.912503 183.847181 \nL 112.687368 182.319833 \nL 114.462233 180.805742 \nL 116.237098 179.306654 \nL 118.011963 177.881131 \nL 119.786828 176.583338 \nL 121.561693 175.362905 \nL 123.336558 174.370959 \nL 125.111423 173.654633 \nL 126.886288 172.960059 \nL 128.661153 172.28367 \nL 130.436018 171.627772 \nL 132.210884 170.996788 \nL 133.985749 170.390335 \nL 135.760614 169.816171 \nL 137.535479 169.291322 \nL 139.310344 168.931724 \nL 141.085209 169.229566 \nL 142.860074 171.43021 \nL 144.634939 174.810507 \nL 146.409804 178.382003 \nL 148.184669 182.003677 \nL 149.959534 185.664549 \nL 151.734399 189.370555 \nL 153.509264 193.11861 \nL 155.284129 196.910325 \nL 157.058994 200.746633 \nL 158.833859 204.627905 \nL 160.608724 208.56062 \nL 162.38359 212.585974 \nL 164.158455 216.664206 \nL 165.93332 220.757064 \nL 167.708185 224.863813 \nL 169.48305 228.985612 \nL 171.257915 233.124292 \nL 173.03278 237.278232 \nL 174.807645 241.449748 \nL 176.58251 245.638857 \nL 178.357375 249.844442 \nL 180.13224 254.064419 \nL 181.907105 258.289121 \nL 183.68197 261.65979 \nL 185.456835 263.83718 \nL 187.2317 265.981716 \nL 189.006565 268.129098 \nL 190.78143 270.272948 \nL 192.556295 272.401575 \nL 194.331161 274.172451 \nL 196.106026 274.776246 \nL 197.880891 274.789303 \nL 199.655756 274.616006 \nL 201.430621 274.436855 \nL 203.205486 274.247708 \nL 204.980351 274.064059 \nL 206.755216 273.884578 \nL 208.530081 273.709737 \nL 210.304946 273.536256 \nL 212.079811 273.364336 \nL 213.854676 273.18397 \nL 215.629541 272.870938 \nL 217.404406 271.73178 \nL 219.179271 269.025777 \nL 220.954136 265.122461 \nL 222.729001 260.822362 \nL 224.503866 256.360355 \nL 226.278732 251.944119 \nL 228.053597 247.601742 \nL 229.828462 243.324666 \nL 231.603327 239.137922 \nL 233.378192 235.129911 \nL 235.153057 231.328278 \nL 236.927922 227.737868 \nL 238.702787 224.529417 \nL 240.477652 221.769118 \nL 242.252517 219.508063 \nL 244.027382 217.85595 \nL 245.802247 217.036494 \nL 247.577112 217.108395 \nL 249.351977 218.347008 \nL 251.126842 221.110129 \nL 252.901707 225.714226 \nL 254.676572 232.756413 \nL 256.451437 243.159958 \nL 258.226303 258.75797 \nL 260.001168 279.513446 \nL 261.776033 306.24486 \nL 263.550898 341.046411 \nL 265.325763 385.903607 \nL 267.100628 443.139821 \nL 268.875493 512.75312 \nL 270.650358 600.746215 \nL 272.425223 711.519692 \nL 274.200088 849.548919 \nL 275.974953 1024.511321 \nL 277.749818 1245.770295 \nL 279.524683 1520.394632 \nL 281.299548 1873.226722 \nL 283.074413 2313.096592 \nL 284.849278 2863.169724 \nL 286.624143 3541.208479 \nL 288.399009 4395.290708 \nL 290.173874 5531.949452 \nL 291.948739 7014.390846 \nL 293.723604 8949.915878 \nL 295.498469 11453.967108 \nL 297.273334 14639.1358 \nL 299.048199 18610.001251 \nL 300.823064 23580.470758 \nL 302.597929 29934.229846 \nL 304.372794 38007.735529 \nL 306.147659 47509.579187 \nL 307.922524 59498.949695 \nL 309.697389 74759.923584 \nL 311.472254 94659.231494 \nL 313.247119 120227.409717 \nL 315.021984 153273.295215 \nL 316.796849 197044.641406 \nL 318.571714 226387.141309 \nL 320.34658 226380.018945 \nL 322.121445 226372.918701 \nL 323.89631 226365.818457 \nL 325.671175 226358.740332 \nL 327.44604 226351.59585 \nL 329.220905 226344.429248 \nL 330.99577 226337.306885 \nL 332.770635 226330.184521 \nL 334.5455 226323.084277 \nL 336.320365 226316.006152 \nL 338.09523 226308.905908 \nL 339.870095 226301.805664 \nL 341.64496 226294.683301 \nL 343.419825 226287.516699 \nL 345.19469 226280.350098 \nL 346.969555 226273.161377 \nL 348.74442 226265.950537 \nL 348.74442 -226734.049463 \nL 348.74442 -226734.049463 \nL 346.969555 -226726.838623 \nL 345.19469 -226719.649902 \nL 343.419825 -226712.483301 \nL 341.64496 -226705.316699 \nL 339.870095 -226698.194336 \nL 338.09523 -226691.094092 \nL 336.320365 -226683.993848 \nL 334.5455 -226676.915723 \nL 332.770635 -226669.815479 \nL 330.99577 -226662.693115 \nL 329.220905 -226655.570752 \nL 327.44604 -226648.40415 \nL 325.671175 -226641.259668 \nL 323.89631 -226634.181543 \nL 322.121445 -226627.081299 \nL 320.34658 -226619.981055 \nL 318.571714 -226612.858691 \nL 316.796849 -197256.202539 \nL 315.021984 -153470.722217 \nL 313.247119 -120410.746826 \nL 311.472254 -94828.522949 \nL 309.697389 -74915.125146 \nL 307.922524 -59640.127722 \nL 306.147659 -47636.766858 \nL 304.372794 -38120.899664 \nL 302.597929 -30033.370447 \nL 300.823064 -23665.593353 \nL 299.048199 -18681.125195 \nL 297.273334 -14696.335745 \nL 295.498469 -11497.341208 \nL 293.723604 -8979.537402 \nL 291.948739 -7030.190672 \nL 290.173874 -5533.92758 \nL 288.399009 -4383.458198 \nL 286.624143 -3515.648277 \nL 284.849278 -2823.980676 \nL 283.074413 -2260.363026 \nL 281.299548 -1806.946218 \nL 279.524683 -1440.570649 \nL 277.749818 -1152.551443 \nL 275.974953 -918.046561 \nL 274.200088 -729.860714 \nL 272.425223 -578.631977 \nL 270.650358 -454.790321 \nL 268.875493 -353.730949 \nL 267.100628 -271.146761 \nL 265.325763 -200.989772 \nL 263.550898 -143.198635 \nL 261.776033 -95.535127 \nL 260.001168 -55.934521 \nL 258.226303 -22.275065 \nL 256.451437 6.273726 \nL 254.676572 29.607356 \nL 252.901707 49.557579 \nL 251.126842 67.000141 \nL 249.351977 82.555384 \nL 247.577112 96.477201 \nL 245.802247 109.131574 \nL 244.027382 120.718926 \nL 242.252517 131.443939 \nL 240.477652 141.497574 \nL 238.702787 150.950936 \nL 236.927922 159.838662 \nL 235.153057 168.233073 \nL 233.378192 176.320468 \nL 231.603327 184.133085 \nL 229.828462 191.747237 \nL 228.053597 199.170475 \nL 226.278732 206.42812 \nL 224.503866 213.530579 \nL 222.729001 220.453021 \nL 220.954136 226.898607 \nL 219.179271 232.553495 \nL 217.404406 236.312804 \nL 215.629541 237.694397 \nL 213.854676 237.849399 \nL 212.079811 237.78218 \nL 210.304946 237.702846 \nL 208.530081 237.617186 \nL 206.755216 237.530305 \nL 204.980351 237.438564 \nL 203.205486 237.340853 \nL 201.430621 237.247011 \nL 199.655756 237.137574 \nL 197.880891 237.020115 \nL 196.106026 236.767256 \nL 194.331161 236.06205 \nL 192.556295 234.440281 \nL 190.78143 232.528479 \nL 189.006565 230.604468 \nL 187.2317 228.678037 \nL 185.456835 226.751375 \nL 183.68197 224.805801 \nL 181.907105 222.114845 \nL 180.13224 218.870181 \nL 178.357375 215.607246 \nL 176.58251 212.332551 \nL 174.807645 209.047761 \nL 173.03278 205.752116 \nL 171.257915 202.447258 \nL 169.48305 199.133569 \nL 167.708185 195.812096 \nL 165.93332 192.481844 \nL 164.158455 189.143046 \nL 162.38359 185.812491 \nL 160.608724 182.855184 \nL 158.833859 180.358993 \nL 157.058994 177.880958 \nL 155.284129 175.364384 \nL 153.509264 172.813679 \nL 151.734399 170.232185 \nL 149.959534 167.62255 \nL 148.184669 164.990759 \nL 146.409804 162.336699 \nL 144.634939 159.671732 \nL 142.860074 157.111484 \nL 141.085209 155.814042 \nL 139.310344 156.543585 \nL 137.535479 157.744418 \nL 135.760614 159.030104 \nL 133.985749 160.310762 \nL 132.210884 161.575881 \nL 130.436018 162.821536 \nL 128.661153 164.051343 \nL 126.886288 165.266438 \nL 125.111423 166.463833 \nL 123.336558 167.649581 \nL 121.561693 169.072515 \nL 119.786828 170.689305 \nL 118.011963 172.360769 \nL 116.237098 174.142764 \nL 114.462233 175.9744 \nL 112.687368 177.798413 \nL 110.912503 179.614716 \nL 109.137638 181.422372 \nL 107.362773 183.221355 \nL 105.587908 185.010135 \nL 103.813043 186.790215 \nL 102.038178 188.563494 \nL 100.263313 190.330547 \nL 98.488447 192.093927 \nL 96.713582 193.8565 \nL 94.938717 195.610787 \nL 93.163852 197.351253 \nL 91.388987 199.085933 \nL 89.614122 200.815944 \nL 87.839257 202.540803 \nL 86.064392 204.262014 \nL 84.289527 205.984481 \nL 82.514662 207.701811 \nL 80.739797 209.411537 \nL 78.964932 211.120471 \nL 77.190067 212.823069 \nL 75.415202 214.522994 \nL 73.640337 216.213951 \nL 71.865472 217.900151 \nL 70.090607 219.586585 \nL 68.315742 221.276616 \nL 66.540876 222.968369 \nL 64.766011 224.667538 \nL 62.991146 226.364386 \nL 61.216281 228.059689 \nL 59.441416 229.754142 \nL 57.666551 231.45677 \nL 55.891686 233.162377 \nL 54.116821 234.868588 \nL 52.341956 236.568735 \nL 50.567091 238.249031 \nL 48.792226 239.913456 \nL 47.017361 241.575437 \nL 45.242496 243.236057 \nL 43.467631 244.892207 \nL 41.692766 246.549592 \nL 39.917901 248.203878 \nL 38.143036 249.857581 \nL 36.368171 251.510382 \nL 34.593305 253.163383 \nL 32.81844 254.817393 \nL 31.043575 256.475616 \nL 29.26871 258.13736 \nL 27.493845 259.799281 \nL 25.71898 261.456408 \nL 23.944115 263.115479 \nL 22.16925 264.775008 \nL 20.394385 266.439511 \nL 18.61952 268.112034 \nL 16.844655 269.793164 \nL 15.06979 271.474845 \nL 13.294925 273.156332 \nL 11.52006 274.836695 \nL 9.745195 276.520461 \nL 7.97033 278.209081 \nL 6.195465 279.893613 \nL 4.4206 281.576817 \nL 2.645734 283.260129 \nL 0.870869 284.945245 \nL -0.903996 286.637808 \nL -2.678861 288.328885 \nL -4.453726 290.017041 \nz\n\" style=\"fill:#ff7f0e;fill-opacity:0.4;stroke:#ff7f0e;stroke-opacity:0.4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p69f385ce35)\" d=\"M 44.18 317.7125 \nL 44.18 45.9125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m78422f5daa\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.18\" xlink:href=\"#m78422f5daa\" y=\"317.7125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −4 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(22.066719 347.507813)scale(0.3 -0.3)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p69f385ce35)\" d=\"M 111.14 317.7125 \nL 111.14 45.9125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"111.14\" xlink:href=\"#m78422f5daa\" y=\"317.7125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(89.026719 347.507813)scale(0.3 -0.3)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p69f385ce35)\" d=\"M 178.1 317.7125 \nL 178.1 45.9125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.1\" xlink:href=\"#m78422f5daa\" y=\"317.7125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(168.55625 347.507813)scale(0.3 -0.3)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p69f385ce35)\" d=\"M 245.06 317.7125 \nL 245.06 45.9125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"245.06\" xlink:href=\"#m78422f5daa\" y=\"317.7125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 2 -->\n      <g transform=\"translate(235.51625 347.507813)scale(0.3 -0.3)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p69f385ce35)\" d=\"M 312.02 317.7125 \nL 312.02 45.9125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"312.02\" xlink:href=\"#m78422f5daa\" y=\"317.7125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 4 -->\n      <g transform=\"translate(302.47625 347.507813)scale(0.3 -0.3)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- $x$ -->\n     <defs>\n      <path d=\"M 60.015625 54.6875 \nL 34.90625 27.875 \nL 50.296875 0 \nL 39.984375 0 \nL 28.421875 21.6875 \nL 8.296875 0 \nL -2.59375 0 \nL 24.3125 28.8125 \nL 10.015625 54.6875 \nL 20.3125 54.6875 \nL 30.8125 34.90625 \nL 49.125 54.6875 \nz\n\" id=\"DejaVuSans-Oblique-120\"/>\n     </defs>\n     <g transform=\"translate(169.1 380.542188)scale(0.3 -0.3)\">\n      <use transform=\"translate(0 0.3125)\" xlink:href=\"#DejaVuSans-Oblique-120\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p69f385ce35)\" d=\"M 10.7 295.0625 \nL 345.5 295.0625 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m88476a98d0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"10.7\" xlink:href=\"#m88476a98d0\" y=\"295.0625\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p69f385ce35)\" d=\"M 10.7 249.7625 \nL 345.5 249.7625 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"10.7\" xlink:href=\"#m88476a98d0\" y=\"249.7625\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p69f385ce35)\" d=\"M 10.7 204.4625 \nL 345.5 204.4625 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"10.7\" xlink:href=\"#m88476a98d0\" y=\"204.4625\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p69f385ce35)\" d=\"M 10.7 159.1625 \nL 345.5 159.1625 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"10.7\" xlink:href=\"#m88476a98d0\" y=\"159.1625\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p69f385ce35)\" d=\"M 10.7 113.8625 \nL 345.5 113.8625 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"10.7\" xlink:href=\"#m88476a98d0\" y=\"113.8625\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p69f385ce35)\" d=\"M 10.7 68.5625 \nL 345.5 68.5625 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-opacity:0.3;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"10.7\" xlink:href=\"#m88476a98d0\" y=\"68.5625\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_23\">\n    <path clip-path=\"url(#p69f385ce35)\" d=\"M -1 286.866819 \nL 22.16925 264.951319 \nL 68.315742 221.771918 \nL 112.687368 180.059123 \nL 118.011963 175.12095 \nL 121.561693 172.21771 \nL 123.336558 171.01027 \nL 137.535479 163.51787 \nL 139.310344 162.737655 \nL 141.085209 162.521804 \nL 142.860074 164.270847 \nL 146.409804 170.359351 \nL 158.833859 192.493449 \nL 160.608724 195.707902 \nL 164.158455 202.903625 \nL 181.907105 240.201982 \nL 183.68197 243.232796 \nL 192.556295 253.420928 \nL 194.331161 255.117249 \nL 196.106026 255.771751 \nL 197.880891 255.904709 \nL 212.079811 255.573257 \nL 213.854676 255.516684 \nL 215.629541 255.282667 \nL 217.404406 254.022292 \nL 219.179271 250.789636 \nL 220.954136 246.010533 \nL 224.503866 234.945468 \nL 231.603327 211.635504 \nL 238.702787 187.740177 \nL 245.802247 163.084033 \nL 252.901707 137.635905 \nL 274.200088 59.844092 \nL 283.074413 26.366751 \nL 290.176643 -1 \nL 290.176643 -1 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 10.7 317.7125 \nL 10.7 45.9125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 345.5 317.7125 \nL 345.5 45.9125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 10.7 317.7125 \nL 345.5 317.7125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 10.7 45.9125 \nL 345.5 45.9125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_7\">\n    <!-- BBP Gaussian -->\n    <defs>\n     <path d=\"M 19.671875 34.8125 \nL 19.671875 8.109375 \nL 35.5 8.109375 \nQ 43.453125 8.109375 47.28125 11.40625 \nQ 51.125 14.703125 51.125 21.484375 \nQ 51.125 28.328125 47.28125 31.5625 \nQ 43.453125 34.8125 35.5 34.8125 \nz\nM 19.671875 64.796875 \nL 19.671875 42.828125 \nL 34.28125 42.828125 \nQ 41.5 42.828125 45.03125 45.53125 \nQ 48.578125 48.25 48.578125 53.8125 \nQ 48.578125 59.328125 45.03125 62.0625 \nQ 41.5 64.796875 34.28125 64.796875 \nz\nM 9.8125 72.90625 \nL 35.015625 72.90625 \nQ 46.296875 72.90625 52.390625 68.21875 \nQ 58.5 63.53125 58.5 54.890625 \nQ 58.5 48.1875 55.375 44.234375 \nQ 52.25 40.28125 46.1875 39.3125 \nQ 53.46875 37.75 57.5 32.78125 \nQ 61.53125 27.828125 61.53125 20.40625 \nQ 61.53125 10.640625 54.890625 5.3125 \nQ 48.25 0 35.984375 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-66\"/>\n     <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 59.515625 10.40625 \nL 59.515625 29.984375 \nL 43.40625 29.984375 \nL 43.40625 38.09375 \nL 69.28125 38.09375 \nL 69.28125 6.78125 \nQ 63.578125 2.734375 56.6875 0.65625 \nQ 49.8125 -1.421875 42 -1.421875 \nQ 24.90625 -1.421875 15.25 8.5625 \nQ 5.609375 18.5625 5.609375 36.375 \nQ 5.609375 54.25 15.25 64.234375 \nQ 24.90625 74.21875 42 74.21875 \nQ 49.125 74.21875 55.546875 72.453125 \nQ 61.96875 70.703125 67.390625 67.28125 \nL 67.390625 56.78125 \nQ 61.921875 61.421875 55.765625 63.765625 \nQ 49.609375 66.109375 42.828125 66.109375 \nQ 29.4375 66.109375 22.71875 58.640625 \nQ 16.015625 51.171875 16.015625 36.375 \nQ 16.015625 21.625 22.71875 14.15625 \nQ 29.4375 6.6875 42.828125 6.6875 \nQ 48.046875 6.6875 52.140625 7.59375 \nQ 56.25 8.5 59.515625 10.40625 \nz\n\" id=\"DejaVuSans-71\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n    </defs>\n    <g transform=\"translate(40.4875 37.59375)scale(0.4 -0.4)\">\n     <use xlink:href=\"#DejaVuSans-66\"/>\n     <use x=\"68.603516\" xlink:href=\"#DejaVuSans-66\"/>\n     <use x=\"137.207031\" xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"197.509766\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"229.296875\" xlink:href=\"#DejaVuSans-71\"/>\n     <use x=\"306.787109\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"368.066406\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"431.445312\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"483.544922\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"535.644531\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"563.427734\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"624.707031\" xlink:href=\"#DejaVuSans-110\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p69f385ce35\">\n   <rect height=\"271.8\" width=\"334.8\" x=\"10.7\" y=\"45.9125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "text/plain": [
              "<Figure size 432x360 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "samples, noises = [], []\n",
        "for i in range(100):\n",
        "    preds = best_net.forward(torch.linspace(-5, 5, 200).cuda())[0]\n",
        "    samples.append(preds[:, 0].cpu().data.numpy()* y_std + y_mean)\n",
        "    noises.append(preds[:, 1].exp().cpu().data.numpy()* y_std)\n",
        "\n",
        "samples = np.array(samples)\n",
        "noises = np.array(noises)\n",
        "means = samples.mean(axis = 0)\n",
        "\n",
        "aleatoric = (noises**2).mean(axis = 0)**0.5\n",
        "epistemic = samples.var(axis = 0)**0.5\n",
        "aleatoric = np.minimum(aleatoric, 10e3)\n",
        "epistemic = np.minimum(epistemic, 10e3)\n",
        "\n",
        "total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
        "\n",
        "c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
        "     '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
        "\n",
        "x_mean, x_std = x_train.mean(), x_train.var()**0.5\n",
        "plt.figure(figsize = (6, 5))\n",
        "plt.style.use('default')\n",
        "plt.scatter(x_train * x_std + x_mean, y_train * y_std + y_mean, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n",
        "plt.fill_between(np.linspace(-5, 5, 200)* x_std + x_mean, means + aleatoric, means + total_unc, color = c[0], alpha = 0.3, label = 'Epistemic + Aleatoric')\n",
        "plt.fill_between(np.linspace(-5, 5, 200)* x_std + x_mean, means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
        "plt.fill_between(np.linspace(-5, 5, 200)* x_std + x_mean, means - aleatoric, means + aleatoric, color = c[1], alpha = 0.4, label = 'Aleatoric')\n",
        "plt.plot(np.linspace(-5, 5, 200)* x_std + x_mean, means, color = 'black', linewidth = 1)\n",
        "plt.xlim([-5, 5])\n",
        "plt.ylim([-5, 7])\n",
        "plt.xlabel('$x$', fontsize=30)\n",
        "plt.title('BBP Gaussian', fontsize=40)\n",
        "plt.tick_params(labelsize=30)\n",
        "plt.xticks(np.arange(-4, 5, 2))\n",
        "plt.yticks(np.arange(-4, 7, 2))\n",
        "plt.gca().set_yticklabels([])\n",
        "plt.gca().yaxis.grid(alpha=0.3)\n",
        "plt.gca().xaxis.grid(alpha=0.3)\n",
        "plt.savefig('bbp_hetero.pdf', bbox_inches = 'tight')\n",
        "\n",
        "files.download(\"bbp_hetero.pdf\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dTuOxp7CrmZ"
      },
      "source": [
        "UCI dataset fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN9rvBjyVKxt"
      },
      "outputs": [],
      "source": [
        "class BBP_Heteroscedastic_Model_UCI(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(BBP_Heteroscedastic_Model_UCI, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = BayesLinear_Normalq(input_dim, num_units, gaussian(0, 1))\n",
        "        self.layer2 = BayesLinear_Normalq(num_units, num_units, gaussian(0, 1))\n",
        "        self.layer3 = BayesLinear_Normalq(num_units, 2*output_dim, gaussian(0, 1))\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        KL_loss_total = 0\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x, KL_loss = self.layer1(x)\n",
        "        KL_loss_total = KL_loss_total + KL_loss\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x, KL_loss = self.layer2(x)\n",
        "        KL_loss_total = KL_loss_total + KL_loss\n",
        "        \n",
        "        return x, KL_loss_total\n",
        "\n",
        "def train_BBP(data, n_splits, num_epochs, num_units, learn_rate, log_every):\n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for i, idx in enumerate(kf.split(data)):\n",
        "        print('FOLD %d:' % i)\n",
        "\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        x_train = (x_train - x_means)/x_stds\n",
        "        y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        x_test = (x_test - x_means)/x_stds\n",
        "        y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        batch_size, nb_train = len(x_train), len(x_train)\n",
        "\n",
        "        net = BBP_Heteroscedastic_Model_Wrapper(network=BBP_Heteroscedastic_Model_UCI(input_dim=x_test.shape[-1], output_dim=1, num_units=num_units),\n",
        "                                                learn_rate=1e-2, batch_size=batch_size, no_batches=1)\n",
        "\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "        KL_loss_train = np.zeros(num_epochs)\n",
        "        total_loss = np.zeros(num_epochs)\n",
        "\n",
        "        best_net, best_loss = None, float('inf')\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "\n",
        "            fit_loss, KL_loss = net.fit(x_train, y_train, no_samples = 20)\n",
        "            fit_loss_train[i] += fit_loss.cpu().data.numpy()\n",
        "            KL_loss_train[i] += KL_loss.cpu().data.numpy()\n",
        "\n",
        "            total_loss[i] = fit_loss_train[i] + KL_loss_train[i]\n",
        "\n",
        "            if fit_loss < best_loss:\n",
        "                best_loss = fit_loss\n",
        "                best_net = copy.deepcopy(net.network)\n",
        "\n",
        "            if i % log_every == 0 or i == num_epochs - 1:\n",
        "\n",
        "                train_losses, train_rmse = net.get_loss_and_rmse(x_train, y_train, 20)\n",
        "                test_losses, test_rmse = net.get_loss_and_rmse(x_test, y_test, 20)\n",
        "\n",
        "                print('Epoch: %s/%d, Train loglik = %.3f, Test loglik = %.3f, Train RMSE = %.3f, Test RMSE = %.3f' %\\\n",
        "                      (str(i+1).zfill(3), num_epochs, -train_losses.mean() - np.log(y_stds)[0],\n",
        "                       -test_losses.mean() - np.log(y_stds)[0], y_stds*train_rmse, y_stds*test_rmse))\n",
        "\n",
        "\n",
        "        train_losses, train_rmse = net.get_loss_and_rmse(x_train, y_train, 20)\n",
        "        test_losses, test_rmse = net.get_loss_and_rmse(x_test, y_test, 20)\n",
        "\n",
        "        train_logliks.append((train_losses.cpu().data.numpy().mean() + np.log(y_stds)[0]))\n",
        "        test_logliks.append((test_losses.cpu().data.numpy().mean() + np.log(y_stds)[0]))\n",
        "\n",
        "        train_rmses.append(y_stds*train_rmse)\n",
        "        test_rmses.append(y_stds*test_rmse)\n",
        "\n",
        "    print('Train log. lik. = %6.3f +/- %6.3f' % (-np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %6.3f +/- %6.3f' % (-np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %6.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %6.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "\n",
        "    return best_net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXpA9VBGMsxc"
      },
      "source": [
        "Housing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOq4tM_PLRUV",
        "outputId": "eef68dc6-e158-44e1-eedd-a0f8856877fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-07 09:23:22--  https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49082 (48K) [application/x-httpd-php]\n",
            "Saving to: ‘housing.data’\n",
            "\n",
            "housing.data        100%[===================>]  47.93K   161KB/s    in 0.3s    \n",
            "\n",
            "2021-12-07 09:23:23 (161 KB/s) - ‘housing.data’ saved [49082/49082]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\" --no-check-certificate \n",
        "data = pd.read_csv('housing.data', header=0, delimiter=\"\\s+\").values\n",
        "data = data[np.random.permutation(np.arange(len(data)))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVdx2IEbV3Kt",
        "outputId": "d741033a-3ace-4942-e859-09e0df031e22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOLD 0:\n",
            "Epoch: 001/100, Train loglik = -3.635, Test loglik = -3.724, Train RMSE = 9.100, Test RMSE = 9.673\n",
            "Epoch: 011/100, Train loglik = -3.304, Test loglik = -3.319, Train RMSE = 5.371, Test RMSE = 6.651\n",
            "Epoch: 021/100, Train loglik = -2.826, Test loglik = -2.754, Train RMSE = 4.163, Test RMSE = 2.919\n",
            "Epoch: 031/100, Train loglik = -2.676, Test loglik = -2.459, Train RMSE = 4.143, Test RMSE = 2.974\n",
            "Epoch: 041/100, Train loglik = -2.466, Test loglik = -2.722, Train RMSE = 3.910, Test RMSE = 2.600\n",
            "Epoch: 051/100, Train loglik = -2.462, Test loglik = -2.393, Train RMSE = 3.817, Test RMSE = 2.501\n",
            "Epoch: 061/100, Train loglik = -2.451, Test loglik = -2.623, Train RMSE = 3.823, Test RMSE = 2.805\n",
            "Epoch: 071/100, Train loglik = -2.326, Test loglik = -2.263, Train RMSE = 3.692, Test RMSE = 2.409\n",
            "Epoch: 081/100, Train loglik = -2.228, Test loglik = -2.422, Train RMSE = 3.676, Test RMSE = 2.535\n",
            "Epoch: 091/100, Train loglik = -2.343, Test loglik = -2.210, Train RMSE = 3.581, Test RMSE = 2.201\n",
            "Epoch: 100/100, Train loglik = -2.147, Test loglik = -2.297, Train RMSE = 3.522, Test RMSE = 2.291\n",
            "FOLD 1:\n",
            "Epoch: 001/100, Train loglik = -3.608, Test loglik = -3.608, Train RMSE = 9.118, Test RMSE = 9.119\n",
            "Epoch: 011/100, Train loglik = -3.168, Test loglik = -3.097, Train RMSE = 5.363, Test RMSE = 4.690\n",
            "Epoch: 021/100, Train loglik = -2.725, Test loglik = -3.034, Train RMSE = 4.091, Test RMSE = 3.619\n",
            "Epoch: 031/100, Train loglik = -2.546, Test loglik = -2.842, Train RMSE = 3.983, Test RMSE = 3.586\n",
            "Epoch: 041/100, Train loglik = -2.508, Test loglik = -2.601, Train RMSE = 3.784, Test RMSE = 3.277\n",
            "Epoch: 051/100, Train loglik = -2.635, Test loglik = -2.535, Train RMSE = 3.694, Test RMSE = 3.389\n",
            "Epoch: 061/100, Train loglik = -2.367, Test loglik = -2.555, Train RMSE = 3.700, Test RMSE = 3.257\n",
            "Epoch: 071/100, Train loglik = -2.342, Test loglik = -2.703, Train RMSE = 3.688, Test RMSE = 3.339\n",
            "Epoch: 081/100, Train loglik = -2.217, Test loglik = -2.533, Train RMSE = 3.662, Test RMSE = 3.176\n",
            "Epoch: 091/100, Train loglik = -2.191, Test loglik = -2.527, Train RMSE = 3.548, Test RMSE = 3.134\n",
            "Epoch: 100/100, Train loglik = -2.231, Test loglik = -2.625, Train RMSE = 3.551, Test RMSE = 3.135\n",
            "FOLD 2:\n",
            "Epoch: 001/100, Train loglik = -3.652, Test loglik = -3.520, Train RMSE = 9.320, Test RMSE = 7.813\n",
            "Epoch: 011/100, Train loglik = -3.085, Test loglik = -3.353, Train RMSE = 5.133, Test RMSE = 5.645\n",
            "Epoch: 021/100, Train loglik = -2.753, Test loglik = -2.873, Train RMSE = 4.038, Test RMSE = 5.382\n",
            "Epoch: 031/100, Train loglik = -2.639, Test loglik = -3.250, Train RMSE = 3.811, Test RMSE = 4.809\n",
            "Epoch: 041/100, Train loglik = -2.553, Test loglik = -2.576, Train RMSE = 3.622, Test RMSE = 4.644\n",
            "Epoch: 051/100, Train loglik = -2.488, Test loglik = -2.677, Train RMSE = 3.505, Test RMSE = 4.685\n",
            "Epoch: 061/100, Train loglik = -2.598, Test loglik = -2.776, Train RMSE = 3.358, Test RMSE = 4.529\n",
            "Epoch: 071/100, Train loglik = -2.395, Test loglik = -2.583, Train RMSE = 3.312, Test RMSE = 4.427\n",
            "Epoch: 081/100, Train loglik = -2.286, Test loglik = -2.934, Train RMSE = 3.267, Test RMSE = 4.357\n",
            "Epoch: 091/100, Train loglik = -2.487, Test loglik = -2.877, Train RMSE = 3.399, Test RMSE = 4.498\n",
            "Epoch: 100/100, Train loglik = -2.370, Test loglik = -2.942, Train RMSE = 3.256, Test RMSE = 4.436\n",
            "FOLD 3:\n",
            "Epoch: 001/100, Train loglik = -3.620, Test loglik = -3.650, Train RMSE = 9.147, Test RMSE = 9.599\n",
            "Epoch: 011/100, Train loglik = -3.145, Test loglik = -3.250, Train RMSE = 5.219, Test RMSE = 6.158\n",
            "Epoch: 021/100, Train loglik = -2.752, Test loglik = -2.965, Train RMSE = 3.922, Test RMSE = 5.118\n",
            "Epoch: 031/100, Train loglik = -2.665, Test loglik = -2.869, Train RMSE = 3.779, Test RMSE = 4.944\n",
            "Epoch: 041/100, Train loglik = -2.743, Test loglik = -2.856, Train RMSE = 3.663, Test RMSE = 4.844\n",
            "Epoch: 051/100, Train loglik = -2.404, Test loglik = -2.666, Train RMSE = 3.530, Test RMSE = 4.627\n",
            "Epoch: 061/100, Train loglik = -2.333, Test loglik = -2.725, Train RMSE = 3.416, Test RMSE = 4.518\n",
            "Epoch: 071/100, Train loglik = -2.453, Test loglik = -2.743, Train RMSE = 3.336, Test RMSE = 4.401\n",
            "Epoch: 081/100, Train loglik = -2.312, Test loglik = -2.830, Train RMSE = 3.311, Test RMSE = 4.434\n",
            "Epoch: 091/100, Train loglik = -2.339, Test loglik = -2.642, Train RMSE = 3.291, Test RMSE = 4.326\n",
            "Epoch: 100/100, Train loglik = -2.237, Test loglik = -2.607, Train RMSE = 3.293, Test RMSE = 4.393\n",
            "FOLD 4:\n",
            "Epoch: 001/100, Train loglik = -3.617, Test loglik = -3.640, Train RMSE = 9.130, Test RMSE = 9.104\n",
            "Epoch: 011/100, Train loglik = -3.055, Test loglik = -3.194, Train RMSE = 5.373, Test RMSE = 5.723\n",
            "Epoch: 021/100, Train loglik = -2.962, Test loglik = -2.841, Train RMSE = 4.058, Test RMSE = 4.138\n",
            "Epoch: 031/100, Train loglik = -2.520, Test loglik = -2.579, Train RMSE = 3.734, Test RMSE = 5.580\n",
            "Epoch: 041/100, Train loglik = -2.728, Test loglik = -2.477, Train RMSE = 3.604, Test RMSE = 4.661\n",
            "Epoch: 051/100, Train loglik = -2.478, Test loglik = -2.888, Train RMSE = 3.407, Test RMSE = 5.437\n",
            "Epoch: 061/100, Train loglik = -2.397, Test loglik = -2.835, Train RMSE = 3.339, Test RMSE = 5.010\n",
            "Epoch: 071/100, Train loglik = -2.364, Test loglik = -2.732, Train RMSE = 3.272, Test RMSE = 5.064\n",
            "Epoch: 081/100, Train loglik = -2.343, Test loglik = -3.083, Train RMSE = 3.302, Test RMSE = 4.900\n",
            "Epoch: 091/100, Train loglik = -2.256, Test loglik = -2.548, Train RMSE = 3.085, Test RMSE = 4.803\n",
            "Epoch: 100/100, Train loglik = -2.192, Test loglik = -3.268, Train RMSE = 3.141, Test RMSE = 4.685\n",
            "FOLD 5:\n",
            "Epoch: 001/100, Train loglik = -3.674, Test loglik = -3.486, Train RMSE = 9.419, Test RMSE = 7.066\n",
            "Epoch: 011/100, Train loglik = -3.122, Test loglik = -3.061, Train RMSE = 5.254, Test RMSE = 4.589\n",
            "Epoch: 021/100, Train loglik = -2.900, Test loglik = -2.680, Train RMSE = 4.212, Test RMSE = 3.134\n",
            "Epoch: 031/100, Train loglik = -2.692, Test loglik = -2.808, Train RMSE = 4.133, Test RMSE = 3.473\n",
            "Epoch: 041/100, Train loglik = -2.546, Test loglik = -2.708, Train RMSE = 3.784, Test RMSE = 3.003\n",
            "Epoch: 051/100, Train loglik = -2.644, Test loglik = -2.536, Train RMSE = 3.803, Test RMSE = 3.112\n",
            "Epoch: 061/100, Train loglik = -2.367, Test loglik = -2.552, Train RMSE = 3.646, Test RMSE = 2.715\n",
            "Epoch: 071/100, Train loglik = -2.508, Test loglik = -2.802, Train RMSE = 3.736, Test RMSE = 3.198\n",
            "Epoch: 081/100, Train loglik = -2.225, Test loglik = -3.095, Train RMSE = 3.490, Test RMSE = 2.843\n",
            "Epoch: 091/100, Train loglik = -2.325, Test loglik = -2.462, Train RMSE = 3.378, Test RMSE = 2.582\n",
            "Epoch: 100/100, Train loglik = -2.153, Test loglik = -2.690, Train RMSE = 3.398, Test RMSE = 2.591\n",
            "FOLD 6:\n",
            "Epoch: 001/100, Train loglik = -3.679, Test loglik = -3.574, Train RMSE = 9.178, Test RMSE = 8.024\n",
            "Epoch: 011/100, Train loglik = -3.108, Test loglik = -3.020, Train RMSE = 5.388, Test RMSE = 4.000\n",
            "Epoch: 021/100, Train loglik = -2.724, Test loglik = -2.950, Train RMSE = 4.091, Test RMSE = 3.396\n",
            "Epoch: 031/100, Train loglik = -2.592, Test loglik = -2.530, Train RMSE = 3.988, Test RMSE = 2.888\n",
            "Epoch: 041/100, Train loglik = -2.542, Test loglik = -2.552, Train RMSE = 3.832, Test RMSE = 3.020\n",
            "Epoch: 051/100, Train loglik = -2.498, Test loglik = -2.414, Train RMSE = 3.861, Test RMSE = 2.897\n",
            "Epoch: 061/100, Train loglik = -2.359, Test loglik = -2.333, Train RMSE = 3.620, Test RMSE = 2.792\n",
            "Epoch: 071/100, Train loglik = -2.336, Test loglik = -2.277, Train RMSE = 3.564, Test RMSE = 2.677\n",
            "Epoch: 081/100, Train loglik = -2.328, Test loglik = -2.480, Train RMSE = 3.487, Test RMSE = 2.648\n",
            "Epoch: 091/100, Train loglik = -2.295, Test loglik = -2.219, Train RMSE = 3.484, Test RMSE = 2.563\n",
            "Epoch: 100/100, Train loglik = -2.263, Test loglik = -2.285, Train RMSE = 3.427, Test RMSE = 2.631\n",
            "FOLD 7:\n",
            "Epoch: 001/100, Train loglik = -3.585, Test loglik = -3.893, Train RMSE = 8.766, Test RMSE = 11.726\n",
            "Epoch: 011/100, Train loglik = -3.070, Test loglik = -3.190, Train RMSE = 4.920, Test RMSE = 5.563\n",
            "Epoch: 021/100, Train loglik = -2.698, Test loglik = -2.916, Train RMSE = 3.870, Test RMSE = 4.841\n",
            "Epoch: 031/100, Train loglik = -2.547, Test loglik = -2.992, Train RMSE = 3.909, Test RMSE = 4.142\n",
            "Epoch: 041/100, Train loglik = -2.421, Test loglik = -2.785, Train RMSE = 3.643, Test RMSE = 3.899\n",
            "Epoch: 051/100, Train loglik = -2.453, Test loglik = -2.813, Train RMSE = 3.567, Test RMSE = 3.851\n",
            "Epoch: 061/100, Train loglik = -2.457, Test loglik = -2.633, Train RMSE = 3.551, Test RMSE = 4.018\n",
            "Epoch: 071/100, Train loglik = -2.276, Test loglik = -2.465, Train RMSE = 3.437, Test RMSE = 3.857\n",
            "Epoch: 081/100, Train loglik = -2.363, Test loglik = -2.725, Train RMSE = 3.366, Test RMSE = 3.795\n",
            "Epoch: 091/100, Train loglik = -2.370, Test loglik = -2.599, Train RMSE = 3.335, Test RMSE = 3.573\n",
            "Epoch: 100/100, Train loglik = -2.193, Test loglik = -3.057, Train RMSE = 3.302, Test RMSE = 3.552\n",
            "FOLD 8:\n",
            "Epoch: 001/100, Train loglik = -3.610, Test loglik = -3.787, Train RMSE = 8.840, Test RMSE = 10.545\n",
            "Epoch: 011/100, Train loglik = -3.120, Test loglik = -3.488, Train RMSE = 4.956, Test RMSE = 6.311\n",
            "Epoch: 021/100, Train loglik = -2.741, Test loglik = -3.005, Train RMSE = 3.980, Test RMSE = 5.343\n",
            "Epoch: 031/100, Train loglik = -2.621, Test loglik = -2.863, Train RMSE = 3.758, Test RMSE = 4.939\n",
            "Epoch: 041/100, Train loglik = -2.471, Test loglik = -2.774, Train RMSE = 3.738, Test RMSE = 4.857\n",
            "Epoch: 051/100, Train loglik = -2.436, Test loglik = -2.553, Train RMSE = 3.589, Test RMSE = 4.493\n",
            "Epoch: 061/100, Train loglik = -2.390, Test loglik = -2.821, Train RMSE = 3.461, Test RMSE = 4.533\n",
            "Epoch: 071/100, Train loglik = -2.403, Test loglik = -2.496, Train RMSE = 3.532, Test RMSE = 4.772\n",
            "Epoch: 081/100, Train loglik = -2.315, Test loglik = -2.400, Train RMSE = 3.252, Test RMSE = 4.439\n",
            "Epoch: 091/100, Train loglik = -2.333, Test loglik = -2.453, Train RMSE = 3.163, Test RMSE = 4.472\n",
            "Epoch: 100/100, Train loglik = -2.315, Test loglik = -2.350, Train RMSE = 3.077, Test RMSE = 4.312\n",
            "FOLD 9:\n",
            "Epoch: 001/100, Train loglik = -3.648, Test loglik = -3.535, Train RMSE = 9.242, Test RMSE = 8.111\n",
            "Epoch: 011/100, Train loglik = -3.151, Test loglik = -2.994, Train RMSE = 5.329, Test RMSE = 4.607\n",
            "Epoch: 021/100, Train loglik = -2.802, Test loglik = -3.072, Train RMSE = 4.108, Test RMSE = 3.732\n",
            "Epoch: 031/100, Train loglik = -2.693, Test loglik = -2.756, Train RMSE = 3.945, Test RMSE = 3.193\n",
            "Epoch: 041/100, Train loglik = -2.502, Test loglik = -2.475, Train RMSE = 3.742, Test RMSE = 3.161\n",
            "Epoch: 051/100, Train loglik = -2.834, Test loglik = -2.898, Train RMSE = 3.648, Test RMSE = 3.152\n",
            "Epoch: 061/100, Train loglik = -2.347, Test loglik = -2.684, Train RMSE = 3.559, Test RMSE = 2.907\n",
            "Epoch: 071/100, Train loglik = -2.333, Test loglik = -2.534, Train RMSE = 3.475, Test RMSE = 2.942\n",
            "Epoch: 081/100, Train loglik = -2.229, Test loglik = -2.491, Train RMSE = 3.491, Test RMSE = 2.817\n",
            "Epoch: 091/100, Train loglik = -2.214, Test loglik = -2.555, Train RMSE = 3.301, Test RMSE = 3.027\n",
            "Epoch: 100/100, Train loglik = -2.252, Test loglik = -2.615, Train RMSE = 3.275, Test RMSE = 2.904\n",
            "Train log. lik. = -2.210 +/-  0.026\n",
            "Test  log. lik. = -2.637 +/-  0.384\n",
            "Train RMSE      =  3.330 +/-  0.150\n",
            "Test  RMSE      =  3.488 +/-  0.879\n"
          ]
        }
      ],
      "source": [
        "model = train_BBP(data, n_splits=10, num_epochs=100, num_units=100, learn_rate=1e-2, log_every=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnGD_RefRxkZ"
      },
      "source": [
        "Concrete compressive dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "s5G3-z_jRxkZ",
        "outputId": "6649f453-a080-44ec-c81f-299fb74ca756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-07 09:24:27--  https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 124928 (122K) [application/x-httpd-php]\n",
            "Saving to: ‘Concrete_Data.xls.1’\n",
            "\n",
            "Concrete_Data.xls.1 100%[===================>] 122.00K   275KB/s    in 0.4s    \n",
            "\n",
            "2021-12-07 09:24:28 (275 KB/s) - ‘Concrete_Data.xls.1’ saved [124928/124928]\n",
            "\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1783152d2020>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\" --no-check-certificate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Concrete_Data.xls'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\s+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: read_excel() got an unexpected keyword argument 'delimiter'"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\" --no-check-certificate\n",
        "data = pd.read_excel('Concrete_Data.xls', header=0, delimiter=\"\\s+\").values\n",
        "data = data[np.random.permutation(np.arange(len(data)))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRTkNIf7WsaE",
        "outputId": "468ffc7f-f12b-4154-8bf9-536d32ca507e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOLD 0:\n",
            "Epoch: 001/100, Train loglik = -3.618, Test loglik = -3.594, Train RMSE = 9.095, Test RMSE = 9.607\n",
            "Epoch: 011/100, Train loglik = -3.218, Test loglik = -3.130, Train RMSE = 5.249, Test RMSE = 6.413\n",
            "Epoch: 021/100, Train loglik = -2.704, Test loglik = -2.766, Train RMSE = 4.168, Test RMSE = 2.979\n",
            "Epoch: 031/100, Train loglik = -2.558, Test loglik = -2.636, Train RMSE = 4.089, Test RMSE = 2.853\n",
            "Epoch: 041/100, Train loglik = -2.535, Test loglik = -2.529, Train RMSE = 3.958, Test RMSE = 2.519\n",
            "Epoch: 051/100, Train loglik = -2.450, Test loglik = -2.319, Train RMSE = 3.822, Test RMSE = 2.344\n",
            "Epoch: 061/100, Train loglik = -2.424, Test loglik = -2.231, Train RMSE = 3.679, Test RMSE = 2.437\n",
            "Epoch: 071/100, Train loglik = -2.265, Test loglik = -2.347, Train RMSE = 3.595, Test RMSE = 2.455\n",
            "Epoch: 081/100, Train loglik = -2.230, Test loglik = -2.435, Train RMSE = 3.581, Test RMSE = 2.545\n",
            "Epoch: 091/100, Train loglik = -2.267, Test loglik = -2.294, Train RMSE = 3.442, Test RMSE = 2.256\n",
            "Epoch: 100/100, Train loglik = -2.262, Test loglik = -2.282, Train RMSE = 3.470, Test RMSE = 2.288\n",
            "FOLD 1:\n",
            "Epoch: 001/100, Train loglik = -3.671, Test loglik = -3.627, Train RMSE = 9.221, Test RMSE = 9.116\n",
            "Epoch: 011/100, Train loglik = -3.291, Test loglik = -3.074, Train RMSE = 5.373, Test RMSE = 4.939\n",
            "Epoch: 021/100, Train loglik = -2.800, Test loglik = -2.846, Train RMSE = 4.095, Test RMSE = 3.790\n",
            "Epoch: 031/100, Train loglik = -2.674, Test loglik = -2.696, Train RMSE = 3.981, Test RMSE = 3.552\n",
            "Epoch: 041/100, Train loglik = -2.529, Test loglik = -2.701, Train RMSE = 3.885, Test RMSE = 3.435\n",
            "Epoch: 051/100, Train loglik = -2.359, Test loglik = -2.637, Train RMSE = 3.665, Test RMSE = 3.361\n",
            "Epoch: 061/100, Train loglik = -2.384, Test loglik = -2.612, Train RMSE = 3.721, Test RMSE = 3.259\n",
            "Epoch: 071/100, Train loglik = -2.301, Test loglik = -2.578, Train RMSE = 3.600, Test RMSE = 3.202\n",
            "Epoch: 081/100, Train loglik = -2.288, Test loglik = -2.731, Train RMSE = 3.609, Test RMSE = 3.484\n",
            "Epoch: 091/100, Train loglik = -2.278, Test loglik = -2.500, Train RMSE = 3.482, Test RMSE = 3.158\n",
            "Epoch: 100/100, Train loglik = -2.200, Test loglik = -2.592, Train RMSE = 3.401, Test RMSE = 3.076\n",
            "FOLD 2:\n",
            "Epoch: 001/100, Train loglik = -3.674, Test loglik = -3.527, Train RMSE = 9.207, Test RMSE = 7.664\n",
            "Epoch: 011/100, Train loglik = -3.189, Test loglik = -3.416, Train RMSE = 5.215, Test RMSE = 5.537\n",
            "Epoch: 021/100, Train loglik = -2.832, Test loglik = -2.780, Train RMSE = 3.944, Test RMSE = 5.173\n",
            "Epoch: 031/100, Train loglik = -2.627, Test loglik = -2.753, Train RMSE = 3.825, Test RMSE = 4.946\n",
            "Epoch: 041/100, Train loglik = -2.455, Test loglik = -2.703, Train RMSE = 3.626, Test RMSE = 4.723\n",
            "Epoch: 051/100, Train loglik = -2.514, Test loglik = -2.960, Train RMSE = 3.459, Test RMSE = 4.591\n",
            "Epoch: 061/100, Train loglik = -2.326, Test loglik = -2.658, Train RMSE = 3.339, Test RMSE = 4.536\n",
            "Epoch: 071/100, Train loglik = -2.303, Test loglik = -3.047, Train RMSE = 3.423, Test RMSE = 4.325\n",
            "Epoch: 081/100, Train loglik = -2.291, Test loglik = -2.839, Train RMSE = 3.280, Test RMSE = 4.503\n",
            "Epoch: 091/100, Train loglik = -2.268, Test loglik = -2.628, Train RMSE = 3.284, Test RMSE = 4.545\n",
            "Epoch: 100/100, Train loglik = -2.286, Test loglik = -2.605, Train RMSE = 3.291, Test RMSE = 4.508\n",
            "FOLD 3:\n",
            "Epoch: 001/100, Train loglik = -3.633, Test loglik = -3.682, Train RMSE = 9.035, Test RMSE = 9.470\n",
            "Epoch: 011/100, Train loglik = -3.142, Test loglik = -3.208, Train RMSE = 5.306, Test RMSE = 6.109\n",
            "Epoch: 021/100, Train loglik = -2.799, Test loglik = -3.044, Train RMSE = 4.119, Test RMSE = 5.402\n",
            "Epoch: 031/100, Train loglik = -2.575, Test loglik = -3.105, Train RMSE = 3.806, Test RMSE = 5.025\n",
            "Epoch: 041/100, Train loglik = -2.923, Test loglik = -2.983, Train RMSE = 3.653, Test RMSE = 4.888\n",
            "Epoch: 051/100, Train loglik = -2.375, Test loglik = -2.919, Train RMSE = 3.551, Test RMSE = 4.726\n",
            "Epoch: 061/100, Train loglik = -2.316, Test loglik = -2.688, Train RMSE = 3.482, Test RMSE = 4.624\n",
            "Epoch: 071/100, Train loglik = -2.355, Test loglik = -2.756, Train RMSE = 3.439, Test RMSE = 4.561\n",
            "Epoch: 081/100, Train loglik = -2.477, Test loglik = -2.742, Train RMSE = 3.372, Test RMSE = 4.424\n",
            "Epoch: 091/100, Train loglik = -2.276, Test loglik = -2.659, Train RMSE = 3.299, Test RMSE = 4.354\n",
            "Epoch: 100/100, Train loglik = -2.199, Test loglik = -2.668, Train RMSE = 3.267, Test RMSE = 4.219\n",
            "FOLD 4:\n",
            "Epoch: 001/100, Train loglik = -3.602, Test loglik = -3.623, Train RMSE = 9.160, Test RMSE = 9.089\n",
            "Epoch: 011/100, Train loglik = -3.078, Test loglik = -3.285, Train RMSE = 5.168, Test RMSE = 5.531\n",
            "Epoch: 021/100, Train loglik = -2.728, Test loglik = -2.644, Train RMSE = 4.172, Test RMSE = 3.949\n",
            "Epoch: 031/100, Train loglik = -2.583, Test loglik = -2.636, Train RMSE = 3.796, Test RMSE = 5.329\n",
            "Epoch: 041/100, Train loglik = -2.478, Test loglik = -2.590, Train RMSE = 3.515, Test RMSE = 5.331\n",
            "Epoch: 051/100, Train loglik = -2.420, Test loglik = -2.753, Train RMSE = 3.425, Test RMSE = 5.095\n",
            "Epoch: 061/100, Train loglik = -2.457, Test loglik = -3.631, Train RMSE = 3.295, Test RMSE = 4.776\n",
            "Epoch: 071/100, Train loglik = -2.343, Test loglik = -3.021, Train RMSE = 3.336, Test RMSE = 4.576\n",
            "Epoch: 081/100, Train loglik = -2.482, Test loglik = -2.505, Train RMSE = 3.180, Test RMSE = 4.771\n",
            "Epoch: 091/100, Train loglik = -2.278, Test loglik = -2.678, Train RMSE = 3.163, Test RMSE = 4.886\n",
            "Epoch: 100/100, Train loglik = -2.215, Test loglik = -2.988, Train RMSE = 3.063, Test RMSE = 4.849\n",
            "FOLD 5:\n",
            "Epoch: 001/100, Train loglik = -3.620, Test loglik = -3.452, Train RMSE = 9.364, Test RMSE = 7.086\n",
            "Epoch: 011/100, Train loglik = -3.192, Test loglik = -3.102, Train RMSE = 5.754, Test RMSE = 4.878\n",
            "Epoch: 021/100, Train loglik = -2.775, Test loglik = -2.662, Train RMSE = 4.168, Test RMSE = 2.927\n",
            "Epoch: 031/100, Train loglik = -2.628, Test loglik = -2.829, Train RMSE = 3.974, Test RMSE = 3.339\n",
            "Epoch: 041/100, Train loglik = -2.638, Test loglik = -2.929, Train RMSE = 3.983, Test RMSE = 3.152\n",
            "Epoch: 051/100, Train loglik = -2.419, Test loglik = -2.802, Train RMSE = 3.712, Test RMSE = 3.001\n",
            "Epoch: 061/100, Train loglik = -2.454, Test loglik = -2.506, Train RMSE = 3.667, Test RMSE = 2.979\n",
            "Epoch: 071/100, Train loglik = -2.565, Test loglik = -2.545, Train RMSE = 3.711, Test RMSE = 2.982\n",
            "Epoch: 081/100, Train loglik = -2.462, Test loglik = -2.368, Train RMSE = 3.591, Test RMSE = 2.939\n",
            "Epoch: 091/100, Train loglik = -2.300, Test loglik = -2.626, Train RMSE = 3.513, Test RMSE = 2.876\n",
            "Epoch: 100/100, Train loglik = -2.200, Test loglik = -2.408, Train RMSE = 3.518, Test RMSE = 2.769\n",
            "FOLD 6:\n",
            "Epoch: 001/100, Train loglik = -3.622, Test loglik = -3.544, Train RMSE = 9.251, Test RMSE = 7.991\n",
            "Epoch: 011/100, Train loglik = -3.145, Test loglik = -3.072, Train RMSE = 5.197, Test RMSE = 3.905\n",
            "Epoch: 021/100, Train loglik = -2.951, Test loglik = -2.793, Train RMSE = 4.079, Test RMSE = 3.255\n",
            "Epoch: 031/100, Train loglik = -2.676, Test loglik = -2.662, Train RMSE = 3.965, Test RMSE = 3.028\n",
            "Epoch: 041/100, Train loglik = -3.285, Test loglik = -2.632, Train RMSE = 3.785, Test RMSE = 3.111\n",
            "Epoch: 051/100, Train loglik = -2.480, Test loglik = -2.355, Train RMSE = 3.790, Test RMSE = 2.726\n",
            "Epoch: 061/100, Train loglik = -2.326, Test loglik = -2.387, Train RMSE = 3.652, Test RMSE = 2.661\n",
            "Epoch: 071/100, Train loglik = -2.313, Test loglik = -2.514, Train RMSE = 3.607, Test RMSE = 2.775\n",
            "Epoch: 081/100, Train loglik = -2.347, Test loglik = -2.330, Train RMSE = 3.524, Test RMSE = 2.633\n",
            "Epoch: 091/100, Train loglik = -2.291, Test loglik = -2.351, Train RMSE = 3.450, Test RMSE = 2.592\n",
            "Epoch: 100/100, Train loglik = -2.328, Test loglik = -2.219, Train RMSE = 3.404, Test RMSE = 2.591\n",
            "FOLD 7:\n",
            "Epoch: 001/100, Train loglik = -3.599, Test loglik = -3.925, Train RMSE = 8.914, Test RMSE = 11.750\n",
            "Epoch: 011/100, Train loglik = -3.149, Test loglik = -3.199, Train RMSE = 5.094, Test RMSE = 5.586\n",
            "Epoch: 021/100, Train loglik = -2.689, Test loglik = -2.930, Train RMSE = 3.962, Test RMSE = 4.859\n",
            "Epoch: 031/100, Train loglik = -2.541, Test loglik = -2.847, Train RMSE = 3.866, Test RMSE = 4.250\n",
            "Epoch: 041/100, Train loglik = -2.573, Test loglik = -2.773, Train RMSE = 3.677, Test RMSE = 4.176\n",
            "Epoch: 051/100, Train loglik = -2.445, Test loglik = -2.667, Train RMSE = 3.666, Test RMSE = 3.939\n",
            "Epoch: 061/100, Train loglik = -2.431, Test loglik = -2.792, Train RMSE = 3.512, Test RMSE = 3.985\n",
            "Epoch: 071/100, Train loglik = -2.285, Test loglik = -2.661, Train RMSE = 3.443, Test RMSE = 3.817\n",
            "Epoch: 081/100, Train loglik = -2.252, Test loglik = -2.678, Train RMSE = 3.367, Test RMSE = 3.758\n",
            "Epoch: 091/100, Train loglik = -2.236, Test loglik = -2.658, Train RMSE = 3.260, Test RMSE = 3.682\n",
            "Epoch: 100/100, Train loglik = -2.267, Test loglik = -2.579, Train RMSE = 3.220, Test RMSE = 3.540\n",
            "FOLD 8:\n",
            "Epoch: 001/100, Train loglik = -3.607, Test loglik = -3.761, Train RMSE = 8.936, Test RMSE = 10.536\n",
            "Epoch: 011/100, Train loglik = -3.144, Test loglik = -3.264, Train RMSE = 5.270, Test RMSE = 6.499\n",
            "Epoch: 021/100, Train loglik = -2.859, Test loglik = -2.897, Train RMSE = 3.963, Test RMSE = 5.214\n",
            "Epoch: 031/100, Train loglik = -2.561, Test loglik = -2.730, Train RMSE = 3.767, Test RMSE = 4.768\n",
            "Epoch: 041/100, Train loglik = -2.584, Test loglik = -2.797, Train RMSE = 3.570, Test RMSE = 4.686\n",
            "Epoch: 051/100, Train loglik = -2.443, Test loglik = -2.589, Train RMSE = 3.596, Test RMSE = 4.599\n",
            "Epoch: 061/100, Train loglik = -2.443, Test loglik = -2.436, Train RMSE = 3.471, Test RMSE = 4.607\n",
            "Epoch: 071/100, Train loglik = -2.441, Test loglik = -2.446, Train RMSE = 3.352, Test RMSE = 4.587\n",
            "Epoch: 081/100, Train loglik = -2.309, Test loglik = -2.537, Train RMSE = 3.246, Test RMSE = 4.476\n",
            "Epoch: 091/100, Train loglik = -2.282, Test loglik = -2.350, Train RMSE = 3.142, Test RMSE = 4.491\n",
            "Epoch: 100/100, Train loglik = -2.228, Test loglik = -2.668, Train RMSE = 3.190, Test RMSE = 4.645\n",
            "FOLD 9:\n",
            "Epoch: 001/100, Train loglik = -3.648, Test loglik = -3.544, Train RMSE = 9.275, Test RMSE = 8.223\n",
            "Epoch: 011/100, Train loglik = -3.098, Test loglik = -3.222, Train RMSE = 5.201, Test RMSE = 4.728\n",
            "Epoch: 021/100, Train loglik = -2.716, Test loglik = -2.847, Train RMSE = 4.329, Test RMSE = 4.138\n",
            "Epoch: 031/100, Train loglik = -2.633, Test loglik = -2.542, Train RMSE = 3.991, Test RMSE = 3.474\n",
            "Epoch: 041/100, Train loglik = -2.445, Test loglik = -2.809, Train RMSE = 3.826, Test RMSE = 3.066\n",
            "Epoch: 051/100, Train loglik = -2.434, Test loglik = -2.639, Train RMSE = 3.641, Test RMSE = 3.024\n",
            "Epoch: 061/100, Train loglik = -2.341, Test loglik = -2.465, Train RMSE = 3.500, Test RMSE = 2.838\n",
            "Epoch: 071/100, Train loglik = -2.340, Test loglik = -2.600, Train RMSE = 3.429, Test RMSE = 2.940\n",
            "Epoch: 081/100, Train loglik = -2.231, Test loglik = -2.937, Train RMSE = 3.365, Test RMSE = 3.160\n",
            "Epoch: 091/100, Train loglik = -2.304, Test loglik = -2.530, Train RMSE = 3.310, Test RMSE = 2.860\n",
            "Epoch: 100/100, Train loglik = -2.213, Test loglik = -2.535, Train RMSE = 3.295, Test RMSE = 2.908\n",
            "Train log. lik. = -2.244 +/-  0.035\n",
            "Test  log. lik. = -2.662 +/-  0.461\n",
            "Train RMSE      =  3.294 +/-  0.139\n",
            "Test  RMSE      =  3.550 +/-  0.868\n"
          ]
        }
      ],
      "source": [
        "model = train_BBP(data, n_splits=10, num_epochs=100, num_units=100, learn_rate=1e-2, log_every=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxatwTIqVrZ1"
      },
      "source": [
        "\n",
        "Energy efficiency dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZDQX5l3ZfLV"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\" --no-check-certificate\n",
        "data = pd.read_excel('ENB2012_data.xlsx', header=0, delimiter=\"\\s+\").values\n",
        "data = data[np.random.permutation(np.arange(len(data)))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHW1eArNUMKw"
      },
      "outputs": [],
      "source": [
        "model = train_BBP(data, n_splits=10, num_epochs=100, num_units=100, learn_rate=1e-2, log_every=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFsWDzx8NSry"
      },
      "source": [
        "Power dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzdZLChPNQ9X"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00294/CCPP.zip\" --no-check-certificate \n",
        "zipped = zipfile.ZipFile(\"CCPP.zip\")\n",
        "data = pd.read_excel(zipped.open('CCPP/Folds5x2_pp.xlsx'), header=0, delimiter=\"\\t\").values\n",
        "np.random.shuffle(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld95mVAKyvo9"
      },
      "outputs": [],
      "source": [
        "model = train_BBP(data, n_splits=10, num_epochs=100, num_units=100, learn_rate=1e-2, log_every=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRPICBiXCegI"
      },
      "source": [
        "Red wine dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOqgIBXcCegJ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\" --no-check-certificate \n",
        "data = pd.read_csv('winequality-red.csv', header=1, delimiter=';').values\n",
        "data = data[np.random.permutation(np.arange(len(data)))]\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8gWjBbKCegM"
      },
      "outputs": [],
      "source": [
        "model = train_BBP(data, n_splits=10, num_epochs=100, num_units=100, learn_rate=1e-2, log_every=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVZw0uJzDgdy"
      },
      "source": [
        "Yacht dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja_kIet3Dgdz"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\" --no-check-certificate \n",
        "data = pd.read_csv('yacht_hydrodynamics.data', header=1, delimiter='\\s+').values\n",
        "data = data[np.random.permutation(np.arange(len(data)))]\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiviAdYxXsbN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clxNDH6dDgd3"
      },
      "outputs": [],
      "source": [
        "model = train_BBP(data, n_splits=10, num_epochs=100, num_units=100, learn_rate=1e-2, log_every=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkPfMYh_w2Z1"
      },
      "source": [
        "MSCN Code from: [Andreas Kipf's Learned Cardinalities GitHub Repository](https://github.com/andreaskipf/learnedcardinalities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-wwzicHM-Tw"
      },
      "source": [
        "# MSCN with BNN, based on Javier Antoran's repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxNYjScwNh0X",
        "outputId": "237c7a0a-54b1-41ec-d912-d10d0cbc3cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2IWg3TdN1uo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/Database_Project')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzzm7IQ2NqbX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Helper functions for data processing\n",
        "\n",
        "def chunks(l, n):\n",
        "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
        "    for i in range(0, len(l), n):\n",
        "        yield l[i:i + n]\n",
        "\n",
        "\n",
        "def get_all_column_names(predicates):\n",
        "    column_names = set()\n",
        "    for query in predicates:\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                column_name = predicate[0]\n",
        "                column_names.add(column_name)\n",
        "    return column_names\n",
        "\n",
        "\n",
        "def get_all_table_names(tables):\n",
        "    table_names = set()\n",
        "    for query in tables:\n",
        "        for table in query:\n",
        "            table_names.add(table)\n",
        "    return table_names\n",
        "\n",
        "\n",
        "def get_all_operators(predicates):\n",
        "    operators = set()\n",
        "    for query in predicates:\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                operator = predicate[1]\n",
        "                operators.add(operator)\n",
        "    return operators\n",
        "\n",
        "\n",
        "def get_all_joins(joins):\n",
        "    join_set = set()\n",
        "    for query in joins:\n",
        "        for join in query:\n",
        "            join_set.add(join)\n",
        "    return join_set\n",
        "\n",
        "\n",
        "def idx_to_onehot(idx, num_elements):\n",
        "    onehot = np.zeros(num_elements, dtype=np.float32)\n",
        "    onehot[idx] = 1.\n",
        "    return onehot\n",
        "\n",
        "\n",
        "def get_set_encoding(source_set, onehot=True):\n",
        "    num_elements = len(source_set)\n",
        "    source_list = list(source_set)\n",
        "    # Sort list to avoid non-deterministic behavior\n",
        "    source_list.sort()\n",
        "    # Build map from s to i\n",
        "    thing2idx = {s: i for i, s in enumerate(source_list)}\n",
        "    # Build array (essentially a map from idx to s)\n",
        "    idx2thing = [s for i, s in enumerate(source_list)]\n",
        "    if onehot:\n",
        "        thing2vec = {s: idx_to_onehot(i, num_elements) for i, s in enumerate(source_list)}\n",
        "        return thing2vec, idx2thing\n",
        "    return thing2idx, idx2thing\n",
        "\n",
        "\n",
        "def get_min_max_vals(predicates, column_names):\n",
        "    min_max_vals = {t: [float('inf'), float('-inf')] for t in column_names}\n",
        "    for query in predicates:\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                column_name = predicate[0]\n",
        "                val = float(predicate[2])\n",
        "                if val < min_max_vals[column_name][0]:\n",
        "                    min_max_vals[column_name][0] = val\n",
        "                if val > min_max_vals[column_name][1]:\n",
        "                    min_max_vals[column_name][1] = val\n",
        "    return min_max_vals\n",
        "\n",
        "\n",
        "def normalize_data(val, column_name, column_min_max_vals):\n",
        "    min_val = column_min_max_vals[column_name][0]\n",
        "    max_val = column_min_max_vals[column_name][1]\n",
        "    val = float(val)\n",
        "    val_norm = 0.0\n",
        "    if max_val > min_val:\n",
        "        val_norm = (val - min_val) / (max_val - min_val)\n",
        "    return np.array(val_norm, dtype=np.float32)\n",
        "\n",
        "\n",
        "def normalize_labels(labels, min_val=None, max_val=None):\n",
        "    labels = np.array([np.log(float(l)) for l in labels])\n",
        "    if min_val is None:\n",
        "        min_val = labels.min()\n",
        "        print(\"min log(label): {}\".format(min_val))\n",
        "    if max_val is None:\n",
        "        max_val = labels.max()\n",
        "        print(\"max log(label): {}\".format(max_val))\n",
        "    labels_norm = (labels - min_val) / (max_val - min_val)\n",
        "    # Threshold labels\n",
        "    labels_norm = np.minimum(labels_norm, 1)\n",
        "    labels_norm = np.maximum(labels_norm, 0)\n",
        "    return labels_norm, min_val, max_val\n",
        "\n",
        "\n",
        "def unnormalize_labels(labels_norm, min_val, max_val):\n",
        "    labels_norm = np.array(labels_norm, dtype=np.float32)\n",
        "    labels = (labels_norm * (max_val - min_val)) + min_val\n",
        "    return np.array(np.round(np.exp(labels)), dtype=np.int64)\n",
        "\n",
        "\n",
        "def encode_samples(tables, samples, table2vec):\n",
        "    samples_enc = []\n",
        "    for i, query in enumerate(tables):\n",
        "        samples_enc.append(list())\n",
        "        for j, table in enumerate(query):\n",
        "            sample_vec = []\n",
        "            # Append table one-hot vector\n",
        "            sample_vec.append(table2vec[table])\n",
        "            # Append bit vector\n",
        "            sample_vec.append(samples[i][j])\n",
        "            sample_vec = np.hstack(sample_vec)\n",
        "            samples_enc[i].append(sample_vec)\n",
        "    return samples_enc\n",
        "\n",
        "\n",
        "def encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec):\n",
        "    predicates_enc = []\n",
        "    joins_enc = []\n",
        "    for i, query in enumerate(predicates):\n",
        "        predicates_enc.append(list())\n",
        "        joins_enc.append(list())\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                # Proper predicate\n",
        "                column = predicate[0]\n",
        "                operator = predicate[1]\n",
        "                val = predicate[2]\n",
        "                norm_val = normalize_data(val, column, column_min_max_vals)\n",
        "\n",
        "                pred_vec = []\n",
        "                pred_vec.append(column2vec[column])\n",
        "                pred_vec.append(op2vec[operator])\n",
        "                pred_vec.append(norm_val)\n",
        "                pred_vec = np.hstack(pred_vec)\n",
        "            else:\n",
        "                pred_vec = np.zeros((len(column2vec) + len(op2vec) + 1))\n",
        "\n",
        "            predicates_enc[i].append(pred_vec)\n",
        "\n",
        "        for predicate in joins[i]:\n",
        "            # Join instruction\n",
        "            join_vec = join2vec[predicate]\n",
        "            joins_enc[i].append(join_vec)\n",
        "    return predicates_enc, joins_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mStJ4QBDOAXl"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import torch\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "\n",
        "def load_data(file_name, num_materialized_samples):\n",
        "    joins = []\n",
        "    predicates = []\n",
        "    tables = []\n",
        "    samples = []\n",
        "    label = []\n",
        "\n",
        "    # Load queries\n",
        "    with open(file_name + \".csv\") as f:\n",
        "        data_raw = list(list(rec) for rec in csv.reader(f, delimiter='#'))\n",
        "        for row in data_raw:\n",
        "            tables.append(row[0].split(','))\n",
        "            joins.append(row[1].split(','))\n",
        "            predicates.append(row[2].split(','))\n",
        "            if int(row[3]) < 1:\n",
        "                print(\"Queries must have non-zero cardinalities\")\n",
        "                exit(1)\n",
        "            label.append(row[3])\n",
        "    print(\"Loaded queries\")\n",
        "\n",
        "    # Load bitmaps\n",
        "    num_bytes_per_bitmap = int((num_materialized_samples + 7) >> 3)\n",
        "    with open(file_name + \".bitmaps\", 'rb') as f:\n",
        "        for i in range(len(tables)):\n",
        "            four_bytes = f.read(4)\n",
        "            if not four_bytes:\n",
        "                print(\"Error while reading 'four_bytes'\")\n",
        "                exit(1)\n",
        "            num_bitmaps_curr_query = int.from_bytes(four_bytes, byteorder='little')\n",
        "            bitmaps = np.empty((num_bitmaps_curr_query, num_bytes_per_bitmap * 8), dtype=np.uint8)\n",
        "            for j in range(num_bitmaps_curr_query):\n",
        "                # Read bitmap\n",
        "                bitmap_bytes = f.read(num_bytes_per_bitmap)\n",
        "                if not bitmap_bytes:\n",
        "                    print(\"Error while reading 'bitmap_bytes'\")\n",
        "                    exit(1)\n",
        "                bitmaps[j] = np.unpackbits(np.frombuffer(bitmap_bytes, dtype=np.uint8))\n",
        "            samples.append(bitmaps)\n",
        "    print(\"Loaded bitmaps\")\n",
        "\n",
        "    # Split predicates\n",
        "    predicates = [list(chunks(d, 3)) for d in predicates]\n",
        "\n",
        "    return joins, predicates, tables, samples, label\n",
        "\n",
        "\n",
        "def load_and_encode_train_data(num_queries, num_materialized_samples):\n",
        "    file_name_queries = \"/content/drive/MyDrive/Colab Notebooks/Database_Project/data/train\"\n",
        "    file_name_column_min_max_vals = \"/content/drive/MyDrive/Colab Notebooks/Database_Project/data/column_min_max_vals.csv\"\n",
        "\n",
        "    joins, predicates, tables, samples, label = load_data(file_name_queries, num_materialized_samples)\n",
        "\n",
        "    # Get column name dict\n",
        "    column_names = get_all_column_names(predicates)\n",
        "    column2vec, idx2column = get_set_encoding(column_names)\n",
        "\n",
        "    # Get table name dict\n",
        "    table_names = get_all_table_names(tables)\n",
        "    table2vec, idx2table = get_set_encoding(table_names)\n",
        "\n",
        "    # Get operator name dict\n",
        "    operators = get_all_operators(predicates)\n",
        "    op2vec, idx2op = get_set_encoding(operators)\n",
        "\n",
        "    # Get join name dict\n",
        "    join_set = get_all_joins(joins)\n",
        "    join2vec, idx2join = get_set_encoding(join_set)\n",
        "\n",
        "    # Get min and max values for each column\n",
        "    with open(file_name_column_min_max_vals) as f:\n",
        "        data_raw = list(list(rec) for rec in csv.reader(f, delimiter=','))\n",
        "        column_min_max_vals = {}\n",
        "        for i, row in enumerate(data_raw):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            column_min_max_vals[row[0]] = [float(row[1]), float(row[2])]\n",
        "\n",
        "    # Get feature encoding and proper normalization\n",
        "    samples_enc = encode_samples(tables, samples, table2vec)\n",
        "    predicates_enc, joins_enc = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
        "    label_norm, min_val, max_val = normalize_labels(label)\n",
        "\n",
        "    # Split in training and validation samples\n",
        "    num_train = int(num_queries * 0.9)\n",
        "    num_test = num_queries - num_train\n",
        "\n",
        "    samples_train = samples_enc[:num_train]\n",
        "    predicates_train = predicates_enc[:num_train]\n",
        "    joins_train = joins_enc[:num_train]\n",
        "    labels_train = label_norm[:num_train]\n",
        "\n",
        "    samples_test = samples_enc[num_train:num_train + num_test]\n",
        "    predicates_test = predicates_enc[num_train:num_train + num_test]\n",
        "    joins_test = joins_enc[num_train:num_train + num_test]\n",
        "    labels_test = label_norm[num_train:num_train + num_test]\n",
        "\n",
        "    print(\"Number of training samples: {}\".format(len(labels_train)))\n",
        "    print(\"Number of validation samples: {}\".format(len(labels_test)))\n",
        "\n",
        "    max_num_joins = max(max([len(j) for j in joins_train]), max([len(j) for j in joins_test]))\n",
        "    max_num_predicates = max(max([len(p) for p in predicates_train]), max([len(p) for p in predicates_test]))\n",
        "\n",
        "    dicts = [table2vec, column2vec, op2vec, join2vec]\n",
        "    train_data = [samples_train, predicates_train, joins_train]\n",
        "    test_data = [samples_test, predicates_test, joins_test]\n",
        "    return dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data\n",
        "\n",
        "\n",
        "def make_dataset(samples, predicates, joins, labels, max_num_joins, max_num_predicates):\n",
        "    \"\"\"Add zero-padding and wrap as tensor dataset.\"\"\"\n",
        "\n",
        "    sample_masks = []\n",
        "    sample_tensors = []\n",
        "    for sample in samples:\n",
        "        sample_tensor = np.vstack(sample)\n",
        "        num_pad = max_num_joins + 1 - sample_tensor.shape[0]\n",
        "        sample_mask = np.ones_like(sample_tensor).mean(1, keepdims=True)\n",
        "        sample_tensor = np.pad(sample_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
        "        sample_mask = np.pad(sample_mask, ((0, num_pad), (0, 0)), 'constant')\n",
        "        sample_tensors.append(np.expand_dims(sample_tensor, 0))\n",
        "        sample_masks.append(np.expand_dims(sample_mask, 0))\n",
        "    sample_tensors = np.vstack(sample_tensors)\n",
        "    sample_tensors = torch.FloatTensor(sample_tensors)\n",
        "    sample_masks = np.vstack(sample_masks)\n",
        "    sample_masks = torch.FloatTensor(sample_masks)\n",
        "\n",
        "    predicate_masks = []\n",
        "    predicate_tensors = []\n",
        "    for predicate in predicates:\n",
        "        predicate_tensor = np.vstack(predicate)\n",
        "        num_pad = max_num_predicates - predicate_tensor.shape[0]\n",
        "        predicate_mask = np.ones_like(predicate_tensor).mean(1, keepdims=True)\n",
        "        predicate_tensor = np.pad(predicate_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
        "        predicate_mask = np.pad(predicate_mask, ((0, num_pad), (0, 0)), 'constant')\n",
        "        predicate_tensors.append(np.expand_dims(predicate_tensor, 0))\n",
        "        predicate_masks.append(np.expand_dims(predicate_mask, 0))\n",
        "    predicate_tensors = np.vstack(predicate_tensors)\n",
        "    predicate_tensors = torch.FloatTensor(predicate_tensors)\n",
        "    predicate_masks = np.vstack(predicate_masks)\n",
        "    predicate_masks = torch.FloatTensor(predicate_masks)\n",
        "\n",
        "    join_masks = []\n",
        "    join_tensors = []\n",
        "    for join in joins:\n",
        "        join_tensor = np.vstack(join)\n",
        "        num_pad = max_num_joins - join_tensor.shape[0]\n",
        "        join_mask = np.ones_like(join_tensor).mean(1, keepdims=True)\n",
        "        join_tensor = np.pad(join_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
        "        join_mask = np.pad(join_mask, ((0, num_pad), (0, 0)), 'constant')\n",
        "        join_tensors.append(np.expand_dims(join_tensor, 0))\n",
        "        join_masks.append(np.expand_dims(join_mask, 0))\n",
        "    join_tensors = np.vstack(join_tensors)\n",
        "    join_tensors = torch.FloatTensor(join_tensors)\n",
        "    join_masks = np.vstack(join_masks)\n",
        "    join_masks = torch.FloatTensor(join_masks)\n",
        "\n",
        "    target_tensor = torch.FloatTensor(labels)\n",
        "\n",
        "    return dataset.TensorDataset(sample_tensors, predicate_tensors, join_tensors, target_tensor, sample_masks,\n",
        "                                 predicate_masks, join_masks)\n",
        "\n",
        "\n",
        "def get_train_datasets(num_queries, num_materialized_samples):\n",
        "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = load_and_encode_train_data(\n",
        "        num_queries, num_materialized_samples)\n",
        "    train_dataset = make_dataset(*train_data, labels=labels_train, max_num_joins=max_num_joins,\n",
        "                                 max_num_predicates=max_num_predicates)\n",
        "    print(\"Created TensorDataset for training data\")\n",
        "    test_dataset = make_dataset(*test_data, labels=labels_test, max_num_joins=max_num_joins,\n",
        "                                max_num_predicates=max_num_predicates)\n",
        "    print(\"Created TensorDataset for validation data\")\n",
        "    return dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_dataset, test_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1DS3pzrOHku"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Define model architecture\n",
        "\n",
        "class SetConv(nn.Module):\n",
        "    def __init__(self, sample_feats, predicate_feats, join_feats, hid_units):\n",
        "        super(SetConv, self).__init__()\n",
        "\n",
        "        self.input_dim = hid_units * 3\n",
        "        self.output_dim = 1\n",
        "        self.num_units = hid_units\n",
        "        \n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "        self.sample_mlp1 = nn.Linear(sample_feats, hid_units)\n",
        "        self.sample_mlp2 = nn.Linear(hid_units, hid_units)\n",
        "        self.predicate_mlp1 = nn.Linear(predicate_feats, hid_units)\n",
        "        self.predicate_mlp2 = nn.Linear(hid_units, hid_units)\n",
        "        self.join_mlp1 = nn.Linear(join_feats, hid_units)\n",
        "        self.join_mlp2 = nn.Linear(hid_units, hid_units)\n",
        "\n",
        "        self.out_mlp1 = BayesLinear_Normalq(self.input_dim, self.num_units, gaussian(0, 1))\n",
        "        self.out_mlp2 = BayesLinear_Normalq(self.num_units, 2*self.output_dim, gaussian(0, 1))\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "\n",
        "    def forward(self, samples, predicates, joins, sample_mask, predicate_mask, join_mask):\n",
        "        # samples has shape [batch_size x num_joins+1 x sample_feats]\n",
        "        # predicates has shape [batch_size x num_predicates x predicate_feats]\n",
        "        # joins has shape [batch_size x num_joins x join_feats]\n",
        "\n",
        "        hid_sample = F.relu(self.sample_mlp1(samples))\n",
        "        hid_sample = F.relu(self.sample_mlp2(hid_sample))\n",
        "        hid_sample = hid_sample * sample_mask  # Mask\n",
        "        hid_sample = torch.sum(hid_sample, dim=1, keepdim=False)\n",
        "        sample_norm = sample_mask.sum(1, keepdim=False)\n",
        "        hid_sample = hid_sample / sample_norm  # Calculate average only over non-masked parts\n",
        "\n",
        "        hid_predicate = F.relu(self.predicate_mlp1(predicates))\n",
        "        hid_predicate = F.relu(self.predicate_mlp2(hid_predicate))\n",
        "        hid_predicate = hid_predicate * predicate_mask\n",
        "        hid_predicate = torch.sum(hid_predicate, dim=1, keepdim=False)\n",
        "        predicate_norm = predicate_mask.sum(1, keepdim=False)\n",
        "        hid_predicate = hid_predicate / predicate_norm\n",
        "\n",
        "        hid_join = F.relu(self.join_mlp1(joins))\n",
        "        hid_join = F.relu(self.join_mlp2(hid_join))\n",
        "        hid_join = hid_join * join_mask\n",
        "        hid_join = torch.sum(hid_join, dim=1, keepdim=False)\n",
        "        join_norm = join_mask.sum(1, keepdim=False)\n",
        "        hid_join = hid_join / join_norm\n",
        "\n",
        "        x = torch.cat((hid_sample, hid_predicate, hid_join), 1)\n",
        "        \n",
        "        KL_loss_total = 0\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        x, KL_loss = self.out_mlp1(x)\n",
        "        KL_loss_total = KL_loss_total + KL_loss\n",
        "        x = self.activation(x)\n",
        "        x, KL_loss = self.out_mlp2(x)\n",
        "        KL_loss_total = KL_loss_total + KL_loss\n",
        "\n",
        "        #out = torch.sigmoid(self.out_mlp2(hid))\n",
        "        #x = torch.sigmoid(x)\n",
        "        #x = self.activation(x)\n",
        "        return x, KL_loss_total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs7WTq9OOO83"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def unnormalize_torch(vals, min_val, max_val):\n",
        "    vals = (vals * (max_val - min_val)) + min_val\n",
        "    return torch.exp(vals)\n",
        "\n",
        "\n",
        "def qerror_loss(preds, targets, sigma, no_dim, min_val, max_val):\n",
        "    qerror = []\n",
        "    preds = unnormalize_torch(preds, min_val, max_val)\n",
        "    #sigma = unnormalize_torch(sigma, min_val, max_val)\n",
        "    targets = unnormalize_torch(targets, min_val, max_val)\n",
        "    #preds = torch.normal()\n",
        "    for i in range(len(targets)):\n",
        "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
        "            qerror.append(preds[i] / targets[i])\n",
        "        else:\n",
        "            qerror.append(targets[i] / preds[i])\n",
        "    return torch.mean(torch.cat(qerror))\n",
        "\n",
        "def qerror_gaussian_loss_we_sample_it_version(output, target, sigma, no_dim, min_val, max_val):\n",
        "    sum_reduce = False\n",
        "    #exponent = 0.5*(torch.sigmoid(output).cpu())**2\n",
        "    #exponent = exponent.cpu()/sigma.cpu()**2\n",
        "    #log_coeff = -no_dim*torch.log(sigma.cpu()) - 0.5*no_dim*np.log(2*np.pi)\n",
        "\n",
        "    preds = unnormalize_torch(output, min_val, max_val)    \n",
        "    sigma = unnormalize_torch(sigma, min_val, max_val)\n",
        "    targets = unnormalize_torch(target, min_val, max_val).cpu()\n",
        "    actual_preds = torch.normal (preds, sigma)\n",
        "    qerror = []\n",
        "    for i in range(len(targets)):\n",
        "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
        "            qerror.append(preds[i] / targets[i])\n",
        "        else:\n",
        "            qerror.append(targets[i] / preds[i])\n",
        "    qerror = torch.cat(qerror)\n",
        "    #qerror, _, _ = normalize_labels(torch.cat(qerror), min_val, max_val)\n",
        "    #qerror = torch.FloatTensor(qerror)\n",
        "    \n",
        "    if sum_reduce:\n",
        "        #return -(log_coeff + exponent).sum()\n",
        "        return torch.mean(qerror)\n",
        "    else:\n",
        "        return torch.mean(qerror)\n",
        "    \n",
        "def predict(model, data_loader, cuda):\n",
        "    preds = []\n",
        "    vars = []\n",
        "    t_total = 0.\n",
        "\n",
        "    model.eval()\n",
        "    for batch_idx, data_batch in enumerate(data_loader):\n",
        "\n",
        "        samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
        "\n",
        "        if cuda:\n",
        "            samples, predicates, joins, targets = samples.cuda(), predicates.cuda(), joins.cuda(), targets.cuda()\n",
        "            sample_masks, predicate_masks, join_masks = sample_masks.cuda(), predicate_masks.cuda(), join_masks.cuda()\n",
        "        samples, predicates, joins, targets = Variable(samples), Variable(predicates), Variable(joins), Variable(\n",
        "            targets)\n",
        "        sample_masks, predicate_masks, join_masks = Variable(sample_masks), Variable(predicate_masks), Variable(\n",
        "            join_masks)\n",
        "\n",
        "        t = time.time()\n",
        "        outputs, kl_loss = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
        "        t_total += time.time() - t\n",
        "\n",
        "        for i in range(outputs.data.shape[0]):\n",
        "            preds.append(outputs.data[i][0])\n",
        "            vars.append(outputs.data[i][1])\n",
        "    return preds, t_total, vars\n",
        "\n",
        "\n",
        "def print_qerror(preds_unnorm, labels_unnorm):\n",
        "    qerror = []\n",
        "    for i in range(len(preds_unnorm)):\n",
        "        if preds_unnorm[i] > float(labels_unnorm[i]):\n",
        "            qerror.append(float(preds_unnorm[i]) / float(labels_unnorm[i]) if float(labels_unnorm[i])!=0 else float(preds_unnorm[i]) / float(0.0001))\n",
        "        else:\n",
        "            qerror.append(float(labels_unnorm[i]) / float(preds_unnorm[i]) if float(preds_unnorm[i])!=0 else float(labels_unnorm[i]) / float(0.0001))\n",
        "\n",
        "    print(\"Median: {}\".format(np.median(qerror)))\n",
        "    print(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
        "    print(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
        "    print(\"99th percentile: {}\".format(np.percentile(qerror, 99)))\n",
        "    print(\"Max: {}\".format(np.max(qerror)))\n",
        "    print(\"Mean: {}\".format(np.mean(qerror)))\n",
        "\n",
        "\n",
        "def train_and_predict(workload_name, num_queries, num_epochs, batch_size, hid_units, cuda):\n",
        "    # Load training and validation data\n",
        "    num_materialized_samples = 1000\n",
        "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(\n",
        "        num_queries, num_materialized_samples)\n",
        "    table2vec, column2vec, op2vec, join2vec = dicts\n",
        "\n",
        "    # Train model\n",
        "    sample_feats = len(table2vec) + num_materialized_samples\n",
        "    predicate_feats = len(column2vec) + len(op2vec) + 1\n",
        "    join_feats = len(join2vec)\n",
        "\n",
        "    model = SetConv(sample_feats, predicate_feats, join_feats, hid_units)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "    no_samples = 20\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        loss_total = 0.\n",
        "\n",
        "        for batch_idx, data_batch in enumerate(train_data_loader):\n",
        "\n",
        "            samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
        "\n",
        "            if cuda:\n",
        "                samples, predicates, joins, targets = samples.cuda(), predicates.cuda(), joins.cuda(), targets.cuda()\n",
        "                sample_masks, predicate_masks, join_masks = sample_masks.cuda(), predicate_masks.cuda(), join_masks.cuda()\n",
        "            samples, predicates, joins, targets = Variable(samples), Variable(predicates), Variable(joins), Variable(\n",
        "                targets)\n",
        "            sample_masks, predicate_masks, join_masks = Variable(sample_masks), Variable(predicate_masks), Variable(\n",
        "                join_masks)\n",
        "            fit_loss_total = 0\n",
        "            kloss_total = 0\n",
        "            optimizer.zero_grad()\n",
        "            for i in range(no_samples):\n",
        "                output, KL_loss_total = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
        "                # calculate fit loss based on mean and standard deviation of output\n",
        "                fit_loss = qerror_gaussian_loss_we_sample_it_version(output[:, :1], targets.float(), output[:, 1:].exp(), 1, min_val, max_val)\n",
        "                #fit_loss = qerror_loss(output[:, :1], targets.float(), output[:, 1:].exp(), 1, min_val, max_val)\n",
        "                #fit_loss = log_gaussian_loss(output[:, :1], targets.float(), output[:, 1:].exp(), 1)\n",
        "                fit_loss_total = fit_loss_total + fit_loss\n",
        "                kloss_total =KL_loss_total\n",
        "        \n",
        "            #KL_loss_total = KL_loss_total/ len(train_data_loader)\n",
        "            total_loss = (fit_loss_total + kloss_total)/(no_samples*samples.shape[0])\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            loss_total += fit_loss_total/no_samples\n",
        "            \n",
        "        print(\"Epoch {}, loss: {}\".format(epoch, loss_total / len(train_data_loader)))\n",
        "\n",
        "    # Get final training and validation set predictions\n",
        "    preds_train, t_total, extra_train = predict(model, train_data_loader, cuda)\n",
        "    print(\"Prediction time per training sample: {}\".format(t_total / len(labels_train) * 1000))\n",
        "\n",
        "    preds_test, t_total, extra_val = predict(model, test_data_loader, cuda)\n",
        "    print(\"Prediction time per validation sample: {}\".format(t_total / len(labels_test) * 1000))\n",
        "\n",
        "    # Unnormalize\n",
        "    preds_train_unnorm = unnormalize_labels(preds_train, min_val, max_val)\n",
        "    labels_train_unnorm = unnormalize_labels(labels_train, min_val, max_val)\n",
        "\n",
        "    preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
        "    labels_test_unnorm = unnormalize_labels(labels_test, min_val, max_val)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nQ-Error training set:\")\n",
        "    print_qerror(preds_train_unnorm, labels_train_unnorm)\n",
        "\n",
        "    print(\"\\nQ-Error validation set:\")\n",
        "    print_qerror(preds_test_unnorm, labels_test_unnorm)\n",
        "    print(\"\")\n",
        "\n",
        "    # Load test data\n",
        "    file_name = \"/content/drive/MyDrive/Colab Notebooks/Database_Project/workloads/\" + workload_name\n",
        "    joins, predicates, tables, samples, label = load_data(file_name, num_materialized_samples)\n",
        "\n",
        "    # Get feature encoding and proper normalization\n",
        "    samples_test = encode_samples(tables, samples, table2vec)\n",
        "    predicates_test, joins_test = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
        "    labels_test, _, _ = normalize_labels(label, min_val, max_val)\n",
        "\n",
        "    print(\"Number of test samples: {}\".format(len(labels_test)))\n",
        "\n",
        "    max_num_predicates = max([len(p) for p in predicates_test])\n",
        "    max_num_joins = max([len(j) for j in joins_test])\n",
        "\n",
        "    # Get test set predictions\n",
        "    test_data = make_dataset(samples_test, predicates_test, joins_test, labels_test, max_num_joins, max_num_predicates)\n",
        "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "    preds_test, t_total, extra_test = predict(model, test_data_loader, cuda)\n",
        "    print(\"Prediction time per test sample: {}\".format(t_total / len(labels_test) * 1000))\n",
        "\n",
        "    # Unnormalize\n",
        "    preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
        "    extra_test_unnorm = unnormalize_labels(extra_test, min_val, max_val)\n",
        "    # Print metrics\n",
        "    print(\"\\nQ-Error \" + workload_name + \":\")\n",
        "    print_qerror(preds_test_unnorm, label)\n",
        "\n",
        "    # Write predictions\n",
        "    file_name = \"/content/predictions_\" + workload_name + \".csv\"\n",
        "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "    with open(file_name, \"w\") as f:\n",
        "        for i in range(len(preds_test_unnorm)):\n",
        "            f.write(str(preds_test_unnorm[i]) + \",+/-\"+ str(extra_test_unnorm[i]) + \",\"+label[i] + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "Enpc2M-YOVd_",
        "outputId": "282ae5c2-4f3a-4dd6-8330-623ab4ce58ab"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-39316609e1da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job-light\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"use CUDA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_and_predict' is not defined"
          ]
        }
      ],
      "source": [
        "train_and_predict(\"job-light\",10000,500,128,256,\"use CUDA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLK4QDAVep3n"
      },
      "source": [
        "# MSCN with BNNs, based on ProbFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41_DSAqjirRs",
        "outputId": "0989ed09-9e91-4ee8-fa57-1fff2aee51ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting probflow[pytorch]\n",
            "  Downloading probflow-2.4.1-py3-none-any.whl (93 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▌                            | 10 kB 30.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 20 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 30 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 40 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 51 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 93 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from probflow[pytorch]) (1.21.5)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from probflow[pytorch]) (1.3.0)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from probflow[pytorch]) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from probflow[pytorch]) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from probflow[pytorch]) (1.10.0+cu111)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->probflow[pytorch]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->probflow[pytorch]) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->probflow[pytorch]) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->probflow[pytorch]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->probflow[pytorch]) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.0->probflow[pytorch]) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->probflow[pytorch]) (3.10.0.2)\n",
            "Installing collected packages: probflow\n",
            "Successfully installed probflow-2.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install probflow[pytorch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttqPm20seuAN",
        "outputId": "deca8b8a-1296-4f3b-bf3b-f6813d86db22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rggx4LFyezTB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/Database_Project')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WVulSmegR8e"
      },
      "source": [
        "# MSCN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqnIbwwohoLc"
      },
      "source": [
        "UTIL-helper function for data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RqUmPu8e3IM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Helper functions for data processing\n",
        "\n",
        "def chunks(l, n):\n",
        "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
        "    for i in range(0, len(l), n):\n",
        "        yield l[i:i + n]\n",
        "\n",
        "\n",
        "def get_all_column_names(predicates):\n",
        "    column_names = set()\n",
        "    for query in predicates:\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                column_name = predicate[0]\n",
        "                column_names.add(column_name)\n",
        "    return column_names\n",
        "\n",
        "\n",
        "def get_all_table_names(tables):\n",
        "    table_names = set()\n",
        "    for query in tables:\n",
        "        for table in query:\n",
        "            table_names.add(table)\n",
        "    return table_names\n",
        "\n",
        "\n",
        "def get_all_operators(predicates):\n",
        "    operators = set()\n",
        "    for query in predicates:\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                operator = predicate[1]\n",
        "                operators.add(operator)\n",
        "    return operators\n",
        "\n",
        "\n",
        "def get_all_joins(joins):\n",
        "    join_set = set()\n",
        "    for query in joins:\n",
        "        for join in query:\n",
        "            join_set.add(join)\n",
        "    return join_set\n",
        "\n",
        "\n",
        "def idx_to_onehot(idx, num_elements):\n",
        "    onehot = np.zeros(num_elements, dtype=np.float32)\n",
        "    onehot[idx] = 1.\n",
        "    return onehot\n",
        "\n",
        "\n",
        "def get_set_encoding(source_set, onehot=True):\n",
        "    num_elements = len(source_set)\n",
        "    source_list = list(source_set)\n",
        "    # Sort list to avoid non-deterministic behavior\n",
        "    source_list.sort()\n",
        "    # Build map from s to i\n",
        "    thing2idx = {s: i for i, s in enumerate(source_list)}\n",
        "    # Build array (essentially a map from idx to s)\n",
        "    idx2thing = [s for i, s in enumerate(source_list)]\n",
        "    if onehot:\n",
        "        thing2vec = {s: idx_to_onehot(i, num_elements) for i, s in enumerate(source_list)}\n",
        "        return thing2vec, idx2thing\n",
        "    return thing2idx, idx2thing\n",
        "\n",
        "\n",
        "def get_min_max_vals(predicates, column_names):\n",
        "    min_max_vals = {t: [float('inf'), float('-inf')] for t in column_names}\n",
        "    for query in predicates:\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                column_name = predicate[0]\n",
        "                val = float(predicate[2])\n",
        "                if val < min_max_vals[column_name][0]:\n",
        "                    min_max_vals[column_name][0] = val\n",
        "                if val > min_max_vals[column_name][1]:\n",
        "                    min_max_vals[column_name][1] = val\n",
        "    return min_max_vals\n",
        "\n",
        "\n",
        "def normalize_data(val, column_name, column_min_max_vals):\n",
        "    min_val = column_min_max_vals[column_name][0]\n",
        "    max_val = column_min_max_vals[column_name][1]\n",
        "    val = float(val)\n",
        "    val_norm = 0.0\n",
        "    if max_val > min_val:\n",
        "        val_norm = (val - min_val) / (max_val - min_val)\n",
        "    return np.array(val_norm, dtype=np.float32)\n",
        "\n",
        "\n",
        "def normalize_labels(labels, min_val=None, max_val=None):\n",
        "    labels = np.array([np.log(float(l)) for l in labels])\n",
        "    if min_val is None:\n",
        "        min_val = labels.min()\n",
        "        print(\"min log(label): {}\".format(min_val))\n",
        "    if max_val is None:\n",
        "        max_val = labels.max()\n",
        "        print(\"max log(label): {}\".format(max_val))\n",
        "    labels_norm = (labels - min_val) / (max_val - min_val)\n",
        "    # Threshold labels\n",
        "    labels_norm = np.minimum(labels_norm, 1)\n",
        "    labels_norm = np.maximum(labels_norm, 0)\n",
        "    return labels_norm, min_val, max_val\n",
        "\n",
        "\n",
        "def unnormalize_labels(labels_norm, min_val, max_val):\n",
        "    labels_norm = np.array(labels_norm, dtype=np.float32)\n",
        "    labels = (labels_norm * (max_val - min_val)) + min_val\n",
        "    return np.array(np.round(np.exp(labels)), dtype=np.int64)\n",
        "\n",
        "\n",
        "def encode_samples(tables, samples, table2vec):\n",
        "    samples_enc = []\n",
        "    for i, query in enumerate(tables):\n",
        "        samples_enc.append(list())\n",
        "        for j, table in enumerate(query):\n",
        "            sample_vec = []\n",
        "            # Append table one-hot vector\n",
        "            sample_vec.append(table2vec[table])\n",
        "            # Append bit vector\n",
        "            sample_vec.append(samples[i][j])\n",
        "            sample_vec = np.hstack(sample_vec)\n",
        "            samples_enc[i].append(sample_vec)\n",
        "    return samples_enc\n",
        "\n",
        "\n",
        "def encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec):\n",
        "    predicates_enc = []\n",
        "    joins_enc = []\n",
        "    for i, query in enumerate(predicates):\n",
        "        predicates_enc.append(list())\n",
        "        joins_enc.append(list())\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                # Proper predicate\n",
        "                column = predicate[0]\n",
        "                operator = predicate[1]\n",
        "                val = predicate[2]\n",
        "                norm_val = normalize_data(val, column, column_min_max_vals)\n",
        "\n",
        "                pred_vec = []\n",
        "                pred_vec.append(column2vec[column])\n",
        "                pred_vec.append(op2vec[operator])\n",
        "                pred_vec.append(norm_val)\n",
        "                pred_vec = np.hstack(pred_vec)\n",
        "            else:\n",
        "                pred_vec = np.zeros((len(column2vec) + len(op2vec) + 1))\n",
        "\n",
        "            predicates_enc[i].append(pred_vec)\n",
        "\n",
        "        for predicate in joins[i]:\n",
        "            # Join instruction\n",
        "            join_vec = join2vec[predicate]\n",
        "            joins_enc[i].append(join_vec)\n",
        "    return predicates_enc, joins_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw8Zh6gHhwg8"
      },
      "source": [
        "**Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYy-oyjjkWTf"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import torch\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "\n",
        "def load_data(file_name, num_materialized_samples):\n",
        "    joins = []\n",
        "    predicates = []\n",
        "    tables = []\n",
        "    samples = []\n",
        "    label = []\n",
        "\n",
        "    # Load queries\n",
        "    with open(file_name + \".csv\") as f:\n",
        "        data_raw = list(list(rec) for rec in csv.reader(f, delimiter='#'))\n",
        "        for row in data_raw:\n",
        "            tables.append(row[0].split(','))\n",
        "            joins.append(row[1].split(','))\n",
        "            predicates.append(row[2].split(','))\n",
        "            if int(row[3]) < 1:\n",
        "                print(\"Queries must have non-zero cardinalities\")\n",
        "                exit(1)\n",
        "            label.append(row[3])\n",
        "    print(\"Loaded queries\")\n",
        "\n",
        "    # Load bitmaps\n",
        "    num_bytes_per_bitmap = int((num_materialized_samples + 7) >> 3)\n",
        "    with open(file_name + \".bitmaps\", 'rb') as f:\n",
        "        for i in range(len(tables)):\n",
        "            four_bytes = f.read(4)\n",
        "            if not four_bytes:\n",
        "                print(\"Error while reading 'four_bytes'\")\n",
        "                exit(1)\n",
        "            num_bitmaps_curr_query = int.from_bytes(four_bytes, byteorder='little')\n",
        "            bitmaps = np.empty((num_bitmaps_curr_query, num_bytes_per_bitmap * 8), dtype=np.uint8)\n",
        "            for j in range(num_bitmaps_curr_query):\n",
        "                # Read bitmap\n",
        "                bitmap_bytes = f.read(num_bytes_per_bitmap)\n",
        "                if not bitmap_bytes:\n",
        "                    print(\"Error while reading 'bitmap_bytes'\")\n",
        "                    exit(1)\n",
        "                bitmaps[j] = np.unpackbits(np.frombuffer(bitmap_bytes, dtype=np.uint8))\n",
        "            samples.append(bitmaps)\n",
        "    print(\"Loaded bitmaps\")\n",
        "\n",
        "    # Split predicates\n",
        "    predicates = [list(chunks(d, 3)) for d in predicates]\n",
        "\n",
        "    return joins, predicates, tables, samples, label\n",
        "\n",
        "\n",
        "def load_and_encode_train_data(num_queries, num_materialized_samples):\n",
        "    file_name_queries = \"/content/drive/MyDrive/Colab Notebooks/Database_Project/data/train\"\n",
        "    file_name_column_min_max_vals = \"/content/drive/MyDrive/Colab Notebooks/Database_Project/data/column_min_max_vals.csv\"\n",
        "\n",
        "    joins, predicates, tables, samples, label = load_data(file_name_queries, num_materialized_samples)\n",
        "\n",
        "    # Get column name dict\n",
        "    column_names = get_all_column_names(predicates)\n",
        "    column2vec, idx2column = get_set_encoding(column_names)\n",
        "\n",
        "    # Get table name dict\n",
        "    table_names = get_all_table_names(tables)\n",
        "    table2vec, idx2table = get_set_encoding(table_names)\n",
        "\n",
        "    # Get operator name dict\n",
        "    operators = get_all_operators(predicates)\n",
        "    op2vec, idx2op = get_set_encoding(operators)\n",
        "\n",
        "    # Get join name dict\n",
        "    join_set = get_all_joins(joins)\n",
        "    join2vec, idx2join = get_set_encoding(join_set)\n",
        "\n",
        "    # Get min and max values for each column\n",
        "    with open(file_name_column_min_max_vals) as f:\n",
        "        data_raw = list(list(rec) for rec in csv.reader(f, delimiter=','))\n",
        "        column_min_max_vals = {}\n",
        "        for i, row in enumerate(data_raw):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            column_min_max_vals[row[0]] = [float(row[1]), float(row[2])]\n",
        "\n",
        "    # Get feature encoding and proper normalization\n",
        "    samples_enc = encode_samples(tables, samples, table2vec)\n",
        "    predicates_enc, joins_enc = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
        "    label_norm, min_val, max_val = normalize_labels(label)\n",
        "\n",
        "    # Split in training and validation samples\n",
        "    num_train = int(num_queries * 0.9)\n",
        "    num_test = num_queries - num_train\n",
        "\n",
        "    samples_train = samples_enc[:num_train]\n",
        "    predicates_train = predicates_enc[:num_train]\n",
        "    joins_train = joins_enc[:num_train]\n",
        "    labels_train = label_norm[:num_train]\n",
        "\n",
        "    samples_test = samples_enc[num_train:num_train + num_test]\n",
        "    predicates_test = predicates_enc[num_train:num_train + num_test]\n",
        "    joins_test = joins_enc[num_train:num_train + num_test]\n",
        "    labels_test = label_norm[num_train:num_train + num_test]\n",
        "\n",
        "    print(\"Number of training samples: {}\".format(len(labels_train)))\n",
        "    print(\"Number of validation samples: {}\".format(len(labels_test)))\n",
        "\n",
        "    max_num_joins = max(max([len(j) for j in joins_train]), max([len(j) for j in joins_test]))\n",
        "    max_num_predicates = max(max([len(p) for p in predicates_train]), max([len(p) for p in predicates_test]))\n",
        "\n",
        "    dicts = [table2vec, column2vec, op2vec, join2vec]\n",
        "    train_data = [samples_train, predicates_train, joins_train]\n",
        "    test_data = [samples_test, predicates_test, joins_test]\n",
        "    return dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data\n",
        "\n",
        "\n",
        "def make_dataset(samples, predicates, joins, labels, max_num_joins, max_num_predicates):\n",
        "    \"\"\"Add zero-padding and wrap as tensor dataset.\"\"\"\n",
        "\n",
        "    sample_masks = []\n",
        "    sample_tensors = []\n",
        "    for sample in samples:\n",
        "        sample_tensor = np.vstack(sample)\n",
        "        num_pad = max_num_joins + 1 - sample_tensor.shape[0]\n",
        "        sample_mask = np.ones_like(sample_tensor).mean(1, keepdims=True)\n",
        "        sample_tensor = np.pad(sample_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
        "        sample_mask = np.pad(sample_mask, ((0, num_pad), (0, 0)), 'constant')\n",
        "        sample_tensors.append(np.expand_dims(sample_tensor, 0))\n",
        "        sample_masks.append(np.expand_dims(sample_mask, 0))\n",
        "    sample_tensors = np.vstack(sample_tensors)\n",
        "    sample_tensors = torch.FloatTensor(sample_tensors)\n",
        "    #sample_tensors.requires_grad_(False)\n",
        "    sample_masks = np.vstack(sample_masks)\n",
        "    sample_masks = torch.FloatTensor(sample_masks)\n",
        "    #sample_masks.requires_grad_(False)\n",
        "\n",
        "    predicate_masks = []\n",
        "    predicate_tensors = []\n",
        "    for predicate in predicates:\n",
        "        predicate_tensor = np.vstack(predicate)\n",
        "        num_pad = max_num_predicates - predicate_tensor.shape[0]\n",
        "        predicate_mask = np.ones_like(predicate_tensor).mean(1, keepdims=True)\n",
        "        predicate_tensor = np.pad(predicate_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
        "        predicate_mask = np.pad(predicate_mask, ((0, num_pad), (0, 0)), 'constant')\n",
        "        predicate_tensors.append(np.expand_dims(predicate_tensor, 0))\n",
        "        predicate_masks.append(np.expand_dims(predicate_mask, 0))\n",
        "    predicate_tensors = np.vstack(predicate_tensors)\n",
        "    predicate_tensors = torch.FloatTensor(predicate_tensors)\n",
        "    #predicate_tensors.requires_grad_(False) ####???\n",
        "    predicate_masks = np.vstack(predicate_masks)\n",
        "    predicate_masks = torch.FloatTensor(predicate_masks)\n",
        "    #predicate_masks.requires_grad_(False) ####?????\n",
        "\n",
        "    join_masks = []\n",
        "    join_tensors = []\n",
        "    for join in joins:\n",
        "        join_tensor = np.vstack(join)\n",
        "        num_pad = max_num_joins - join_tensor.shape[0]\n",
        "        join_mask = np.ones_like(join_tensor).mean(1, keepdims=True)\n",
        "        join_tensor = np.pad(join_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
        "        join_mask = np.pad(join_mask, ((0, num_pad), (0, 0)), 'constant')\n",
        "        join_tensors.append(np.expand_dims(join_tensor, 0))\n",
        "        join_masks.append(np.expand_dims(join_mask, 0))\n",
        "    join_tensors = np.vstack(join_tensors)\n",
        "    join_tensors = torch.FloatTensor(join_tensors)\n",
        "    join_masks = np.vstack(join_masks)\n",
        "    join_masks = torch.FloatTensor(join_masks)\n",
        "    #join_tensors.requires_grad_(False) #####???\n",
        "    #join_masks.requires_grad_(False) #####????\n",
        "\n",
        "    target_tensor = torch.FloatTensor(labels)\n",
        "    #target_tensor.requires_grad_(False)\n",
        "\n",
        "    ds = dataset.TensorDataset(sample_tensors, predicate_tensors, join_tensors, target_tensor, sample_masks,\n",
        "                                 predicate_masks, join_masks)\n",
        "    #ds.requires_grad_(False) \n",
        "    return ds\n",
        "\n",
        "\n",
        "def get_train_datasets(num_queries, num_materialized_samples):\n",
        "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = load_and_encode_train_data(\n",
        "        num_queries, num_materialized_samples)\n",
        "    train_dataset = make_dataset(*train_data, labels=labels_train, max_num_joins=max_num_joins,\n",
        "                                 max_num_predicates=max_num_predicates)\n",
        "    print(\"Created TensorDataset for training data\")\n",
        "    test_dataset = make_dataset(*test_data, labels=labels_test, max_num_joins=max_num_joins,\n",
        "                                max_num_predicates=max_num_predicates)\n",
        "    print(\"Created TensorDataset for validation data\")\n",
        "    return dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_dataset, test_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4iBHjTxkc8F"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfPrRNyRgeNc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Define model architecture\n",
        "\n",
        "class SetConv(nn.Module):\n",
        "    def __init__(self, sample_feats, predicate_feats, join_feats, hid_units):\n",
        "        super(SetConv, self).__init__()\n",
        "        self.sample_mlp1 = nn.Linear(sample_feats, hid_units)\n",
        "        self.sample_mlp2 = nn.Linear(hid_units, hid_units)\n",
        "        self.predicate_mlp1 = nn.Linear(predicate_feats, hid_units)\n",
        "        self.predicate_mlp2 = nn.Linear(hid_units, hid_units)\n",
        "        self.join_mlp1 = nn.Linear(join_feats, hid_units)\n",
        "        self.join_mlp2 = nn.Linear(hid_units, hid_units)\n",
        "        self.out_mlp1 = nn.Linear(hid_units * 3, hid_units)\n",
        "        self.out_mlp2 = nn.Linear(hid_units, 1)\n",
        "\n",
        "    def forward(self, samples, predicates, joins, sample_mask, predicate_mask, join_mask):\n",
        "        # samples has shape [batch_size x num_joins+1 x sample_feats]\n",
        "        # predicates has shape [batch_size x num_predicates x predicate_feats]\n",
        "        # joins has shape [batch_size x num_joins x join_feats]\n",
        "\n",
        "        hid_sample = F.relu(self.sample_mlp1(samples))\n",
        "        hid_sample = F.relu(self.sample_mlp2(hid_sample))\n",
        "        hid_sample = hid_sample * sample_mask  # Mask\n",
        "        hid_sample = torch.sum(hid_sample, dim=1, keepdim=False)\n",
        "        sample_norm = sample_mask.sum(1, keepdim=False)\n",
        "        hid_sample = hid_sample / sample_norm  # Calculate average only over non-masked parts\n",
        "\n",
        "        hid_predicate = F.relu(self.predicate_mlp1(predicates))\n",
        "        hid_predicate = F.relu(self.predicate_mlp2(hid_predicate))\n",
        "        hid_predicate = hid_predicate * predicate_mask\n",
        "        hid_predicate = torch.sum(hid_predicate, dim=1, keepdim=False)\n",
        "        predicate_norm = predicate_mask.sum(1, keepdim=False)\n",
        "        hid_predicate = hid_predicate / predicate_norm\n",
        "\n",
        "        hid_join = F.relu(self.join_mlp1(joins))\n",
        "        hid_join = F.relu(self.join_mlp2(hid_join))\n",
        "        hid_join = hid_join * join_mask\n",
        "        hid_join = torch.sum(hid_join, dim=1, keepdim=False)\n",
        "        join_norm = join_mask.sum(1, keepdim=False)\n",
        "        hid_join = hid_join / join_norm\n",
        "\n",
        "        hid = torch.cat((hid_sample, hid_predicate, hid_join), 1)\n",
        "        hid = F.relu(self.out_mlp1(hid))\n",
        "        hid2 = F.relu(self.out_mlp2(hid))#### for adding bnn in the last layer here?\n",
        "        out = torch.sigmoid(hid2) ##bnn output through sigmoid\n",
        "        return out, hid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oouV2wVmLG8"
      },
      "source": [
        "# Training MSCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Wb1iOvmgyiJ"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def unnormalize_torch(vals, min_val, max_val):\n",
        "    vals = (vals * (max_val - min_val)) + min_val\n",
        "    return torch.exp(vals)\n",
        "\n",
        "\n",
        "def qerror_loss(preds, targets, min_val, max_val):\n",
        "    qerror = []\n",
        "    preds = unnormalize_torch(preds, min_val, max_val)\n",
        "    targets = unnormalize_torch(targets, min_val, max_val)\n",
        "\n",
        "    for i in range(len(targets)):\n",
        "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
        "            qerror.append(preds[i] / targets[i])\n",
        "        else:\n",
        "            qerror.append(targets[i] / preds[i])\n",
        "    return torch.mean(torch.cat(qerror))\n",
        "\n",
        "\n",
        "def predict(model, data_loader, cuda):\n",
        "    preds = []\n",
        "    t_total = 0.\n",
        "\n",
        "    model.eval()\n",
        "    for batch_idx, data_batch in enumerate(data_loader):\n",
        "\n",
        "        samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
        "\n",
        "        if cuda:\n",
        "            samples, predicates, joins, targets = samples.cuda(), predicates.cuda(), joins.cuda(), targets.cuda()\n",
        "            sample_masks, predicate_masks, join_masks = sample_masks.cuda(), predicate_masks.cuda(), join_masks.cuda()\n",
        "        samples, predicates, joins, targets = Variable(samples), Variable(predicates), Variable(joins), Variable(\n",
        "            targets)\n",
        "        sample_masks, predicate_masks, join_masks = Variable(sample_masks), Variable(predicate_masks), Variable(\n",
        "            join_masks)\n",
        "\n",
        "        t = time.time()\n",
        "        outputs, hid = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
        "        t_total += time.time() - t\n",
        "\n",
        "        for i in range(outputs.data.shape[0]):\n",
        "            preds.append(outputs.data[i])\n",
        "\n",
        "    return preds, t_total\n",
        "\n",
        "\n",
        "def print_qerror(preds_unnorm, labels_unnorm):\n",
        "    qerror = []\n",
        "    for i in range(len(preds_unnorm)):\n",
        "        if preds_unnorm[i] > float(labels_unnorm[i]) and float(labels_unnorm[i])!=0:   #### logic?\n",
        "            qerror.append(preds_unnorm[i] / float(labels_unnorm[i]))\n",
        "        elif float(preds_unnorm[i])!=0:\n",
        "            qerror.append(float(labels_unnorm[i]) / float(preds_unnorm[i]))\n",
        "\n",
        "    print(\"Median: {}\".format(np.median(qerror)))\n",
        "    print(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
        "    print(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
        "    print(\"99th percentile: {}\".format(np.percentile(qerror, 99)))\n",
        "    print(\"Max: {}\".format(np.max(qerror)))\n",
        "    print(\"Mean: {}\".format(np.mean(qerror)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfmEUlE7g3__"
      },
      "outputs": [],
      "source": [
        "def train_and_predict_mscn(workload_name, num_queries, num_epochs, batch_size, hid_units, cuda):\n",
        "    # Load training and validation data\n",
        "    num_materialized_samples = 1000\n",
        "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(\n",
        "        num_queries, num_materialized_samples)\n",
        "    table2vec, column2vec, op2vec, join2vec = dicts\n",
        "\n",
        "    # Train model\n",
        "    sample_feats = len(table2vec) + num_materialized_samples\n",
        "    predicate_feats = len(column2vec) + len(op2vec) + 1\n",
        "    join_feats = len(join2vec)\n",
        "\n",
        "    model = SetConv(sample_feats, predicate_feats, join_feats, hid_units)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        loss_total = 0.\n",
        "\n",
        "        for batch_idx, data_batch in enumerate(train_data_loader):\n",
        "\n",
        "            samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
        "\n",
        "            if cuda:\n",
        "                samples, predicates, joins, targets = samples.cuda(), predicates.cuda(), joins.cuda(), targets.cuda()\n",
        "                sample_masks, predicate_masks, join_masks = sample_masks.cuda(), predicate_masks.cuda(), join_masks.cuda()\n",
        "            samples, predicates, joins, targets = Variable(samples), Variable(predicates), Variable(joins), Variable(\n",
        "                targets)\n",
        "            sample_masks, predicate_masks, join_masks = Variable(sample_masks), Variable(predicate_masks), Variable(\n",
        "                join_masks)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs, hid = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks) #### for bnn \n",
        "            loss = qerror_loss(outputs, targets.float(), min_val, max_val)\n",
        "            loss_total += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Epoch {}, loss: {}\".format(epoch, loss_total / len(train_data_loader)))\n",
        "\n",
        "    # Get final training and validation set predictions\n",
        "    preds_train, t_total = predict(model, train_data_loader, cuda)\n",
        "    print(\"Prediction time per training sample: {}\".format(t_total / len(labels_train) * 1000))\n",
        "\n",
        "    preds_test, t_total = predict(model, test_data_loader, cuda)\n",
        "    print(\"Prediction time per validation sample: {}\".format(t_total / len(labels_test) * 1000))\n",
        "\n",
        "    # Unnormalize\n",
        "    preds_train_unnorm = unnormalize_labels(preds_train, min_val, max_val)\n",
        "    labels_train_unnorm = unnormalize_labels(labels_train, min_val, max_val)\n",
        "\n",
        "    preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
        "    labels_test_unnorm = unnormalize_labels(labels_test, min_val, max_val)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nQ-Error training set:\")\n",
        "    print_qerror(preds_train_unnorm, labels_train_unnorm)\n",
        "\n",
        "    print(\"\\nQ-Error validation set:\")\n",
        "    print_qerror(preds_test_unnorm, labels_test_unnorm)\n",
        "    print(\"\")\n",
        "\n",
        "    # Load test data\n",
        "    file_name = \"/content/drive/MyDrive/Colab Notebooks/Database_Project/workloads/\" + workload_name\n",
        "    joins, predicates, tables, samples, label = load_data(file_name, num_materialized_samples)\n",
        "\n",
        "    # Get feature encoding and proper normalization\n",
        "    samples_test = encode_samples(tables, samples, table2vec)\n",
        "    predicates_test, joins_test = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
        "    labels_test, _, _ = normalize_labels(label, min_val, max_val)\n",
        "\n",
        "    print(\"Number of test samples: {}\".format(len(labels_test)))\n",
        "\n",
        "    max_num_predicates = max([len(p) for p in predicates_test])\n",
        "    max_num_joins = max([len(j) for j in joins_test])\n",
        "\n",
        "    # Get test set predictions\n",
        "    test_data = make_dataset(samples_test, predicates_test, joins_test, labels_test, max_num_joins, max_num_predicates)\n",
        "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "    preds_test, t_total = predict(model, test_data_loader, cuda)\n",
        "    print(\"Prediction time per test sample: {}\".format(t_total / len(labels_test) * 1000))\n",
        "\n",
        "    # Unnormalize\n",
        "    preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nQ-Error \" + workload_name + \":\")\n",
        "    print_qerror(preds_test_unnorm, label)\n",
        "\n",
        "    # Write predictions\n",
        "    file_name = \"/content/basic_mscn_predictions_\" + workload_name + \".csv\"\n",
        "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "    with open(file_name, \"w\") as f:\n",
        "        for i in range(len(preds_test_unnorm)):\n",
        "            f.write(str(preds_test_unnorm[i]) + \",\" + label[i] + \"\\n\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "2t3SKrXNhEka",
        "outputId": "9f3efbed-353b-432f-bac6-bb7244effd0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded queries\n",
            "Loaded bitmaps\n",
            "min log(label): 0.0\n",
            "max log(label): 19.94772801931604\n",
            "Number of training samples: 90\n",
            "Number of validation samples: 10\n",
            "Created TensorDataset for training data\n",
            "Created TensorDataset for validation data\n",
            "Epoch 0, loss: 587.7970581054688\n",
            "Epoch 1, loss: 818.1390991210938\n",
            "Prediction time per training sample: 0.00679228040907118\n",
            "Prediction time per validation sample: 0.05905628204345703\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-797548d086a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mour_mscn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_predict_mscn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job-light\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"use CUDA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-c217f13ffb5e>\u001b[0m in \u001b[0;36mtrain_and_predict_mscn\u001b[0;34m(workload_name, num_queries, num_epochs, batch_size, hid_units, cuda)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Unnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mpreds_train_unnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munnormalize_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mlabels_train_unnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munnormalize_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-a1bcdda9f151>\u001b[0m in \u001b[0;36munnormalize_labels\u001b[0;34m(labels_norm, min_val, max_val)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munnormalize_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mlabels_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels_norm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;31m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ],
      "source": [
        "our_mscn_model = train_and_predict_mscn(\"job-light\",100,2,128,256,\"use CUDA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv0aEE-ryBF-"
      },
      "source": [
        "# Beta distribution for BNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5sqVqSlhfeM"
      },
      "outputs": [],
      "source": [
        "import probflow as pf\n",
        "from probflow.utils.validation import ensure_tensor_like\n",
        "from probflow.utils.casting import to_tensor\n",
        "class OurBeta(pf.BaseDistribution):\n",
        "    def __init__(self, loc=0, scale=1):\n",
        "\n",
        "        # Check input\n",
        "        ensure_tensor_like(loc, \"loc\")\n",
        "        ensure_tensor_like(scale, \"scale\")\n",
        "\n",
        "        # Store args\n",
        "        self.loc = loc\n",
        "        self.scale = scale\n",
        "\n",
        "    def __call__(self):\n",
        "        return torch.distributions.beta.Beta(self[\"loc\"], self[\"scale\"])\n",
        "        #return torch.distributions.kumaraswamy.Kumaraswamy(self[\"loc\"], self[\"scale\"])\n",
        "\n",
        "    def prob(self, y):\n",
        "      return torch.distributions.beta.Beta(self[\"loc\"], self[\"scale\"]).log_prob(to_tensor(y)).exp()\n",
        "      #return torch.distributions.kumaraswamy.Kumaraswamy(self[\"loc\"], self[\"scale\"]).log_prob(to_tensor(y)).exp()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt8fgUbYiCuO"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from probflow.data import make_generator\n",
        "from probflow.utils.settings import Sampling, get_backend\n",
        "from probflow.utils.base import BaseCallback\n",
        "import torch\n",
        "\n",
        "\n",
        "class OurContinuousModel(pf.ContinuousModel):  \n",
        "\n",
        "    def sample(self, data_loader, ed=None, axis=1, batch_size=None, method=\"mean\"):\n",
        "        \"\"\"Sample from the model\"\"\"\n",
        "        sampls = []\n",
        "        for batch_idx, data_batch in enumerate(data_loader):\n",
        "            data_batch = torch.from_numpy(data_batch)\n",
        "            data_batch.requires_grad_(False)   \n",
        "            dist = self(data_batch)\n",
        "            s = dist.sample(n=1000)\n",
        "            s = torch.as_tensor(s, device='cpu')\n",
        "            if method==\"mode\":\n",
        "                mode = torch.mode(s, axis=0).values\n",
        "                sampls += [mode]\n",
        "            if method==\"mean\":\n",
        "                mean = torch.mean(s, dim=0)\n",
        "                sampls += [mean]\n",
        "        return np.concatenate([x.detach().numpy() for x in sampls], axis=axis)\n",
        "\n",
        "    \"\"\"def our_sample(self, data_loader, func, ed=None, axis=1, batch_size=None):\n",
        "        #Sample from the model\n",
        "        sampls = []\n",
        "        for num, item in enumerate(data_loader):\n",
        "            item = torch.from_numpy(item).requires_grad_(False)\n",
        "            sampls += [func(self(item))]\n",
        "        return np.concatenate(to_numpy(sampls), axis=axis)\n",
        "    \"\"\"\n",
        "\n",
        "    def predict(self, data_loader, method=\"mean\", batch_size=None):\n",
        "        \"\"\"Predict dependent variable using the model\n",
        "        TODO... using maximum a posteriori param estimates etc\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : |ndarray| or |DataFrame| or |Series| or |DataGenerator|\n",
        "            Independent variable values of the dataset to evaluate (aka the\n",
        "            \"features\").\n",
        "        method : str\n",
        "            Method to use for prediction.  If ``'mean'``, uses the mean of the\n",
        "            predicted target distribution as the prediction.  If ``'mode'``,\n",
        "            uses the mode of the distribution.\n",
        "        batch_size : None or int\n",
        "            Compute using batches of this many datapoints.  Default is `None`\n",
        "            (i.e., do not use batching).\n",
        "        Returns\n",
        "        -------\n",
        "        |ndarray|\n",
        "            Predicted y-value for each sample in ``x``.  Of size\n",
        "            (x.shape[0], y.shape[0], ..., y.shape[-1])\n",
        "        Examples\n",
        "        --------\n",
        "        TODO: Docs...\n",
        "        \"\"\"\n",
        "        return self.sample(data_loader, axis=0, batch_size=batch_size, method=method)\n",
        "\n",
        "    def our_train_step_pytorch(self, n, flipout=False, eager=False, n_mc=1):\n",
        "        \"\"\"Get the training step function for PyTorch\"\"\"\n",
        "\n",
        "        import torch\n",
        "\n",
        "        if eager:\n",
        "\n",
        "            def train_fn(x_data, y_data):\n",
        "                self.reset_kl_loss()\n",
        "                with Sampling(n=n_mc, flipout=flipout):\n",
        "                    self._optimizer.zero_grad()\n",
        "                    elbo_loss = self.elbo_loss(x_data, y_data, n, n_mc)\n",
        "                    elbo_loss.backward()\n",
        "                    torch.nn.utils.clip_grad_value_(self.trainable_variables, 10000)\n",
        "                    self._optimizer.step()\n",
        "                return elbo_loss\n",
        "\n",
        "            return train_fn\n",
        "\n",
        "        # Use PyTorch tracing, for which we have to build a module :roll_eyes:\n",
        "        # and also a caching class for inputs of different sizes, b/c\n",
        "        # last batch might have different number of datapoints :vomiting_face:\n",
        "        else:\n",
        "\n",
        "            class PyTorchModule(torch.nn.Module):\n",
        "                def __init__(self, model):\n",
        "                    super(PyTorchModule, self).__init__()\n",
        "                    for i, p in enumerate(model.trainable_variables):\n",
        "                        setattr(self, str(i), p)\n",
        "                    self._probflow_model = model\n",
        "\n",
        "                def elbo_loss(self, *args):\n",
        "                    self._probflow_model.reset_kl_loss()\n",
        "                    with Sampling(n=n_mc, flipout=flipout):\n",
        "                        if len(args) == 1:\n",
        "                            elbo_loss = self._probflow_model.elbo_loss(\n",
        "                                None, args[0], n, n_mc\n",
        "                            )\n",
        "                        else:\n",
        "                            elbo_loss = self._probflow_model.elbo_loss(\n",
        "                                args[0], args[1], n, n_mc\n",
        "                            )\n",
        "                    return elbo_loss\n",
        "\n",
        "            class TraceCacher:\n",
        "                \"\"\"Cache traces for inputs of different sizes\"\"\"\n",
        "\n",
        "                def __init__(self, model):\n",
        "                    self.fns = {}  # map from input shapes to traced function\n",
        "                    self.model = model\n",
        "\n",
        "                def get_traced_module(self, *args):\n",
        "                    shape = \"_\".join(str(e.shape) for e in args)\n",
        "                    if shape in self.fns:\n",
        "                        return self.fns[shape]\n",
        "                    else:\n",
        "                        m = PyTorchModule(self.model)\n",
        "                        inputs = {\"elbo_loss\": args}\n",
        "                        self.fns[shape] = torch.jit.trace_module(m, inputs)\n",
        "                        return self.fns[shape]\n",
        "\n",
        "                def __call__(self, *args):\n",
        "                    self.model._optimizer.zero_grad()\n",
        "                    traced_module = self.get_traced_module(*args)\n",
        "                    elbo_loss = traced_module.elbo_loss(*args)\n",
        "                    elbo_loss.backward()\n",
        "                    self.model._optimizer.step()\n",
        "                    return elbo_loss\n",
        "\n",
        "            pytorch_trainer = TraceCacher(self)\n",
        "\n",
        "            def train_fn(x_data, y_data):\n",
        "                if x_data is None:\n",
        "                    elbo_loss = pytorch_trainer(torch.tensor(y_data))\n",
        "                else:\n",
        "                    elbo_loss = pytorch_trainer(\n",
        "                        torch.tensor(x_data),\n",
        "                        torch.tensor(y_data),\n",
        "                    )\n",
        "                return elbo_loss\n",
        "\n",
        "            return train_fn\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        x,\n",
        "        y=None,\n",
        "        batch_size: int = 128,\n",
        "        epochs: int = 200,\n",
        "        shuffle: bool = False,\n",
        "        optimizer=None,\n",
        "        optimizer_kwargs: dict = {},\n",
        "        lr: float = None,\n",
        "        flipout: bool = True,\n",
        "        num_workers: int = None,\n",
        "        callbacks: List[BaseCallback] = [],\n",
        "        eager: bool = False,\n",
        "        n_mc: int = 1,\n",
        "    ):\n",
        "        r\"\"\"Fit the model to data\n",
        "        TODO\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : |ndarray| or |DataFrame| or |Series| or |DataGenerator|\n",
        "            Independent variable values (or, if fitting a generative model,\n",
        "            the dependent variable values).  Should be of shape (Nsamples,...)\n",
        "        y : |None| or |ndarray| or |DataFrame| or |Series|\n",
        "            Dependent variable values (or, if fitting a generative model,\n",
        "            ``None``). Should be of shape (Nsamples,...).  Default = ``None``\n",
        "        batch_size : int\n",
        "            Number of samples to use per minibatch.\n",
        "            Default = ``128``\n",
        "        epochs : int\n",
        "            Number of epochs to train the model.\n",
        "            Default = ``200``\n",
        "        shuffle : bool\n",
        "            Whether to shuffle the data each epoch.  Note that this is ignored\n",
        "            if ``x`` is a |DataGenerator|\n",
        "            Default = ``True``\n",
        "        optimizer : |None| or a backend-specific optimizer\n",
        "            What optimizer to use for optimizing the variational posterior\n",
        "            distributions' variables.  When the backend is |TensorFlow| the\n",
        "            default is to use adam (``tf.keras.optimizers.Adam``).  When the\n",
        "            backend is |PyTorch| the default is to use TODO\n",
        "        optimizer_kwargs : dict\n",
        "            Keyword arguments to pass to the optimizer.\n",
        "            Default is an empty dict.\n",
        "        lr : float\n",
        "            Learning rate for the optimizer.\n",
        "            Note that the learning rate can be updated during training using\n",
        "            the set_learning_rate method.\n",
        "            Default is :math:`\\exp (- \\log_{10} (N_p N_b))`, where :math:`N_p`\n",
        "            is the number of parameters in the model, and :math:`N_b` is the\n",
        "            number of samples per batch (``batch_size``).\n",
        "        flipout : bool\n",
        "            Whether to use flipout during training where possible\n",
        "            Default = True\n",
        "        num_workers : None or int > 0\n",
        "            Number of parallel processes to run for loading the data.  If\n",
        "            ``None``, will not use parallel processes.  If an integer, will use\n",
        "            a process pool with that many processes.  Note that this parameter\n",
        "            is ignored if a |DataGenerator| is passed as ``x``.  Default = None\n",
        "        callbacks : List[BaseCallback]\n",
        "            List of callbacks to run while training the model.  Default is\n",
        "            ``[]``, i.e. no callbacks.\n",
        "        eager : bool\n",
        "            Whether to use eager execution.  If False, will use ``tf.function``\n",
        "            (for TensorFlow) or tracing (for PyTorch) to optimize the model\n",
        "            fitting.  Note that even if eager=True, you can still use eager\n",
        "            execution when using the model after it is fit.  Default = False\n",
        "        n_mc : int\n",
        "            Number of monte carlo samples to take from the variational\n",
        "            posteriors per minibatch.  The default is to just take one per\n",
        "            batch.  Using a smaller number of MC samples is faster, but using a\n",
        "            greater number of MC samples will decrease the variance of the\n",
        "            gradients, leading to more stable parameter optimization.\n",
        "        Example\n",
        "        -------\n",
        "        See the user guide section on :doc:`/user_guide/fitting`.\n",
        "        \"\"\"\n",
        "\n",
        "        # Determine a somewhat reasonable learning rate if none was passed\n",
        "        if lr is not None:\n",
        "            self._learning_rate = lr\n",
        "        elif self._learning_rate is None:\n",
        "            default_lr = np.exp(-np.log10(self.n_parameters * batch_size))\n",
        "            self._learning_rate = default_lr\n",
        "\n",
        "        # Create DataGenerator from input data if not already\n",
        "        self._data = make_generator(\n",
        "            x,\n",
        "            y,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=num_workers,\n",
        "        )\n",
        "        \n",
        "        # Use default optimizer if none specified\n",
        "        if optimizer is None and self._optimizer is None:\n",
        "            if get_backend() == \"pytorch\":\n",
        "                import torch\n",
        "\n",
        "                #Parameter #1- Optimizer\n",
        "                self._optimizer = torch.optim.RMSprop(\n",
        "                    self.trainable_variables,\n",
        "                    lr=self._learning_rate,\n",
        "                    **optimizer_kwargs\n",
        "                )\n",
        "                \n",
        "\n",
        "            else:\n",
        "                import tensorflow as tf\n",
        "\n",
        "                self._optimizer = tf.keras.optimizers.RMSprop(\n",
        "                    lambda: self._learning_rate, **optimizer_kwargs\n",
        "                )\n",
        "\n",
        "        # Use eager if input type is dataframe or series\n",
        "        eager_types = (pd.DataFrame, pd.Series)\n",
        "        if any(isinstance(e, eager_types) for e in self._data.get_batch(0)):\n",
        "            eager = True\n",
        "\n",
        "        # Create a function to perform one training step\n",
        "        if get_backend() == \"pytorch\":\n",
        "            self._train_fn = self.our_train_step_pytorch(\n",
        "                self._data.n_samples, flipout, eager=eager, n_mc=n_mc\n",
        "            )\n",
        "        else:\n",
        "            self._train_fn = self._train_step_tensorflow(\n",
        "                self._data.n_samples, flipout, eager=eager, n_mc=n_mc\n",
        "            )\n",
        "\n",
        "        # Assign model param to callbacks\n",
        "        for c in callbacks:\n",
        "            c.model = self\n",
        "\n",
        "        # Run callbacks at start of training\n",
        "        self._is_training = True\n",
        "        for c in callbacks:\n",
        "            c.on_train_start()\n",
        "\n",
        "        # Fit the model!\n",
        "        for i in range(int(epochs)):\n",
        "            print(i)\n",
        "            if (i%20==0) and (i>0):\n",
        "              print(self._current_elbo)\n",
        "              #self.bayesian_update()\n",
        "            # Stop training early?\n",
        "            if not self._is_training:\n",
        "                break\n",
        "\n",
        "            # Run callbacks at start of epoch\n",
        "            self._current_elbo = 0.0\n",
        "            self._data.on_epoch_start()\n",
        "            for c in callbacks:\n",
        "                c.on_epoch_start()\n",
        "\n",
        "            # Update gradients for each batch\n",
        "            for x_data, y_data in self._data:\n",
        "                x_data = torch.from_numpy(x_data)\n",
        "                x_data.requires_grad_(False)\n",
        "                y_data = torch.from_numpy(y_data)\n",
        "                y_data.requires_grad_(False)\n",
        "                #print(x_data.is_leaf)\n",
        "                #print(y_data.is_leaf)\n",
        "                self.train_step(x_data, y_data)\n",
        "\n",
        "            # Run callbacks at end of epoch\n",
        "            self._data.on_epoch_end()\n",
        "            for c in callbacks:\n",
        "                c.on_epoch_end()\n",
        "            #self.bayesian_update()\n",
        "        # Run callbacks at end of training\n",
        "        self._is_training = False\n",
        "        for c in callbacks:\n",
        "            c.on_train_end()\n",
        "\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from probflow.data import make_generator\n",
        "from probflow.utils.metrics import get_metric_fn\n",
        "\n",
        "from probflow.callbacks import Callback\n",
        "\n",
        "\n",
        "class MonitorQErrorVal(pf.MonitorMetric):\n",
        "    \"\"\"Monitor some metric on validation data\n",
        "    Parameters\n",
        "    ----------\n",
        "    metric : str\n",
        "        Name of the metric to evaluate.  See :meth:`.Model.metric` for a list\n",
        "        of available metrics.\n",
        "    x : |ndarray| or |DataFrame| or |Series| or Tensor or |DataGenerator|\n",
        "        Independent variable values of the validation dataset to evaluate (aka\n",
        "        the \"features\").  Or a |DataGenerator| to generate both x and y.\n",
        "    y : |ndarray| or |DataFrame| or |Series| or Tensor\n",
        "        Dependent variable values of the validation dataset to evaluate (aka\n",
        "        the \"target\").\n",
        "    verbose : bool\n",
        "        Whether to print the average ELBO at the end of every training epoch\n",
        "        (if True) or not (if False).  Default = False\n",
        "    Example\n",
        "    -------\n",
        "    See the user guide section on :ref:`monitoring-a-metric`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, metric, x, y=None, verbose=False):\n",
        "        self.metric_name = metric\n",
        "        # Store validation data\n",
        "        self.x = x\n",
        "        self.y = unnormalize_labels(y, 0., 19.94772801931604)\n",
        "\n",
        "        # Store metrics and epochs\n",
        "        self.current_metric = np.nan\n",
        "        self.current_epoch = 0\n",
        "        self.metrics = []\n",
        "        self.epochs = []\n",
        "        self.verbose = verbose\n",
        "        self.start_time = None\n",
        "        self.wall_times = []\n",
        "\n",
        "    def on_epoch_start(self):\n",
        "        \"\"\"Record start time at the beginning of the first epoch\"\"\"\n",
        "        if self.start_time is None:\n",
        "            self.start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Compute the metric on validation data at the end of each epoch.\"\"\"\n",
        "        preds_unnorm = unnormalize_labels(self.model.predict(self.x, batch_size=32), 0., 19.94772801931604)\n",
        "        qerror = []\n",
        "        for i in range(len(preds_unnorm)):\n",
        "            if preds_unnorm[i] > float(self.y[i]):\n",
        "                qerror.append(preds_unnorm[i] / float(self.y[i]))\n",
        "            else:\n",
        "                qerror.append(float(self.y[i]) / float(preds_unnorm[i]))\n",
        "        self.current_metric = np.mean(qerror)\n",
        "        #print(\"Median: {}\".format(np.median(qerror)))\n",
        "        #print(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
        "        #print(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
        "        #print(\"99th percentile: {}\".format(np.percentile(qerror, 99)))\n",
        "        #print(\"Max: {}\".format(np.max(qerror)))\n",
        "        #print(\"Mean: {}\".format(np.mean(qerror)))        \n",
        "        self.current_epoch += 1\n",
        "        self.metrics += [self.current_metric]\n",
        "        self.epochs += [self.current_epoch]\n",
        "        self.wall_times += [time.time() - self.start_time]\n",
        "        \n",
        "    def plot(self, x=\"epoch\", **kwargs):\n",
        "        \"\"\"Plot the metric being monitored as a function of epoch\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : str {'epoch' or 'time'}\n",
        "            Whether to plot the metric as a function of epoch or wall time.\n",
        "            Default is to plot by epoch.\n",
        "        **kwargs\n",
        "            Additional keyword arguments are passed to plt.plot\n",
        "        \"\"\"\n",
        "        if x == \"time\":\n",
        "            plt.plot(self.wall_times, self.metrics, **kwargs)\n",
        "            plt.xlabel(\"Time (s)\")\n",
        "        else:\n",
        "            plt.plot(self.epochs, self.metrics, **kwargs)\n",
        "            plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(self.metric_name)\n",
        "\n",
        "import probflow.utils.ops as O\n",
        "class BetaBNN(OurContinuousModel):\n",
        "\n",
        "    def __init__(self, bayesian_input_dim):\n",
        "        self.bayesian_input_dim = bayesian_input_dim\n",
        "        self.loc = pf.Dense(self.bayesian_input_dim, 1, probabilistic=False)\n",
        "        self.scale = pf.Dense(self.bayesian_input_dim, 1, probabilistic=False)\n",
        "        \n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = torch.nan_to_num(x)\n",
        "        x = torch.clamp(x, min=0.000000001, max=100000)\n",
        "        x.requires_grad_(False)\n",
        "        assert not torch.isnan(x).any()\n",
        "        c1 = self.loc(x)\n",
        "        c1 = torch.nan_to_num(c1)\n",
        "        c1 = torch.clamp(c1, min=0.000000001, max=100000)\n",
        "        assert not torch.isnan(c1).any()\n",
        "        c2 = self.scale(x)\n",
        "        c2 = torch.nan_to_num(c2)\n",
        "        c2 = torch.clamp(c2, min=0.000000001, max=100000)\n",
        "        assert not torch.isnan(c2).any()\n",
        "        #return pf.Normal(O.softplus(c1), O.softplus(c2))\n",
        "        return OurBeta(O.softplus(c1), O.softplus(c2))  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-7cgQ9R1JVZ"
      },
      "source": [
        "# Train BNN by creating Beta Dist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from probflow.utils.casting import to_numpy\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "F60a4wTrrNma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgJJJaY8kno_"
      },
      "outputs": [],
      "source": [
        "from probflow.utils.casting import to_numpy\n",
        "import pandas as pd\n",
        "\n",
        "def get_data_from_dataloader(mscn_model, dataloader):\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    for batch_idx, data_batch in enumerate(dataloader):\n",
        "        samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
        "        if True:#use cuda\n",
        "            samples, predicates, joins, targets = samples.cuda(), predicates.cuda(), joins.cuda(), targets.cuda()\n",
        "            sample_masks, predicate_masks, join_masks = sample_masks.cuda(), predicate_masks.cuda(), join_masks.cuda()\n",
        "        samples, predicates, joins, targets = Variable(samples, requires_grad=False), Variable(predicates, requires_grad=False), Variable(joins, requires_grad=False), Variable(targets, requires_grad=False)\n",
        "        sample_masks, predicate_masks, join_masks = Variable(sample_masks, requires_grad=False), Variable(predicate_masks, requires_grad=False), Variable(join_masks, requires_grad=False)\n",
        "        targets = targets.float()\n",
        "        targets.requires_grad_(False)\n",
        "        outputs, hid = mscn_model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
        "        hid = hid.float()\n",
        "        hid = hid.detach()\n",
        "        for item in range(len(hid)):\n",
        "            x_train.append(hid[item].cpu())\n",
        "            y_train.append(torch.clamp(targets[item].cpu(), min=0.000000001))\n",
        "    y_train = np.array(to_numpy(y_train))\n",
        "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
        "    x_train = np.array(to_numpy(x_train))\n",
        "    return x_train, y_train\n",
        "        \n",
        "def train_and_predict_bnn(mscn_model, workload_name, num_queries, num_epochs, batch_size, hid_units):\n",
        "    # Load training and validation data\n",
        "    num_materialized_samples = 1000\n",
        "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(\n",
        "        num_queries, num_materialized_samples)\n",
        "    table2vec, column2vec, op2vec, join2vec = dicts\n",
        "\n",
        "    # Train model\n",
        "    sample_feats = len(table2vec) + num_materialized_samples\n",
        "    predicate_feats = len(column2vec) + len(op2vec) + 1\n",
        "    join_feats = len(join2vec)\n",
        "\n",
        "    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "    x_train, y_train = get_data_from_dataloader(mscn_model, train_data_loader)\n",
        "\n",
        "    #####BNN#######\n",
        "\n",
        "    bnn_model = BetaBNN(hid_units)    \n",
        "    \n",
        "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "    x_test, y_test = get_data_from_dataloader(mscn_model, test_data_loader)\n",
        "\n",
        "    assert not np.isnan(np.sum(x_train))\n",
        "    assert not np.isnan(np.sum(y_train))\n",
        "    me = pf.MonitorELBO()\n",
        "    #monitor_mae = MonitorQErrorVal('meanqerrorval', x_test, y_test)\n",
        "    early_stopping = pf.EarlyStopping(me, patience=300)\n",
        "    #nlm_lrs = pf.LearningRateScheduler(lambda e: 5e-4 + e * 5e-6)\n",
        "    print(\"Created the model and about to start the fitting process.\") \n",
        "    #Eager False (still) gives errors.\n",
        "    #Flipout seems to tend to produce NaNs.\n",
        "    bnn_model.fit(x=x_train, y=y_train, batch_size=batch_size, num_workers=20, shuffle=True, flipout=False, eager=True, n_mc=100, epochs=num_epochs, callbacks=[me, early_stopping])\n",
        "    #Parameter #2: n_mc: 1, 5, 25, 100\n",
        "    #Parameter #3: Architecture/distribution.\n",
        "    me.plot()\n",
        "    #monitor_mae.plot()\n",
        "\n",
        "\n",
        "    #x_train = torch.from_numpy(x_train).requires_grad_(False)\n",
        "    #x_test = torch.from_numpy(x_test).requires_grad_(False)\n",
        "    #y_test = torch.from_numpy(y_test).requires_grad_(False)\n",
        "    \n",
        "    assert not np.isnan(np.sum(x_train))\n",
        "\n",
        "    preds_train = bnn_model.predict(x_train, method=\"mean\") ####it will predict some values..\n",
        "\n",
        "    assert not np.isnan(np.sum(x_test))\n",
        "\n",
        "    preds_test = bnn_model.predict(x_test, method=\"mean\")\n",
        "    print(\"First 10 predictions\")\n",
        "    for i in range(10):\n",
        "      print(preds_train[i],y_train[i])\n",
        "    #print(preds_train)\n",
        "    #print(y_train)\n",
        "\n",
        "    # Unnormalize\n",
        "    preds_train_unnorm = unnormalize_labels(preds_train, min_val, max_val)\n",
        "    labels_train_unnorm = unnormalize_labels(labels_train, min_val, max_val)\n",
        "\n",
        "    preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
        "    labels_test_unnorm = unnormalize_labels(labels_test, min_val, max_val)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nQ-Error training set:\")\n",
        "    print_qerror(preds_train_unnorm, labels_train_unnorm)\n",
        "\n",
        "    print(\"\\nQ-Error validation set:\")\n",
        "    print_qerror(preds_test_unnorm, labels_test_unnorm)\n",
        "    print(\"\")\n",
        "\n",
        "    # Load test data\n",
        "    file_name = \"/content/drive/MyDrive/Colab Notebooks/Database_Project/workloads/\" + workload_name\n",
        "    joins, predicates, tables, samples, label = load_data(file_name, num_materialized_samples)\n",
        "\n",
        "    # Get feature encoding and proper normalization\n",
        "    samples_test = encode_samples(tables, samples, table2vec)\n",
        "    predicates_test, joins_test = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
        "    labels_test, _, _ = normalize_labels(label, min_val, max_val)\n",
        "\n",
        "    \n",
        "    max_num_predicates = max([len(p) for p in predicates_test])\n",
        "    max_num_joins = max([len(j) for j in joins_test])\n",
        "\n",
        "    # Get test set predictions\n",
        "    test_data = make_dataset(samples_test, predicates_test, joins_test, labels_test, max_num_joins, max_num_predicates)\n",
        "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "    \n",
        "    x_test, y_test = get_data_from_dataloader(mscn_model, test_data_loader)\n",
        "    #x_test = torch.from_numpy(x_test).requires_grad_(False)\n",
        "    #y_test = torch.from_numpy(y_test).requires_grad_(False)\n",
        "    assert not np.isnan(np.sum(x_test))\n",
        "\n",
        "    preds_test = bnn_model.predict(x_test, method=\"mean\")\n",
        "\n",
        "    # Unnormalize\n",
        "    preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nQ-Error \" + workload_name + \":\")\n",
        "    print_qerror(preds_test_unnorm, label)\n",
        "\n",
        "    # Write predictions\n",
        "    file_name = \"/content/bnn_predictions_\" + workload_name + \".csv\"\n",
        "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "    with open(file_name, \"w\") as f:\n",
        "        for i in range(len(preds_test_unnorm)):\n",
        "            f.write(str(preds_test_unnorm[i]) + \",\" + label[i] + \"\\n\")\n",
        "    \n",
        "    return bnn_model\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NWhaabFhnMDs",
        "outputId": "326a1c95-b943-41e4-a142-279f60924783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded queries\n",
            "Loaded bitmaps\n",
            "min log(label): 0.0\n",
            "max log(label): 19.94772801931604\n",
            "Number of training samples: 9000\n",
            "Number of validation samples: 1000\n",
            "Created TensorDataset for training data\n",
            "Created TensorDataset for validation data\n",
            "Created the model and about to start the fitting process.\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "inf\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "-inf\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "-inf\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "-inf\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "First 10 predictions\n",
            "[0.6633618] [0.6294485]\n",
            "[0.6628345] [0.69772357]\n",
            "[0.72058874] [0.7571468]\n",
            "[0.6294073] [0.59212756]\n",
            "[0.5845791] [0.5470257]\n",
            "[0.7177658] [0.70057464]\n",
            "[0.7557592] [0.76217115]\n",
            "[0.7569429] [0.74066216]\n",
            "[0.72678936] [0.68841165]\n",
            "[0.71312696] [0.7162709]\n",
            "\n",
            "Q-Error training set:\n",
            "Median: 1.475334122756491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90th percentile: [37.54751997]\n",
            "95th percentile: [393.3315873]\n",
            "99th percentile: [10554.035]\n",
            "Max: [35407.]\n",
            "Mean: [315.33974628]\n",
            "\n",
            "Q-Error validation set:\n",
            "Median: [1.48531085]\n",
            "90th percentile: [86.26388247]\n",
            "95th percentile: [601.66110197]\n",
            "99th percentile: [8128.70666667]\n",
            "Max: [25395.]\n",
            "Mean: [296.63318744]\n",
            "\n",
            "Loaded queries\n",
            "Loaded bitmaps\n",
            "\n",
            "Q-Error job-light:\n",
            "Median: [1.83617271]\n",
            "90th percentile: 23.513801878721246\n",
            "95th percentile: [182.81040364]\n",
            "99th percentile: [1207.78858586]\n",
            "Max: [2774.55555556]\n",
            "Mean: [61.35085663]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "pf.set_backend(\"pytorch\")\n",
        "our_bnn_model = train_and_predict_bnn(our_mscn_model, \"job-light\",10000,100,32,256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCzBsVAaRBjw"
      },
      "outputs": [],
      "source": [
        "def predict_bnn(mscn_model, bnn_model, workload_name, method_name):\n",
        "    # Load training and validation data\n",
        "    batch_size = 32\n",
        "    num_materialized_samples = 1000\n",
        "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(\n",
        "        1000, num_materialized_samples)\n",
        "    table2vec, column2vec, op2vec, join2vec = dicts\n",
        "\n",
        "    # Load test data\n",
        "    file_name = \"/content/drive/MyDrive/Colab Notebooks/Database_Project/workloads/\" + workload_name\n",
        "    joins, predicates, tables, samples, label = load_data(file_name, num_materialized_samples)\n",
        "\n",
        "    # Get feature encoding and proper normalization\n",
        "    samples_test = encode_samples(tables, samples, table2vec)\n",
        "    predicates_test, joins_test = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
        "    labels_test, _, _ = normalize_labels(label, min_val, max_val)\n",
        "\n",
        "    \n",
        "    max_num_predicates = max([len(p) for p in predicates_test])\n",
        "    max_num_joins = max([len(j) for j in joins_test])\n",
        "\n",
        "    # Get test set predictions\n",
        "    test_data = make_dataset(samples_test, predicates_test, joins_test, labels_test, max_num_joins, max_num_predicates)\n",
        "    test_data_loader = DataLoader(test_data, batch_size=128)\n",
        "    \n",
        "    x_test, y_test = get_data_from_dataloader(mscn_model, test_data_loader)\n",
        "    #x_test = torch.from_numpy(x_test).requires_grad_(False)\n",
        "    #y_test = torch.from_numpy(y_test).requires_grad_(False)\n",
        "\n",
        "    preds_test = bnn_model.predict(x_test, method=method_name)\n",
        "\n",
        "    # Unnormalize\n",
        "    preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nQ-Error \" + workload_name + \":\")\n",
        "    print_qerror(preds_test_unnorm, label)\n",
        "    if method_name==\"mean\":\n",
        "        # Write predictions\n",
        "        file_name = \"/content/bnn_predictions\"+method_name+\"_\"+ workload_name + \".csv\"\n",
        "        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "        with open(file_name, \"w\") as f:\n",
        "            for i in range(len(preds_test_unnorm)):\n",
        "                f.write(str(preds_test_unnorm[i][0]) + \",\" + label[i] + \"\\n\")\n",
        "    else:\n",
        "        # Write predictions\n",
        "        file_name = \"/content/bnn_predictions\"+method_name+\"_\"+ workload_name + \".csv\"\n",
        "        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "        with open(file_name, \"w\") as f:\n",
        "            for i in range(len(preds_test_unnorm)):\n",
        "                f.write(str(preds_test_unnorm[i]) + \",\" + label[i] + \"\\n\")\n",
        "    \n",
        "    num_materialized_samples = 1000\n",
        "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(\n",
        "        10000, num_materialized_samples)\n",
        "    table2vec, column2vec, op2vec, join2vec = dicts\n",
        "\n",
        "    # Train model\n",
        "    sample_feats = len(table2vec) + num_materialized_samples\n",
        "    predicate_feats = len(column2vec) + len(op2vec) + 1\n",
        "    join_feats = len(join2vec)\n",
        "\n",
        "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "    x_val, y_val = get_data_from_dataloader(mscn_model, test_data_loader)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], \"k:\", label=\"Ideal\")\n",
        "    bnn_model.calibration_curve_plot(x_val, y_val, batch_size=10000, label=\"Our BNN Model\") \n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    bnn_model.pred_dist_plot(x_test)\n",
        "    print(bnn_model.pred_dist_covered(x_test, y_test, ci=0.95))\n",
        "    print(bnn_model.pred_dist_coverage(x_val, y_val, ci=0.95))\n",
        "    print(bnn_model.pred_dist_coverage(x_test, y_test, ci=0.95))\n",
        "    print(bnn_model.sharpness(x_val))\n",
        "    return bnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xVetLjo-RrUX",
        "outputId": "314cbb81-ef0f-4f58-b754-13851aab8838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded queries\n",
            "Loaded bitmaps\n",
            "min log(label): 0.0\n",
            "max log(label): 19.94772801931604\n",
            "Number of training samples: 900\n",
            "Number of validation samples: 100\n",
            "Created TensorDataset for training data\n",
            "Created TensorDataset for validation data\n",
            "Loaded queries\n",
            "Loaded bitmaps\n",
            "\n",
            "Q-Error job-light:\n",
            "Median: [1.83912841]\n",
            "90th percentile: [21.92380892]\n",
            "95th percentile: [149.76293112]\n",
            "99th percentile: [1021.28454545]\n",
            "Max: [2202.]\n",
            "Mean: [52.98676259]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded queries\n",
            "Loaded bitmaps\n",
            "min log(label): 0.0\n",
            "max log(label): 19.94772801931604\n",
            "Number of training samples: 9000\n",
            "Number of validation samples: 1000\n",
            "Created TensorDataset for training data\n",
            "Created TensorDataset for validation data\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [False]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [False]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [False]\n",
            " [False]\n",
            " [False]\n",
            " [False]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [False]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [False]\n",
            " [ True]\n",
            " [False]\n",
            " [False]\n",
            " [ True]\n",
            " [False]\n",
            " [ True]\n",
            " [ True]\n",
            " [False]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]]\n",
            "0.959\n",
            "0.8428571428571429\n",
            "0.13292213\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.BetaBNN at 0x7f01cd13de10>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#predict_bnn(our_mscn_model, our_bnn_model, \"job-light\", \"mode\")<= Mode is really not working.\n",
        "predict_bnn(our_mscn_model, our_bnn_model, \"job-light\", \"mean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMbvXizFbk8D"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# open a file, where you ant to store the data\n",
        "file = open('/content/drive/MyDrive/Colab Notebooks/DBSE project_CE/our_bnn_model.pkl', 'wb')\n",
        "\n",
        "# dump information to that file\n",
        "pickle.dump(our_bnn_model, file)\n",
        "\n",
        "# close the file\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4YaBhxNcP6W"
      },
      "outputs": [],
      "source": [
        "#pf.utils.io.dump(our_bnn_model, \"/content/bnn_model.pf\")#Actually we still cannot write to file, maybe if we had trained with a single worker, we would see a difference.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "eTfYgWCEM2C4",
        "a-wwzicHM-Tw"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}